"""
Trend Analysis API
è¶‹åŠ¿åˆ†æAPIæ¥å£
"""

from fastapi import APIRouter, HTTPException, Query, BackgroundTasks
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from datetime import datetime
import sys
sys.path.insert(0, '/Users/ywc/ai-stack-super-enhanced/ğŸ” Intelligent Trend Analysis')

from crawlers.news_crawler import default_crawler_manager
from analysis.trend_analyzer import TrendAnalyzer, ReportGenerator

router = APIRouter(prefix="/trend", tags=["Trend Analysis API"])

# åˆå§‹åŒ–åˆ†æå™¨
analyzer = TrendAnalyzer()
report_generator = ReportGenerator()


# ============ Pydantic Models ============

class CrawlTaskRequest(BaseModel):
    """çˆ¬å–ä»»åŠ¡è¯·æ±‚"""
    categories: List[str] = ["policy", "tech", "industry", "hot"]
    frequency: Optional[str] = "daily"  # çˆ¬å–é¢‘ç‡


class ReportRequest(BaseModel):
    """æŠ¥å‘Šç”Ÿæˆè¯·æ±‚"""
    report_type: str  # industry/investment
    industry: Optional[str] = None
    start_date: Optional[str] = None
    end_date: Optional[str] = None


# ============ API Endpoints ============

@router.post("/crawl/start")
async def start_crawling(
    request: CrawlTaskRequest,
    background_tasks: BackgroundTasks
):
    """
    å¯åŠ¨çˆ¬è™«ä»»åŠ¡
    
    æ ¹æ®éœ€æ±‚6.1: æŒ‰ç…§ä¸€å®šé¢‘ç‡è·å–ä¿¡æ¯
    
    Args:
        request: çˆ¬å–ä»»åŠ¡é…ç½®
        background_tasks: åå°ä»»åŠ¡
        
    Returns:
        ä»»åŠ¡å¯åŠ¨çŠ¶æ€
    """
    try:
        # åœ¨åå°æ‰§è¡Œçˆ¬è™«
        background_tasks.add_task(_execute_crawl_task, request.categories)
        
        return {
            "status": "started",
            "message": "çˆ¬è™«ä»»åŠ¡å·²å¯åŠ¨",
            "categories": request.categories,
            "frequency": request.frequency,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨çˆ¬è™«å¤±è´¥: {str(e)}")


@router.get("/data/latest")
async def get_latest_data(
    category: Optional[str] = Query(None, description="ç±»åˆ«ç­›é€‰"),
    limit: int = Query(20, description="è¿”å›æ•°é‡")
):
    """
    è·å–æœ€æ–°çˆ¬å–çš„æ•°æ®
    
    Args:
        category: ç±»åˆ«
        limit: æ•°é‡é™åˆ¶
        
    Returns:
        æœ€æ–°æ•°æ®
    """
    try:
        results = default_crawler_manager.get_latest_results(limit)
        
        if category:
            results = [r for r in results if r.get("category") == category]
        
        return {
            "total": len(results),
            "data": results,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ•°æ®å¤±è´¥: {str(e)}")


@router.get("/analysis/summary")
async def get_trend_summary(
    category: Optional[str] = Query(None, description="ç±»åˆ«ç­›é€‰")
):
    """
    è·å–è¶‹åŠ¿æ±‡æ€»
    
    æ ¹æ®éœ€æ±‚6.3: æ±‡æ€»ã€æ€»ç»“
    
    Args:
        category: ç±»åˆ«
        
    Returns:
        è¶‹åŠ¿æ±‡æ€»
    """
    try:
        # è·å–æ•°æ®
        data = default_crawler_manager.get_latest_results(100)
        
        if category:
            data = [d for d in data if d.get("category") == category]
        
        # åˆ†ææ±‡æ€»
        summary = analyzer.summarize_content(data)
        
        # æ£€æµ‹çƒ­ç‚¹
        hot_topics = analyzer.detect_hot_topics(data)
        
        return {
            "summary": summary,
            "hot_topics": hot_topics[:10],
            "category": category or "å…¨éƒ¨",
            "generated_at": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ†æå¤±è´¥: {str(e)}")


@router.post("/report/generate")
async def generate_report(request: ReportRequest):
    """
    ç”Ÿæˆåˆ†ææŠ¥å‘Š
    
    æ ¹æ®éœ€æ±‚6.4: è¾“å‡ºæŠ¥å‘Šï¼ˆäº§ä¸šæŠ¥å‘Šã€è¡Œä¸šæŠ¥å‘Šã€æŠ•èµ„æŠ¥å‘Šï¼‰
    
    Args:
        request: æŠ¥å‘Šè¯·æ±‚
        
    Returns:
        ç”Ÿæˆçš„æŠ¥å‘Š
    """
    try:
        # è·å–ç›¸å…³æ•°æ®
        data = default_crawler_manager.get_latest_results(200)
        
        # æ ¹æ®æŠ¥å‘Šç±»å‹ç”Ÿæˆ
        if request.report_type == "industry":
            report = report_generator.generate_industry_report(
                industry=request.industry or "ç§‘æŠ€",
                data=data
            )
        elif request.report_type == "investment":
            report = report_generator.generate_investment_report(data)
        else:
            raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æŠ¥å‘Šç±»å‹")
        
        return report
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {str(e)}")


@router.get("/hot-topics")
async def get_hot_topics(limit: int = Query(10, description="è¿”å›æ•°é‡")):
    """
    è·å–çƒ­ç‚¹è¯é¢˜
    
    Args:
        limit: è¿”å›æ•°é‡
        
    Returns:
        çƒ­ç‚¹è¯é¢˜åˆ—è¡¨
    """
    try:
        data = default_crawler_manager.get_latest_results(100)
        hot_topics = analyzer.detect_hot_topics(data)
        
        return {
            "hot_topics": hot_topics[:limit],
            "total": len(hot_topics),
            "generated_at": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–çƒ­ç‚¹å¤±è´¥: {str(e)}")


@router.get("/dashboard")
async def get_trend_dashboard():
    """
    è·å–è¶‹åŠ¿åˆ†æçœ‹æ¿
    
    Returns:
        çœ‹æ¿æ•°æ®
    """
    try:
        # è·å–æœ€æ–°æ•°æ®
        data = default_crawler_manager.get_latest_results(100)
        
        # æ±‡æ€»åˆ†æ
        summary = analyzer.summarize_content(data)
        
        # çƒ­ç‚¹è¯é¢˜
        hot_topics = analyzer.detect_hot_topics(data)
        
        # åˆ†ç±»ç»Ÿè®¡
        classified = analyzer.classify_content(data)
        
        # æ¨¡æ‹Ÿçˆ¬è™«çŠ¶æ€
        crawler_status = {
            "total_crawlers": 4,
            "active_crawlers": 4,
            "last_crawl_time": datetime.now().isoformat(),
            "total_articles": len(data),
            "today_articles": len([d for d in data if d.get("publish_date", "")[:10] == datetime.now().strftime("%Y-%m-%d")]),
        }
        
        return {
            "crawler_status": crawler_status,
            "summary": summary,
            "hot_topics": hot_topics[:10],
            "category_distribution": {k: len(v) for k, v in classified.items()},
            "latest_articles": data[-10:] if data else [],
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–çœ‹æ¿æ•°æ®å¤±è´¥: {str(e)}")


@router.get("/")
def root():
    """è¶‹åŠ¿åˆ†ææ¨¡å—æ ¹è·¯å¾„"""
    return {
        "module": "Intelligent Trend Analysis",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "crawl": "/trend/crawl/start",
            "data": "/trend/data/latest",
            "summary": "/trend/analysis/summary",
            "report": "/trend/report/generate",
            "hot_topics": "/trend/hot-topics",
            "dashboard": "/trend/dashboard"
        }
    }


# ============ è¾…åŠ©å‡½æ•° ============

async def _execute_crawl_task(categories: List[str]):
    """
    æ‰§è¡Œçˆ¬è™«ä»»åŠ¡ï¼ˆåå°ï¼‰
    
    Args:
        categories: çˆ¬å–ç±»åˆ«åˆ—è¡¨
    """
    print(f"ğŸ•·ï¸ å¼€å§‹æ‰§è¡Œçˆ¬è™«ä»»åŠ¡: {categories}")
    
    try:
        # æ‰§è¡Œçˆ¬è™«
        results = default_crawler_manager.crawl_all()
        
        print(f"âœ… çˆ¬è™«ä»»åŠ¡å®Œæˆï¼Œè·å– {len(results)} æ¡æ•°æ®")
        
        # TODO: ä¿å­˜åˆ°æ•°æ®åº“æˆ–RAG
        
    except Exception as e:
        print(f"âŒ çˆ¬è™«ä»»åŠ¡å¤±è´¥: {e}")


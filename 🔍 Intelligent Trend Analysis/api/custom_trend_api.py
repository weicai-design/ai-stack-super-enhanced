"""
è‡ªå®šä¹‰è¶‹åŠ¿åˆ†æAPI
æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰åˆ†æç»´åº¦ã€æ—¶é—´èŒƒå›´ã€æŒ‡æ ‡ç­‰
"""

from fastapi import APIRouter, HTTPException, Query, Body
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import sys
sys.path.insert(0, '/Users/ywc/ai-stack-super-enhanced/ğŸ” Intelligent Trend Analysis')

from analysis.trend_analyzer import TrendAnalyzer
from crawlers.news_crawler import default_crawler_manager

router = APIRouter(prefix="/trend/custom", tags=["Custom Trend Analysis API"])

# åˆå§‹åŒ–åˆ†æå™¨
analyzer = TrendAnalyzer()


class CustomAnalysisRequest(BaseModel):
    """è‡ªå®šä¹‰åˆ†æè¯·æ±‚"""
    # æ—¶é—´èŒƒå›´
    start_date: Optional[str] = Field(None, description="å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)")
    end_date: Optional[str] = Field(None, description="ç»“æŸæ—¥æœŸ (YYYY-MM-DD)")
    time_range: Optional[str] = Field(None, description="æ—¶é—´èŒƒå›´: today/week/month/quarter/year/custom")
    
    # æ•°æ®æºç­›é€‰
    categories: Optional[List[str]] = Field(None, description="ç±»åˆ«ç­›é€‰: policy/tech/industry/hot")
    sources: Optional[List[str]] = Field(None, description="æ¥æºç­›é€‰")
    keywords: Optional[List[str]] = Field(None, description="å…³é”®è¯ç­›é€‰")
    
    # åˆ†æç»´åº¦
    analysis_dimensions: List[str] = Field(
        default=["volume", "sentiment", "keywords", "topics"],
        description="åˆ†æç»´åº¦: volume/sentiment/keywords/topics/trend/comparison"
    )
    
    # è‡ªå®šä¹‰æŒ‡æ ‡
    custom_metrics: Optional[Dict[str, Any]] = Field(None, description="è‡ªå®šä¹‰æŒ‡æ ‡é…ç½®")
    
    # åˆ†ç»„æ–¹å¼
    group_by: Optional[str] = Field(None, description="åˆ†ç»„æ–¹å¼: category/source/date/keyword")
    
    # æ’åºæ–¹å¼
    sort_by: Optional[str] = Field("relevance", description="æ’åºæ–¹å¼: relevance/time/volume")
    sort_order: Optional[str] = Field("desc", description="æ’åºé¡ºåº: asc/desc")
    
    # ç»“æœé™åˆ¶
    limit: Optional[int] = Field(100, description="ç»“æœæ•°é‡é™åˆ¶")


class CustomTrendComparisonRequest(BaseModel):
    """è‡ªå®šä¹‰è¶‹åŠ¿å¯¹æ¯”è¯·æ±‚"""
    # å¯¹æ¯”ç»„1
    group1_name: str
    group1_config: Dict[str, Any]
    
    # å¯¹æ¯”ç»„2
    group2_name: str
    group2_config: Dict[str, Any]
    
    # å¯¹æ¯”ç»´åº¦
    comparison_dimensions: List[str] = ["volume", "keywords", "sentiment", "topics"]


@router.post("/analyze")
async def custom_trend_analysis(request: CustomAnalysisRequest):
    """
    è‡ªå®šä¹‰è¶‹åŠ¿åˆ†æ
    
    æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ï¼š
    - æ—¶é—´èŒƒå›´
    - æ•°æ®æºç­›é€‰
    - åˆ†æç»´åº¦
    - è‡ªå®šä¹‰æŒ‡æ ‡
    - åˆ†ç»„å’Œæ’åºæ–¹å¼
    
    Args:
        request: è‡ªå®šä¹‰åˆ†æè¯·æ±‚
        
    Returns:
        åˆ†æç»“æœ
    """
    try:
        # 1. è·å–æ•°æ®
        all_data = default_crawler_manager.get_latest_results(1000)
        
        # 2. æ—¶é—´èŒƒå›´ç­›é€‰
        filtered_data = _filter_by_time_range(
            all_data,
            request.start_date,
            request.end_date,
            request.time_range
        )
        
        # 3. ç±»åˆ«ç­›é€‰
        if request.categories:
            filtered_data = [
                d for d in filtered_data
                if d.get("category") in request.categories
            ]
        
        # 4. æ¥æºç­›é€‰
        if request.sources:
            filtered_data = [
                d for d in filtered_data
                if d.get("source") in request.sources
            ]
        
        # 5. å…³é”®è¯ç­›é€‰
        if request.keywords:
            filtered_data = [
                d for d in filtered_data
                if any(kw in d.get("content", "").lower() or kw in d.get("title", "").lower()
                      for kw in request.keywords)
            ]
        
        # 6. æ‰§è¡Œåˆ†æ
        analysis_result = {}
        
        if "volume" in request.analysis_dimensions:
            analysis_result["volume"] = _analyze_volume(filtered_data, request.group_by)
        
        if "sentiment" in request.analysis_dimensions:
            analysis_result["sentiment"] = _analyze_sentiment(filtered_data)
        
        if "keywords" in request.analysis_dimensions:
            analysis_result["keywords"] = _analyze_keywords(filtered_data, request.limit)
        
        if "topics" in request.analysis_dimensions:
            analysis_result["topics"] = analyzer.detect_hot_topics(filtered_data)
        
        if "trend" in request.analysis_dimensions:
            analysis_result["trend"] = _analyze_trend(filtered_data, request.group_by)
        
        if "comparison" in request.analysis_dimensions:
            # éœ€è¦å†å²æ•°æ®å¯¹æ¯”
            previous_data = _get_previous_period_data(all_data, request.time_range)
            analysis_result["comparison"] = analyzer.compare_trends(filtered_data, previous_data)
        
        # 7. è‡ªå®šä¹‰æŒ‡æ ‡è®¡ç®—
        if request.custom_metrics:
            analysis_result["custom_metrics"] = _calculate_custom_metrics(
                filtered_data,
                request.custom_metrics
            )
        
        # 8. æ’åºå’Œé™åˆ¶
        if request.sort_by:
            analysis_result = _sort_results(analysis_result, request.sort_by, request.sort_order)
        
        return {
            "success": True,
            "analysis_config": request.dict(),
            "data_count": len(filtered_data),
            "results": analysis_result,
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è‡ªå®šä¹‰åˆ†æå¤±è´¥: {str(e)}")


@router.post("/compare")
async def custom_trend_comparison(request: CustomTrendComparisonRequest):
    """
    è‡ªå®šä¹‰è¶‹åŠ¿å¯¹æ¯”åˆ†æ
    
    å¯¹æ¯”ä¸¤ä¸ªè‡ªå®šä¹‰é…ç½®çš„æ•°æ®ç»„
    
    Args:
        request: å¯¹æ¯”è¯·æ±‚
        
    Returns:
        å¯¹æ¯”ç»“æœ
    """
    try:
        # è·å–ä¸¤ç»„æ•°æ®
        all_data = default_crawler_manager.get_latest_results(1000)
        
        group1_data = _apply_config_filter(all_data, request.group1_config)
        group2_data = _apply_config_filter(all_data, request.group2_config)
        
        # æ‰§è¡Œå¯¹æ¯”åˆ†æ
        comparison_result = {}
        
        if "volume" in request.comparison_dimensions:
            comparison_result["volume"] = {
                "group1": len(group1_data),
                "group2": len(group2_data),
                "difference": len(group1_data) - len(group2_data),
                "difference_percent": (
                    (len(group1_data) - len(group2_data)) / len(group2_data) * 100
                    if len(group2_data) > 0 else 0
                )
            }
        
        if "keywords" in request.comparison_dimensions:
            group1_keywords = set(analyzer.extract_keywords(
                " ".join(d.get("content", "") for d in group1_data), 20
            ))
            group2_keywords = set(analyzer.extract_keywords(
                " ".join(d.get("content", "") for d in group2_data), 20
            ))
            
            comparison_result["keywords"] = {
                "common": list(group1_keywords & group2_keywords),
                "group1_only": list(group1_keywords - group2_keywords),
                "group2_only": list(group2_keywords - group1_keywords)
            }
        
        if "sentiment" in request.comparison_dimensions:
            comparison_result["sentiment"] = {
                "group1": _analyze_sentiment(group1_data),
                "group2": _analyze_sentiment(group2_data)
            }
        
        if "topics" in request.comparison_dimensions:
            comparison_result["topics"] = {
                "group1": analyzer.detect_hot_topics(group1_data)[:10],
                "group2": analyzer.detect_hot_topics(group2_data)[:10]
            }
        
        return {
            "success": True,
            "group1_name": request.group1_name,
            "group2_name": request.group2_name,
            "group1_count": len(group1_data),
            "group2_count": len(group2_data),
            "comparison": comparison_result,
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"å¯¹æ¯”åˆ†æå¤±è´¥: {str(e)}")


@router.get("/dimensions")
async def get_available_dimensions():
    """è·å–å¯ç”¨çš„åˆ†æç»´åº¦"""
    return {
        "dimensions": [
            {
                "id": "volume",
                "name": "æ•°é‡åˆ†æ",
                "description": "åˆ†ææ•°æ®é‡ã€å¢é•¿è¶‹åŠ¿ç­‰"
            },
            {
                "id": "sentiment",
                "name": "æƒ…æ„Ÿåˆ†æ",
                "description": "åˆ†ææƒ…æ„Ÿå€¾å‘ã€æ­£é¢/è´Ÿé¢æ¯”ä¾‹"
            },
            {
                "id": "keywords",
                "name": "å…³é”®è¯åˆ†æ",
                "description": "æå–å’Œåˆ†æå…³é”®è¯é¢‘ç‡"
            },
            {
                "id": "topics",
                "name": "è¯é¢˜åˆ†æ",
                "description": "æ£€æµ‹çƒ­ç‚¹è¯é¢˜å’Œè¶‹åŠ¿"
            },
            {
                "id": "trend",
                "name": "è¶‹åŠ¿åˆ†æ",
                "description": "åˆ†ææ—¶é—´åºåˆ—è¶‹åŠ¿"
            },
            {
                "id": "comparison",
                "name": "å¯¹æ¯”åˆ†æ",
                "description": "ä¸å†å²æ•°æ®å¯¹æ¯”"
            }
        ],
        "group_by_options": [
            "category", "source", "date", "keyword"
        ],
        "sort_options": [
            "relevance", "time", "volume"
        ]
    }


# ============ è¾…åŠ©å‡½æ•° ============

def _filter_by_time_range(
    data: List[Dict],
    start_date: Optional[str],
    end_date: Optional[str],
    time_range: Optional[str]
) -> List[Dict]:
    """æŒ‰æ—¶é—´èŒƒå›´ç­›é€‰æ•°æ®"""
    if start_date and end_date:
        return [
            d for d in data
            if start_date <= d.get("publish_date", "")[:10] <= end_date
        ]
    
    if time_range:
        today = datetime.now()
        if time_range == "today":
            date_str = today.strftime("%Y-%m-%d")
            return [d for d in data if d.get("publish_date", "")[:10] == date_str]
        elif time_range == "week":
            week_ago = today - timedelta(days=7)
            return [d for d in data if d.get("publish_date", "")[:10] >= week_ago.strftime("%Y-%m-%d")]
        elif time_range == "month":
            month_ago = today - timedelta(days=30)
            return [d for d in data if d.get("publish_date", "")[:10] >= month_ago.strftime("%Y-%m-%d")]
        elif time_range == "quarter":
            quarter_ago = today - timedelta(days=90)
            return [d for d in data if d.get("publish_date", "")[:10] >= quarter_ago.strftime("%Y-%m-%d")]
        elif time_range == "year":
            year_ago = today - timedelta(days=365)
            return [d for d in data if d.get("publish_date", "")[:10] >= year_ago.strftime("%Y-%m-%d")]
    
    return data


def _analyze_volume(data: List[Dict], group_by: Optional[str]) -> Dict[str, Any]:
    """æ•°é‡åˆ†æ"""
    result = {
        "total": len(data),
        "by_date": {}
    }
    
    if group_by == "category":
        from collections import Counter
        result["by_category"] = dict(Counter(d.get("category") for d in data))
    elif group_by == "source":
        from collections import Counter
        result["by_source"] = dict(Counter(d.get("source") for d in data))
    
    # æŒ‰æ—¥æœŸç»Ÿè®¡
    dates = [d.get("publish_date", "")[:10] for d in data if d.get("publish_date")]
    from collections import Counter
    result["by_date"] = dict(Counter(dates))
    
    return result


def _analyze_sentiment(data: List[Dict]) -> Dict[str, Any]:
    """æƒ…æ„Ÿåˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„æƒ…æ„Ÿåˆ†ææ¨¡å‹
    positive_keywords = ["å¥½", "ä¼˜", "å¢é•¿", "æå‡", "æˆåŠŸ", "åˆ©å¥½"]
    negative_keywords = ["å·®", "ä¸‹é™", "å¤±è´¥", "é£é™©", "å±æœº", "åˆ©ç©º"]
    
    positive_count = sum(
        1 for d in data
        if any(kw in d.get("content", "").lower() or kw in d.get("title", "").lower()
              for kw in positive_keywords)
    )
    negative_count = sum(
        1 for d in data
        if any(kw in d.get("content", "").lower() or kw in d.get("title", "").lower()
              for kw in negative_keywords)
    )
    
    total = len(data)
    return {
        "positive": positive_count,
        "negative": negative_count,
        "neutral": total - positive_count - negative_count,
        "positive_rate": positive_count / total * 100 if total > 0 else 0,
        "negative_rate": negative_count / total * 100 if total > 0 else 0
    }


def _analyze_keywords(data: List[Dict], limit: int) -> List[Dict[str, Any]]:
    """å…³é”®è¯åˆ†æ"""
    all_content = " ".join(d.get("content", "") + " " + d.get("title", "") for d in data)
    keywords = analyzer.extract_keywords(all_content, limit)
    
    return [
        {"keyword": kw, "frequency": all_content.lower().count(kw.lower())}
        for kw in keywords
    ]


def _analyze_trend(data: List[Dict], group_by: Optional[str]) -> Dict[str, Any]:
    """è¶‹åŠ¿åˆ†æ"""
    # æŒ‰æ—¶é—´æ’åº
    sorted_data = sorted(
        data,
        key=lambda x: x.get("publish_date", ""),
        reverse=False
    )
    
    # è®¡ç®—è¶‹åŠ¿æ–¹å‘
    if len(sorted_data) >= 2:
        early_count = len(sorted_data[:len(sorted_data)//2])
        late_count = len(sorted_data[len(sorted_data)//2:])
        
        if late_count > early_count * 1.2:
            trend_direction = "ä¸Šå‡"
        elif late_count < early_count * 0.8:
            trend_direction = "ä¸‹é™"
        else:
            trend_direction = "ç¨³å®š"
    else:
        trend_direction = "æ•°æ®ä¸è¶³"
    
    return {
        "direction": trend_direction,
        "data_points": len(sorted_data),
        "time_series": [
            {
                "date": d.get("publish_date", "")[:10],
                "count": 1
            }
            for d in sorted_data
        ]
    }


def _calculate_custom_metrics(data: List[Dict], metrics_config: Dict[str, Any]) -> Dict[str, Any]:
    """è®¡ç®—è‡ªå®šä¹‰æŒ‡æ ‡"""
    results = {}
    
    for metric_name, metric_config in metrics_config.items():
        metric_type = metric_config.get("type")
        
        if metric_type == "count":
            # è®¡æ•°æŒ‡æ ‡
            filter_condition = metric_config.get("filter", {})
            filtered = _apply_config_filter(data, filter_condition)
            results[metric_name] = len(filtered)
        
        elif metric_type == "average":
            # å¹³å‡å€¼æŒ‡æ ‡
            field = metric_config.get("field")
            values = [float(d.get(field, 0)) for d in data if d.get(field)]
            results[metric_name] = sum(values) / len(values) if values else 0
        
        elif metric_type == "sum":
            # æ±‚å’ŒæŒ‡æ ‡
            field = metric_config.get("field")
            values = [float(d.get(field, 0)) for d in data if d.get(field)]
            results[metric_name] = sum(values)
    
    return results


def _get_previous_period_data(all_data: List[Dict], time_range: Optional[str]) -> List[Dict]:
    """è·å–ä¸Šä¸€å‘¨æœŸçš„æ•°æ®ç”¨äºå¯¹æ¯”"""
    if not time_range:
        return []
    
    today = datetime.now()
    if time_range == "week":
        start = today - timedelta(days=14)
        end = today - timedelta(days=7)
    elif time_range == "month":
        start = today - timedelta(days=60)
        end = today - timedelta(days=30)
    else:
        return []
    
    return [
        d for d in all_data
        if start.strftime("%Y-%m-%d") <= d.get("publish_date", "")[:10] <= end.strftime("%Y-%m-%d")
    ]


def _apply_config_filter(data: List[Dict], config: Dict[str, Any]) -> List[Dict]:
    """åº”ç”¨é…ç½®ç­›é€‰"""
    filtered = data
    
    if config.get("categories"):
        filtered = [d for d in filtered if d.get("category") in config["categories"]]
    
    if config.get("sources"):
        filtered = [d for d in filtered if d.get("source") in config["sources"]]
    
    if config.get("keywords"):
        filtered = [
            d for d in filtered
            if any(kw in d.get("content", "").lower() or kw in d.get("title", "").lower()
                  for kw in config["keywords"])
        ]
    
    if config.get("start_date") and config.get("end_date"):
        filtered = [
            d for d in filtered
            if config["start_date"] <= d.get("publish_date", "")[:10] <= config["end_date"]
        ]
    
    return filtered


def _sort_results(results: Dict[str, Any], sort_by: str, sort_order: str) -> Dict[str, Any]:
    """æ’åºç»“æœ"""
    # è¿™é‡Œå¯ä»¥æ ¹æ®éœ€è¦å®ç°æ’åºé€»è¾‘
    return results



"""
æ–‡ä»¶å¤„ç†ç»¼åˆæµ‹è¯•è„šæœ¬
"""

import os
import sys
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from processors.file_processor import FileProcessor
from processors.text_processor import TextProcessor
from processors.preprocessor import Preprocessor


def create_test_files():
    """åˆ›å»ºæµ‹è¯•æ–‡ä»¶"""
    test_dir = Path(__file__).parent / "test_files"
    test_dir.mkdir(exist_ok=True)
    
    # 1. åˆ›å»ºæ–‡æœ¬æ–‡ä»¶
    txt_file = test_dir / "test.txt"
    with open(txt_file, 'w', encoding='utf-8') as f:
        f.write("""è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬æ–‡ä»¶ã€‚
        
å®ƒåŒ…å«å¤šä¸ªæ®µè½ã€‚ç”¨äºæµ‹è¯•æ–‡ä»¶å¤„ç†å™¨çš„åŠŸèƒ½ã€‚

This is a test text file.
It contains multiple paragraphs. For testing the file processor.

æµ‹è¯•å†…å®¹åŒ…æ‹¬ï¼š
1. ä¸­æ–‡æ–‡æœ¬
2. English text
3. æ•°å­— 123456
4. æ ‡ç‚¹ç¬¦å·ï¼ï¼Ÿã€‚ï¼Œ
""")
    
    # 2. åˆ›å»ºMarkdownæ–‡ä»¶
    md_file = test_dir / "test.md"
    with open(md_file, 'w', encoding='utf-8') as f:
        f.write("""# æµ‹è¯•Markdownæ–‡ä»¶

## ç¬¬ä¸€èŠ‚

è¿™æ˜¯æ­£æ–‡å†…å®¹ã€‚

## ç¬¬äºŒèŠ‚

- åˆ—è¡¨é¡¹1
- åˆ—è¡¨é¡¹2
- åˆ—è¡¨é¡¹3

```python
def hello():
    print("Hello, World!")
```
""")
    
    # 3. åˆ›å»ºPythonä»£ç æ–‡ä»¶
    py_file = test_dir / "test.py"
    with open(py_file, 'w', encoding='utf-8') as f:
        f.write("""#!/usr/bin/env python3
# -*- coding: utf-8 -*-

\"\"\"
æµ‹è¯•Pythonæ–‡ä»¶
\"\"\"

def test_function():
    \"\"\"æµ‹è¯•å‡½æ•°\"\"\"
    print("è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å‡½æ•°")
    return True

if __name__ == "__main__":
    test_function()
""")
    
    # 4. åˆ›å»ºJSONæ–‡ä»¶
    json_file = test_dir / "test.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        f.write("""{
    "name": "æµ‹è¯•æ•°æ®",
    "type": "JSON",
    "items": [
        {"id": 1, "value": "é¡¹ç›®1"},
        {"id": 2, "value": "é¡¹ç›®2"}
    ]
}""")
    
    print(f"âœ… æµ‹è¯•æ–‡ä»¶å·²åˆ›å»ºåœ¨: {test_dir}")
    return test_dir


def test_file_processor():
    """æµ‹è¯•æ–‡ä»¶å¤„ç†å™¨"""
    print("\n" + "="*70)
    print("  æµ‹è¯• 1: æ–‡ä»¶å¤„ç†å™¨")
    print("="*70)
    
    processor = FileProcessor()
    
    # æ˜¾ç¤ºæ”¯æŒçš„æ ¼å¼
    formats = processor.get_supported_formats_info()
    print(f"\næ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {formats['total_formats']}ç§")
    for category, info in formats['categories'].items():
        print(f"  {category:12s}: {info['count']:2d}ç§")
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_dir = create_test_files()
    
    # æµ‹è¯•æ¯ä¸ªæ–‡ä»¶
    print(f"\nå¼€å§‹å¤„ç†æµ‹è¯•æ–‡ä»¶...")
    for file in test_dir.glob("*"):
        if file.is_file():
            print(f"\n{'â”€'*70}")
            result = processor.process_file(str(file))
            
            if result.get("success"):
                print(f"âœ… æ–‡ä»¶: {result['file_name']}")
                print(f"   ç±»å‹: {result['file_type']} | ç±»åˆ«: {result['file_category']}")
                print(f"   å¤§å°: {result['file_size']}å­—èŠ‚")
                print(f"   å†…å®¹: {result['content_length']}å­—ç¬¦")
                print(f"   é¢„è§ˆ: {result['content'][:100]}...")
            else:
                print(f"âŒ å¤„ç†å¤±è´¥: {result.get('error')}")
    
    print(f"\n{'='*70}")
    print("âœ… æ–‡ä»¶å¤„ç†å™¨æµ‹è¯•å®Œæˆ")


def test_text_processor():
    """æµ‹è¯•æ–‡æœ¬å¤„ç†å™¨"""
    print("\n" + "="*70)
    print("  æµ‹è¯• 2: æ–‡æœ¬å¤„ç†å™¨")
    print("="*70)
    
    processor = TextProcessor()
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚
å®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚

è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚

Machine learning is a subset of artificial intelligence.
It focuses on the development of computer programs that can access data and use it to learn for themselves.
"""
    
    print(f"\nåŸæ–‡é•¿åº¦: {len(test_text)}å­—ç¬¦\n")
    
    # æµ‹è¯•ä¸åŒçš„åˆ†å—æ–¹æ³•
    methods = ["fixed", "sentence", "semantic"]
    
    for method in methods:
        print(f"\n{'â”€'*70}")
        print(f"åˆ†å—æ–¹æ³•: {method}")
        print(f"{'â”€'*70}")
        
        chunks = processor.split_text(test_text, chunk_size=100, method=method)
        
        print(f"ç”Ÿæˆå—æ•°: {len(chunks)}")
        for chunk in chunks[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"\nå— {chunk['chunk_id']}:")
            print(f"  é•¿åº¦: {chunk['length']}å­—ç¬¦")
            print(f"  å†…å®¹: {chunk['content'][:60]}...")
    
    # æµ‹è¯•æ–‡æœ¬æ¸…æ´—
    print(f"\n{'â”€'*70}")
    print("æ–‡æœ¬æ¸…æ´—")
    print(f"{'â”€'*70}")
    
    dirty_text = """
è¿™æ˜¯    ä¸€æ®µ    æœ‰    å¤šä½™ç©ºç™½    çš„æ–‡æœ¬


å®ƒæœ‰å¤ªå¤šæ¢è¡Œç¬¦



éœ€è¦æ¸…æ´—
"""
    cleaned = processor.clean_text(dirty_text)
    print(f"åŸæ–‡: '{dirty_text}'")
    print(f"æ¸…æ´—å: '{cleaned}'")
    
    print(f"\n{'='*70}")
    print("âœ… æ–‡æœ¬å¤„ç†å™¨æµ‹è¯•å®Œæˆ")


def test_preprocessor():
    """æµ‹è¯•é¢„å¤„ç†å™¨"""
    print("\n" + "="*70)
    print("  æµ‹è¯• 3: é¢„å¤„ç†å™¨")
    print("="*70)
    
    preprocessor = Preprocessor()
    
    # æµ‹è¯•æ¡ˆä¾‹
    test_cases = [
        {
            "name": "æ­£å¸¸æ–‡æœ¬",
            "text": "è¿™æ˜¯ä¸€æ®µæ­£å¸¸çš„ä¸­æ–‡æ–‡æœ¬ã€‚It also contains English. ç”¨äºæµ‹è¯•é¢„å¤„ç†åŠŸèƒ½ã€‚"
        },
        {
            "name": "åŒ…å«HTMLæ ‡ç­¾",
            "text": "<p>è¿™æ˜¯<strong>HTML</strong>æ–‡æœ¬</p>"
        },
        {
            "name": "åŒ…å«URL",
            "text": "è®¿é—®ç½‘ç«™ https://example.com è·å–æ›´å¤šä¿¡æ¯"
        },
        {
            "name": "åŒ…å«é‚®ç®±",
            "text": "è”ç³»æˆ‘ä»¬: contact@example.com"
        },
        {
            "name": "æ–‡æœ¬è¿‡çŸ­",
            "text": "çŸ­æ–‡æœ¬"
        }
    ]
    
    for i, case in enumerate(test_cases, 1):
        print(f"\n{'â”€'*70}")
        print(f"æµ‹è¯•æ¡ˆä¾‹ {i}: {case['name']}")
        print(f"{'â”€'*70}")
        
        result = preprocessor.preprocess(case['text'])
        
        print(f"åŸæ–‡: {result['original_text']}")
        print(f"å¤„ç†å: {result['processed_text']}")
        print(f"é•¿åº¦: {result['original_length']} â†’ {result['final_length']}")
        print(f"é‡å¤: {result['is_duplicate']}")
        print(f"éªŒè¯: {'âœ… é€šè¿‡' if result['passed_validation'] else 'âŒ æœªé€šè¿‡'}")
        
        if result['warnings']:
            print(f"âš ï¸  è­¦å‘Š: {', '.join(result['warnings'])}")
    
    # æµ‹è¯•é‡å¤æ£€æµ‹
    print(f"\n{'â”€'*70}")
    print("é‡å¤æ£€æµ‹æµ‹è¯•")
    print(f"{'â”€'*70}")
    
    text1 = "è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬"
    text2 = "è¿™æ˜¯ç¬¬äºŒæ®µæ–‡æœ¬"
    text3 = "è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬"  # ä¸text1é‡å¤
    
    result1 = preprocessor.preprocess(text1)
    print(f"æ–‡æœ¬1: é‡å¤={result1['is_duplicate']}")
    
    result2 = preprocessor.preprocess(text2)
    print(f"æ–‡æœ¬2: é‡å¤={result2['is_duplicate']}")
    
    result3 = preprocessor.preprocess(text3)
    print(f"æ–‡æœ¬3: é‡å¤={result3['is_duplicate']} (åº”è¯¥æ£€æµ‹åˆ°é‡å¤)")
    
    print(f"\n{'='*70}")
    print("âœ… é¢„å¤„ç†å™¨æµ‹è¯•å®Œæˆ")


def test_integrated_workflow():
    """æµ‹è¯•é›†æˆå·¥ä½œæµ"""
    print("\n" + "="*70)
    print("  æµ‹è¯• 4: é›†æˆå·¥ä½œæµ")
    print("="*70)
    
    # åˆå§‹åŒ–æ‰€æœ‰å¤„ç†å™¨
    file_processor = FileProcessor()
    text_processor = TextProcessor()
    preprocessor = Preprocessor()
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_dir = create_test_files()
    test_file = test_dir / "test.txt"
    
    print(f"\nå®Œæ•´å¤„ç†æµç¨‹æ¼”ç¤º:")
    print(f"{'â”€'*70}")
    
    # æ­¥éª¤1: æ–‡ä»¶å¤„ç†
    print("\næ­¥éª¤1: æå–æ–‡ä»¶å†…å®¹")
    file_result = file_processor.process_file(str(test_file))
    if not file_result.get("success"):
        print("âŒ æ–‡ä»¶å¤„ç†å¤±è´¥")
        return
    
    content = file_result["content"]
    print(f"âœ… æå–äº† {len(content)} å­—ç¬¦")
    
    # æ­¥éª¤2: æ–‡æœ¬é¢„å¤„ç†
    print("\næ­¥éª¤2: é¢„å¤„ç†æ–‡æœ¬")
    preprocess_result = preprocessor.preprocess(content)
    processed_content = preprocess_result["processed_text"]
    print(f"âœ… é¢„å¤„ç†å®Œæˆ: {preprocess_result['original_length']} â†’ {preprocess_result['final_length']}å­—ç¬¦")
    print(f"   éªŒè¯: {'é€šè¿‡' if preprocess_result['passed_validation'] else 'æœªé€šè¿‡'}")
    
    # æ­¥éª¤3: æ–‡æœ¬åˆ†å—
    print("\næ­¥éª¤3: æ–‡æœ¬åˆ†å—")
    chunks = text_processor.split_text(processed_content, chunk_size=200)
    print(f"âœ… ç”Ÿæˆäº† {len(chunks)} ä¸ªæ–‡æœ¬å—")
    
    for i, chunk in enumerate(chunks[:3], 1):
        print(f"\n  å— {i}:")
        print(f"    é•¿åº¦: {chunk['length']}å­—ç¬¦")
        print(f"    é¢„è§ˆ: {chunk['content'][:60]}...")
    
    # æ­¥éª¤4: æå–å…³é”®è¯
    print("\næ­¥éª¤4: æå–å…³é”®è¯")
    keywords = text_processor.extract_keywords(processed_content, top_k=5)
    print(f"âœ… å…³é”®è¯: {', '.join(keywords)}")
    
    print(f"\n{'='*70}")
    print("âœ… é›†æˆå·¥ä½œæµæµ‹è¯•å®Œæˆ")
    print(f"{'='*70}")
    
    print(f"\nğŸ“Š å¤„ç†æµç¨‹æ€»ç»“:")
    print(f"  æ–‡ä»¶ â†’ å†…å®¹æå– â†’ é¢„å¤„ç† â†’ åˆ†å— â†’ å…³é”®è¯æå–")
    print(f"  {file_result['file_name']} â†’ {len(content)}å­—ç¬¦ â†’ {len(processed_content)}å­—ç¬¦ â†’ {len(chunks)}å— â†’ {len(keywords)}ä¸ªå…³é”®è¯")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\n")
    print("â•”" + "â•"*68 + "â•—")
    print("â•‘" + " "*68 + "â•‘")
    print("â•‘" + "  ğŸ§ª RAGæ–‡ä»¶å¤„ç†ç³»ç»Ÿ - ç»¼åˆæµ‹è¯•".center(68) + "â•‘")
    print("â•‘" + " "*68 + "â•‘")
    print("â•š" + "â•"*68 + "â•")
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_file_processor()
        test_text_processor()
        test_preprocessor()
        test_integrated_workflow()
        
        # æ€»ç»“
        print("\n\n")
        print("â•”" + "â•"*68 + "â•—")
        print("â•‘" + " "*68 + "â•‘")
        print("â•‘" + "  ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼".center(68) + "â•‘")
        print("â•‘" + " "*68 + "â•‘")
        print("â•š" + "â•"*68 + "â•")
        
        print("\nâœ… æ–‡ä»¶å¤„ç†å¼•æ“å¼€å‘å®Œæˆï¼")
        print("\nå·²å®ç°åŠŸèƒ½:")
        print("  âœ… æ–‡ä»¶å¤„ç†å™¨ (æ”¯æŒ10+ç§æ–‡ä»¶æ ¼å¼)")
        print("  âœ… æ–‡æœ¬å¤„ç†å™¨ (åˆ†å—ã€æ¸…æ´—ã€å…³é”®è¯æå–)")
        print("  âœ… é¢„å¤„ç†å™¨ (å››é¡¹é¢„å¤„ç†åŠŸèƒ½)")
        print("  âœ… é›†æˆå·¥ä½œæµ (å®Œæ•´å¤„ç†æµç¨‹)")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()





"""
æ–‡æœ¬å¤„ç†å™¨ - æ–‡æœ¬åˆ†å—å’Œé¢„å¤„ç†
"""

import re
from typing import List, Dict, Any, Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class TextProcessor:
    """
    æ–‡æœ¬å¤„ç†å™¨
    
    è´Ÿè´£ï¼š
    1. æ–‡æœ¬åˆ†å—ï¼ˆChunkingï¼‰
    2. æ–‡æœ¬æ¸…æ´—
    3. æ–‡æœ¬æ ‡å‡†åŒ–
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–æ–‡æœ¬å¤„ç†å™¨"""
        self.config = config or self._get_default_config()
        logger.info("ğŸ“ æ–‡æœ¬å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "chunk_size": 512,
            "chunk_overlap": 50,
            "chunk_method": "semantic",  # fixed, sentence, semantic
            "min_chunk_size": 10,
            "max_chunk_size": 2000
        }
    
    def split_text(
        self,
        text: str,
        chunk_size: Optional[int] = None,
        chunk_overlap: Optional[int] = None,
        method: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """
        åˆ†å‰²æ–‡æœ¬ä¸ºå—
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            chunk_size: å—å¤§å°
            chunk_overlap: å—é‡å å¤§å°
            method: åˆ†å—æ–¹æ³•ï¼ˆfixed, sentence, semanticï¼‰
            
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        chunk_size = chunk_size or self.config["chunk_size"]
        chunk_overlap = chunk_overlap or self.config["chunk_overlap"]
        method = method or self.config["chunk_method"]
        
        logger.info(f"ğŸ“„ åˆ†å‰²æ–‡æœ¬: {len(text)}å­—ç¬¦ â†’ {chunk_size}å­—ç¬¦/å—")
        
        if method == "fixed":
            chunks = self._split_fixed(text, chunk_size, chunk_overlap)
        elif method == "sentence":
            chunks = self._split_by_sentence(text, chunk_size, chunk_overlap)
        elif method == "semantic":
            chunks = self._split_semantic(text, chunk_size, chunk_overlap)
        else:
            chunks = self._split_fixed(text, chunk_size, chunk_overlap)
        
        # æ·»åŠ å…ƒæ•°æ®
        result = []
        for i, chunk in enumerate(chunks):
            result.append({
                "chunk_id": i,
                "content": chunk,
                "length": len(chunk),
                "start_pos": text.find(chunk),
                "method": method
            })
        
        logger.info(f"âœ… åˆ†å‰²å®Œæˆ: {len(result)}ä¸ªå—")
        return result
    
    def _split_fixed(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """å›ºå®šå¤§å°åˆ†å—"""
        chunks = []
        start = 0
        
        while start < len(text):
            end = start + chunk_size
            chunk = text[start:end]
            
            if chunk.strip():
                chunks.append(chunk)
            
            start = end - overlap
        
        return chunks
    
    def _split_by_sentence(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """æŒ‰å¥å­åˆ†å—"""
        # åˆ†å¥ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
        sentences = re.split(r'([ã€‚ï¼ï¼Ÿ.!?]\s*)', text)
        sentences = [''.join(i) for i in zip(sentences[0::2], sentences[1::2] + [''])]
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= chunk_size:
                current_chunk += sentence
            else:
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                current_chunk = sentence
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _split_semantic(self, text: str, chunk_size: int, overlap: int) -> List[str]:
        """è¯­ä¹‰åˆ†å—ï¼ˆåŸºäºæ®µè½å’Œå¥å­ï¼‰"""
        # å…ˆæŒ‰æ®µè½åˆ†
        paragraphs = text.split('\n\n')
        
        chunks = []
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # å¦‚æœå½“å‰å—+æ®µè½ä¸è¶…è¿‡å¤§å°ï¼Œç›´æ¥æ·»åŠ 
            if len(current_chunk) + len(para) <= chunk_size:
                current_chunk += "\n\n" + para if current_chunk else para
            else:
                # ä¿å­˜å½“å‰å—
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                
                # å¦‚æœæ®µè½æœ¬èº«å¾ˆé•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å¥
                if len(para) > chunk_size:
                    sub_chunks = self._split_by_sentence(para, chunk_size, overlap)
                    chunks.extend(sub_chunks)
                    current_chunk = ""
                else:
                    current_chunk = para
        
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def clean_text(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡æœ¬
        
        - åˆ é™¤å¤šä½™ç©ºç™½
        - åˆ é™¤ç‰¹æ®Šå­—ç¬¦
        - æ ‡å‡†åŒ–æ¢è¡Œç¬¦
        """
        # æ ‡å‡†åŒ–æ¢è¡Œç¬¦
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        
        # åˆ é™¤å¤šä½™ç©ºç™½
        text = re.sub(r' +', ' ', text)
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        # åˆ é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
        lines = [line.strip() for line in text.split('\n')]
        text = '\n'.join(lines)
        
        return text.strip()
    
    def normalize_text(self, text: str) -> str:
        """
        æ ‡å‡†åŒ–æ–‡æœ¬
        
        - ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
        - åˆ é™¤URL
        - åˆ é™¤é‚®ç®±
        """
        # åˆ é™¤URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # åˆ é™¤é‚®ç®±
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', text)
        
        return text
    
    def extract_keywords(self, text: str, top_k: int = 10) -> List[str]:
        """
        æå–å…³é”®è¯ï¼ˆç®€å•å®ç°ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            top_k: è¿”å›å‰Kä¸ªå…³é”®è¯
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        # ç®€å•å®ç°ï¼šåŸºäºè¯é¢‘
        # TODO: å¯ä»¥ä½¿ç”¨jiebaã€TF-IDFç­‰æ›´é«˜çº§çš„æ–¹æ³•
        
        # åˆ†è¯ï¼ˆç®€å•çš„åŸºäºç©ºæ ¼ï¼‰
        words = re.findall(r'\b\w+\b', text.lower())
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = {}
        for word in words:
            if len(word) > 2:  # åªç»Ÿè®¡é•¿åº¦>2çš„è¯
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # æ’åºå¹¶è¿”å›
        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
        return [word for word, freq in sorted_words[:top_k]]


def test_text_processor():
    """æµ‹è¯•æ–‡æœ¬å¤„ç†å™¨"""
    print("="*70)
    print("  æ–‡æœ¬å¤„ç†å™¨æµ‹è¯•")
    print("="*70)
    
    processor = TextProcessor()
    
    # æµ‹è¯•æ–‡æœ¬
    test_text = """
    è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬ã€‚å®ƒåŒ…å«å¤šä¸ªå¥å­ã€‚ç”¨äºæµ‹è¯•åˆ†å—åŠŸèƒ½ã€‚
    
    è¿™æ˜¯ç¬¬äºŒæ®µæ–‡æœ¬ï¼å®ƒä¹ŸåŒ…å«å¤šä¸ªå¥å­ï¼Ÿæµ‹è¯•ä¸åŒçš„åˆ†å—æ–¹æ³•ã€‚
    
    This is the third paragraph. It contains English sentences. For testing purposes.
    """
    
    print(f"\nåŸå§‹æ–‡æœ¬é•¿åº¦: {len(test_text)}å­—ç¬¦\n")
    
    # æµ‹è¯•å›ºå®šåˆ†å—
    print("1. å›ºå®šå¤§å°åˆ†å—:")
    chunks = processor.split_text(test_text, chunk_size=50, method="fixed")
    for chunk in chunks[:3]:
        print(f"   å—{chunk['chunk_id']}: {chunk['length']}å­—ç¬¦")
    
    # æµ‹è¯•å¥å­åˆ†å—
    print("\n2. æŒ‰å¥å­åˆ†å—:")
    chunks = processor.split_text(test_text, chunk_size=100, method="sentence")
    for chunk in chunks:
        print(f"   å—{chunk['chunk_id']}: {chunk['length']}å­—ç¬¦")
    
    # æµ‹è¯•æ–‡æœ¬æ¸…æ´—
    print("\n3. æ–‡æœ¬æ¸…æ´—:")
    dirty_text = "è¿™æ˜¯    å¤šä½™ç©ºç™½\n\n\n\nçš„æ–‡æœ¬"
    clean = processor.clean_text(dirty_text)
    print(f"   åŸæ–‡: '{dirty_text}'")
    print(f"   æ¸…æ´—å: '{clean}'")
    
    # æµ‹è¯•å…³é”®è¯æå–
    print("\n4. å…³é”®è¯æå–:")
    keywords = processor.extract_keywords(test_text, top_k=5)
    print(f"   å…³é”®è¯: {keywords}")
    
    print("\nâœ… æ–‡æœ¬å¤„ç†å™¨æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_text_processor()







"""
é¢„å¤„ç†å™¨ - å››é¡¹é¢„å¤„ç†åŠŸèƒ½

1. æ•°æ®æ¸…æ´—
2. æ ‡å‡†åŒ–å¤„ç†
3. å»é‡éªŒè¯
4. çœŸå®æ€§éªŒè¯
"""

import re
import hashlib
from typing import List, Dict, Any, Optional, Set
from datetime import datetime
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class Preprocessor:
    """
    é¢„å¤„ç†å™¨
    
    å®ç°å››é¡¹é¢„å¤„ç†ï¼š
    1. æ•°æ®æ¸…æ´— (Cleaning)
    2. æ ‡å‡†åŒ–å¤„ç† (Normalization)
    3. å»é‡éªŒè¯ (Deduplication)
    4. çœŸå®æ€§éªŒè¯ (Validation)
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """åˆå§‹åŒ–é¢„å¤„ç†å™¨"""
        self.config = config or self._get_default_config()
        self.seen_hashes: Set[str] = set()  # ç”¨äºå»é‡
        
        logger.info("ğŸ”§ é¢„å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   å¯ç”¨åŠŸèƒ½: æ¸…æ´—={self.config['enable_cleaning']}, "
                   f"æ ‡å‡†åŒ–={self.config['enable_normalization']}, "
                   f"å»é‡={self.config['enable_deduplication']}, "
                   f"éªŒè¯={self.config['enable_validation']}")
    
    def _get_default_config(self) -> Dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            # æ¸…æ´—é…ç½®
            "enable_cleaning": True,
            "remove_html": True,
            "remove_urls": True,
            "remove_emails": True,
            "remove_special_chars": False,
            "normalize_whitespace": True,
            
            # æ ‡å‡†åŒ–é…ç½®
            "enable_normalization": True,
            "lowercase": False,  # ä¿ç•™å¤§å°å†™
            "remove_stopwords": False,  # ä¿ç•™åœç”¨è¯
            
            # å»é‡é…ç½®
            "enable_deduplication": True,
            "similarity_threshold": 0.95,
            "dedup_method": "hash",  # hash, embedding
            
            # éªŒè¯é…ç½®
            "enable_validation": True,
            "min_length": 10,
            "max_length": 50000,
            "check_language": True,
            "allowed_languages": ["zh", "en"]
        }
    
    def preprocess(self, text: str, metadata: Optional[Dict] = None) -> Dict[str, Any]:
        """
        å®Œæ•´çš„é¢„å¤„ç†æµç¨‹
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            metadata: å…ƒæ•°æ®
            
        Returns:
            é¢„å¤„ç†ç»“æœ
        """
        logger.info(f"\nğŸ”§ å¼€å§‹é¢„å¤„ç†: {len(text)}å­—ç¬¦")
        
        result = {
            "original_text": text,
            "original_length": len(text),
            "processed_text": text,
            "metadata": metadata or {},
            "preprocessing_steps": [],
            "warnings": [],
            "passed_validation": True,
            "is_duplicate": False
        }
        
        # 1. æ•°æ®æ¸…æ´—
        if self.config["enable_cleaning"]:
            cleaned_text, cleaning_info = self.clean(text)
            result["processed_text"] = cleaned_text
            result["preprocessing_steps"].append({
                "step": "cleaning",
                "info": cleaning_info
            })
            logger.info(f"   âœ… æ¸…æ´—å®Œæˆ: {len(cleaned_text)}å­—ç¬¦")
        
        # 2. æ ‡å‡†åŒ–
        if self.config["enable_normalization"]:
            normalized_text, norm_info = self.normalize(result["processed_text"])
            result["processed_text"] = normalized_text
            result["preprocessing_steps"].append({
                "step": "normalization",
                "info": norm_info
            })
            logger.info(f"   âœ… æ ‡å‡†åŒ–å®Œæˆ: {len(normalized_text)}å­—ç¬¦")
        
        # 3. å»é‡éªŒè¯
        if self.config["enable_deduplication"]:
            is_duplicate, dedup_info = self.check_duplicate(result["processed_text"])
            result["is_duplicate"] = is_duplicate
            result["preprocessing_steps"].append({
                "step": "deduplication",
                "info": dedup_info
            })
            if is_duplicate:
                logger.warning(f"   âš ï¸  æ£€æµ‹åˆ°é‡å¤å†…å®¹")
                result["warnings"].append("å†…å®¹é‡å¤")
        
        # 4. çœŸå®æ€§éªŒè¯
        if self.config["enable_validation"]:
            is_valid, validation_info = self.validate(result["processed_text"])
            result["passed_validation"] = is_valid
            result["preprocessing_steps"].append({
                "step": "validation",
                "info": validation_info
            })
            if not is_valid:
                logger.warning(f"   âš ï¸  éªŒè¯æœªé€šè¿‡: {validation_info.get('reason')}")
                result["warnings"].append(f"éªŒè¯å¤±è´¥: {validation_info.get('reason')}")
            else:
                logger.info(f"   âœ… éªŒè¯é€šè¿‡")
        
        result["final_length"] = len(result["processed_text"])
        result["processed_at"] = datetime.now().isoformat()
        
        logger.info(f"âœ… é¢„å¤„ç†å®Œæˆ: {result['original_length']} â†’ {result['final_length']}å­—ç¬¦")
        
        return result
    
    def clean(self, text: str) -> tuple[str, Dict]:
        """
        æ•°æ®æ¸…æ´—
        
        Returns:
            (æ¸…æ´—åçš„æ–‡æœ¬, æ¸…æ´—ä¿¡æ¯)
        """
        original_length = len(text)
        cleaned_text = text
        operations = []
        
        # åˆ é™¤HTMLæ ‡ç­¾
        if self.config.get("remove_html"):
            before = len(cleaned_text)
            cleaned_text = re.sub(r'<[^>]+>', '', cleaned_text)
            if len(cleaned_text) < before:
                operations.append("removed_html")
        
        # åˆ é™¤URL
        if self.config.get("remove_urls"):
            before = len(cleaned_text)
            cleaned_text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', cleaned_text)
            if len(cleaned_text) < before:
                operations.append("removed_urls")
        
        # åˆ é™¤é‚®ç®±
        if self.config.get("remove_emails"):
            before = len(cleaned_text)
            cleaned_text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', cleaned_text)
            if len(cleaned_text) < before:
                operations.append("removed_emails")
        
        # åˆ é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™åŸºæœ¬æ ‡ç‚¹ï¼‰
        if self.config.get("remove_special_chars"):
            before = len(cleaned_text)
            cleaned_text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼šã€]', '', cleaned_text)
            if len(cleaned_text) < before:
                operations.append("removed_special_chars")
        
        # æ ‡å‡†åŒ–ç©ºç™½
        if self.config.get("normalize_whitespace"):
            # å¤šä¸ªç©ºæ ¼ â†’ å•ä¸ªç©ºæ ¼
            cleaned_text = re.sub(r' +', ' ', cleaned_text)
            # å¤šä¸ªæ¢è¡Œ â†’ æœ€å¤š2ä¸ªæ¢è¡Œ
            cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text)
            # åˆ é™¤è¡Œé¦–è¡Œå°¾ç©ºç™½
            lines = [line.strip() for line in cleaned_text.split('\n')]
            cleaned_text = '\n'.join(lines)
            operations.append("normalized_whitespace")
        
        cleaned_text = cleaned_text.strip()
        
        info = {
            "original_length": original_length,
            "cleaned_length": len(cleaned_text),
            "reduction": original_length - len(cleaned_text),
            "operations": operations
        }
        
        return cleaned_text, info
    
    def normalize(self, text: str) -> tuple[str, Dict]:
        """
        æ ‡å‡†åŒ–å¤„ç†
        
        Returns:
            (æ ‡å‡†åŒ–åçš„æ–‡æœ¬, æ ‡å‡†åŒ–ä¿¡æ¯)
        """
        normalized_text = text
        operations = []
        
        # å°å†™åŒ–ï¼ˆå¯é€‰ï¼‰
        if self.config.get("lowercase"):
            normalized_text = normalized_text.lower()
            operations.append("lowercased")
        
        # åˆ é™¤åœç”¨è¯ï¼ˆå¯é€‰ï¼‰
        if self.config.get("remove_stopwords"):
            # TODO: å®ç°åœç”¨è¯åˆ é™¤
            # éœ€è¦åœç”¨è¯è¡¨
            operations.append("removed_stopwords")
        
        # ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·ï¼ˆä¸­è‹±æ–‡ï¼‰
        # ä¸­æ–‡æ ‡ç‚¹ â†’ è‹±æ–‡æ ‡ç‚¹
        punctuation_map = {
            'ï¼Œ': ',',
            'ã€‚': '.',
            'ï¼': '!',
            'ï¼Ÿ': '?',
            'ï¼›': ';',
            'ï¼š': ':',
            'ï¼ˆ': '(',
            'ï¼‰': ')',
            'ã€': '[',
            'ã€‘': ']',
            'ã€Š': '<',
            'ã€‹': '>',
            '"': '"',
            '"': '"',
            ''': "'",
            ''': "'"
        }
        
        for cn, en in punctuation_map.items():
            if cn in normalized_text:
                normalized_text = normalized_text.replace(cn, en)
        
        if any(cn in text for cn in punctuation_map.keys()):
            operations.append("normalized_punctuation")
        
        info = {
            "operations": operations
        }
        
        return normalized_text, info
    
    def check_duplicate(self, text: str) -> tuple[bool, Dict]:
        """
        å»é‡éªŒè¯
        
        Returns:
            (æ˜¯å¦é‡å¤, å»é‡ä¿¡æ¯)
        """
        method = self.config.get("dedup_method", "hash")
        
        if method == "hash":
            # åŸºäºå“ˆå¸Œçš„å»é‡
            text_hash = self._compute_hash(text)
            
            is_duplicate = text_hash in self.seen_hashes
            
            if not is_duplicate:
                self.seen_hashes.add(text_hash)
            
            info = {
                "method": "hash",
                "hash": text_hash[:16],  # åªæ˜¾ç¤ºå‰16ä½
                "is_duplicate": is_duplicate,
                "total_seen": len(self.seen_hashes)
            }
            
        elif method == "embedding":
            # åŸºäºåµŒå…¥çš„å»é‡ï¼ˆTODO: éœ€è¦å‘é‡åŒ–ï¼‰
            info = {
                "method": "embedding",
                "is_duplicate": False,
                "note": "åµŒå…¥å»é‡å¾…å®ç°"
            }
            is_duplicate = False
        
        else:
            info = {
                "method": "none",
                "is_duplicate": False
            }
            is_duplicate = False
        
        return is_duplicate, info
    
    def validate(self, text: str) -> tuple[bool, Dict]:
        """
        çœŸå®æ€§éªŒè¯
        
        åŒ…æ‹¬ï¼š
        1. é•¿åº¦éªŒè¯
        2. è¯­è¨€éªŒè¯
        3. å†…å®¹è´¨é‡éªŒè¯
        
        Returns:
            (æ˜¯å¦é€šè¿‡éªŒè¯, éªŒè¯ä¿¡æ¯)
        """
        checks = []
        warnings = []
        
        # 1. é•¿åº¦éªŒè¯
        min_len = self.config.get("min_length", 10)
        max_len = self.config.get("max_length", 50000)
        
        if len(text) < min_len:
            checks.append({"check": "min_length", "passed": False})
            warnings.append(f"æ–‡æœ¬è¿‡çŸ­({len(text)} < {min_len})")
        elif len(text) > max_len:
            checks.append({"check": "max_length", "passed": False})
            warnings.append(f"æ–‡æœ¬è¿‡é•¿({len(text)} > {max_len})")
        else:
            checks.append({"check": "length", "passed": True})
        
        # 2. è¯­è¨€éªŒè¯ï¼ˆç®€å•å®ç°ï¼‰
        if self.config.get("check_language"):
            detected_lang = self._detect_language(text)
            allowed_langs = self.config.get("allowed_languages", ["zh", "en"])
            
            if detected_lang in allowed_langs or detected_lang == "mixed":
                checks.append({"check": "language", "passed": True, "detected": detected_lang})
            else:
                checks.append({"check": "language", "passed": False, "detected": detected_lang})
                warnings.append(f"è¯­è¨€ä¸æ”¯æŒ: {detected_lang}")
        
        # 3. å†…å®¹è´¨é‡éªŒè¯
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤ªå¤šé‡å¤å­—ç¬¦
        if self._has_too_many_repeats(text):
            checks.append({"check": "quality", "passed": False})
            warnings.append("åŒ…å«è¿‡å¤šé‡å¤å­—ç¬¦")
        else:
            checks.append({"check": "quality", "passed": True})
        
        # æ€»ä½“åˆ¤æ–­
        is_valid = all(check.get("passed", True) for check in checks)
        
        info = {
            "is_valid": is_valid,
            "checks": checks,
            "warnings": warnings,
            "reason": warnings[0] if warnings else "é€šè¿‡æ‰€æœ‰éªŒè¯"
        }
        
        return is_valid, info
    
    def _compute_hash(self, text: str) -> str:
        """è®¡ç®—æ–‡æœ¬å“ˆå¸Œ"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()
    
    def _detect_language(self, text: str) -> str:
        """
        æ£€æµ‹è¯­è¨€ï¼ˆç®€å•å®ç°ï¼‰
        
        Returns:
            "zh", "en", "mixed", "unknown"
        """
        # è®¡ç®—ä¸­æ–‡å­—ç¬¦å’Œè‹±æ–‡å­—ç¬¦çš„æ¯”ä¾‹
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        
        total_chars = len(re.findall(r'[\u4e00-\u9fffa-zA-Z]', text))
        
        if total_chars == 0:
            return "unknown"
        
        chinese_ratio = chinese_chars / total_chars
        english_ratio = english_chars / total_chars
        
        if chinese_ratio > 0.5:
            return "zh"
        elif english_ratio > 0.5:
            return "en"
        elif chinese_ratio > 0.1 and english_ratio > 0.1:
            return "mixed"
        else:
            return "unknown"
    
    def _has_too_many_repeats(self, text: str, threshold: int = 10) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šé‡å¤å­—ç¬¦"""
        # æŸ¥æ‰¾è¿ç»­é‡å¤çš„å­—ç¬¦
        pattern = r'(.)\1{' + str(threshold) + r',}'
        return bool(re.search(pattern, text))
    
    def reset_dedup_cache(self):
        """é‡ç½®å»é‡ç¼“å­˜"""
        self.seen_hashes.clear()
        logger.info("ğŸ”„ å»é‡ç¼“å­˜å·²é‡ç½®")


def test_preprocessor():
    """æµ‹è¯•é¢„å¤„ç†å™¨"""
    print("="*70)
    print("  é¢„å¤„ç†å™¨æµ‹è¯•")
    print("="*70)
    
    preprocessor = Preprocessor()
    
    # æµ‹è¯•æ–‡æœ¬
    test_cases = [
        {
            "name": "æ™®é€šæ–‡æœ¬",
            "text": "è¿™æ˜¯ä¸€æ®µæ­£å¸¸çš„ä¸­æ–‡æ–‡æœ¬ã€‚It also contains English. ç”¨äºæµ‹è¯•é¢„å¤„ç†åŠŸèƒ½ã€‚"
        },
        {
            "name": "åŒ…å«HTML",
            "text": "<p>è¿™æ˜¯<strong>HTML</strong>æ–‡æœ¬</p><a href='http://example.com'>é“¾æ¥</a>"
        },
        {
            "name": "åŒ…å«URLå’Œé‚®ç®±",
            "text": "è®¿é—® https://example.com æˆ–å‘é€é‚®ä»¶åˆ° test@example.com"
        },
        {
            "name": "æ–‡æœ¬è¿‡çŸ­",
            "text": "çŸ­"
        },
        {
            "name": "é‡å¤æ–‡æœ¬",
            "text": "è¿™æ˜¯ä¸€æ®µæ­£å¸¸çš„ä¸­æ–‡æ–‡æœ¬ã€‚It also contains English. ç”¨äºæµ‹è¯•é¢„å¤„ç†åŠŸèƒ½ã€‚"
        }
    ]
    
    for i, case in enumerate(test_cases, 1):
        print(f"\n{'='*70}")
        print(f"æµ‹è¯• {i}: {case['name']}")
        print(f"{'='*70}")
        
        result = preprocessor.preprocess(case['text'])
        
        print(f"åŸæ–‡: {case['text'][:50]}...")
        print(f"å¤„ç†å: {result['processed_text'][:50]}...")
        print(f"é•¿åº¦å˜åŒ–: {result['original_length']} â†’ {result['final_length']}")
        print(f"æ˜¯å¦é‡å¤: {result['is_duplicate']}")
        print(f"é€šè¿‡éªŒè¯: {result['passed_validation']}")
        
        if result['warnings']:
            print(f"âš ï¸  è­¦å‘Š: {', '.join(result['warnings'])}")
        
        print(f"\nå¤„ç†æ­¥éª¤:")
        for step in result['preprocessing_steps']:
            print(f"  - {step['step']}: {step['info']}")
    
    print(f"\n{'='*70}")
    print("âœ… é¢„å¤„ç†å™¨æµ‹è¯•å®Œæˆï¼")
    print(f"{'='*70}")


if __name__ == "__main__":
    test_preprocessor()





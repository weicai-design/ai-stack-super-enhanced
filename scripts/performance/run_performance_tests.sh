#!/bin/bash
# -*- coding: utf-8 -*-
# P3-403: æ€§èƒ½æµ‹è¯•è„šæœ¬

set -euo pipefail

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# é…ç½®
PROJECT_ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
SCRIPT_DIR="${PROJECT_ROOT}/scripts/performance"
REPORT_DIR="${PROJECT_ROOT}/reports/performance"

# é»˜è®¤å€¼
TEST_TYPE="${1:-all}"  # all/load/stress/stability/benchmark
BASE_URL="${2:-http://localhost:9000}"

echo -e "${GREEN}========================================${NC}"
echo -e "${GREEN}æ€§èƒ½æµ‹è¯•è„šæœ¬${NC}"
echo -e "${GREEN}========================================${NC}"
echo ""
echo "æµ‹è¯•ç±»å‹: ${TEST_TYPE}"
echo "åŸºç¡€URL: ${BASE_URL}"
echo ""

# æ£€æŸ¥ä¾èµ–
if ! command -v python3 &> /dev/null; then
    echo -e "${RED}é”™è¯¯: æœªæ‰¾åˆ° python3${NC}"
    exit 1
fi

# åˆ›å»ºæŠ¥å‘Šç›®å½•
mkdir -p "${REPORT_DIR}"

# è¿è¡Œæ€§èƒ½æµ‹è¯•
run_performance_test() {
    local test_type=$1
    echo -e "${BLUE}æ‰§è¡Œæ€§èƒ½æµ‹è¯•: ${test_type}${NC}"
    
    python3 << EOF
import asyncio
import sys
import json
sys.path.insert(0, "${PROJECT_ROOT}")

from tests.performance.test_performance_suite import PerformanceTestSuite

async def main():
    suite = PerformanceTestSuite(base_url="${BASE_URL}")
    
    try:
        if "${test_type}" == "all" or "${test_type}" == "load":
            print("\\nğŸ“Š è´Ÿè½½æµ‹è¯•")
            result = await suite.load_test(
                endpoint="/health",
                concurrent_users=10,
                requests_per_user=10,
            )
            print(f"QPS: {result.qps:.2f}")
            print(f"æˆåŠŸç‡: {result.success_rate:.2f}%")
            print(f"å¹³å‡å“åº”æ—¶é—´: {result.avg_response_time:.2f}ms")
        
        if "${test_type}" == "all" or "${test_type}" == "stress":
            print("\\nğŸ“Š å‹åŠ›æµ‹è¯•")
            results = await suite.stress_test(
                endpoint="/health",
                initial_users=10,
                max_users=50,
                step=10,
            )
            print(f"å®Œæˆ {len(results)} ä¸ªå‹åŠ›çº§åˆ«æµ‹è¯•")
        
        if "${test_type}" == "all" or "${test_type}" == "stability":
            print("\\nğŸ“Š ç¨³å®šæ€§æµ‹è¯•")
            result = await suite.stability_test(
                endpoint="/health",
                duration_seconds=60,
                requests_per_second=10,
            )
            print(f"QPS: {result.qps:.2f}")
            print(f"æˆåŠŸç‡: {result.success_rate:.2f}%")
        
        if "${test_type}" == "all" or "${test_type}" == "benchmark":
            print("\\nğŸ“Š åŸºå‡†æµ‹è¯•")
            results = await suite.benchmark_test(
                endpoints=["/health", "/gateway/health"],
                iterations=100,
            )
            for endpoint, metrics in results.items():
                print(f"{endpoint}: {metrics.avg_response_time:.2f}ms")
        
        # ç”ŸæˆæŠ¥å‘Š
        report = suite.generate_report()
        report_file = "${REPORT_DIR}/performance_report_\$(date +%Y%m%d_%H%M%S).json"
        with open(report_file, "w") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
    finally:
        await suite.close()

asyncio.run(main())
EOF
}

# ä¸»å‡½æ•°
main() {
    run_performance_test "${TEST_TYPE}"
    
    echo -e "${GREEN}æ€§èƒ½æµ‹è¯•å®Œæˆ${NC}"
}

# è¿è¡Œä¸»å‡½æ•°
main


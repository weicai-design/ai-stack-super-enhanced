#!/usr/bin/env python3
"""
AI Stack æ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹è¯•å„ä¸ªæ¨¡å—çš„æ€§èƒ½æŒ‡æ ‡
"""
import asyncio
import time
import httpx
from typing import Dict, Any, List
from datetime import datetime
import statistics


class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, base_url: str = "http://localhost"):
        """
        åˆå§‹åŒ–åŸºå‡†æµ‹è¯•
        
        Args:
            base_url: åŸºç¡€URL
        """
        self.base_url = base_url
        self.results = []
    
    async def benchmark_api(
        self,
        name: str,
        url: str,
        method: str = "GET",
        data: Dict[str, Any] = None,
        iterations: int = 100
    ) -> Dict[str, Any]:
        """
        å¯¹å•ä¸ªAPIè¿›è¡ŒåŸºå‡†æµ‹è¯•
        
        Args:
            name: æµ‹è¯•åç§°
            url: API URL
            method: HTTPæ–¹æ³•
            data: è¯·æ±‚æ•°æ®
            iterations: è¿­ä»£æ¬¡æ•°
        
        Returns:
            æµ‹è¯•ç»“æœ
        """
        print(f"\nğŸ”„ æµ‹è¯• {name} ({iterations}æ¬¡è¯·æ±‚)...")
        
        response_times = []
        success_count = 0
        error_count = 0
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            for i in range(iterations):
                try:
                    start_time = time.time()
                    
                    if method == "GET":
                        response = await client.get(url)
                    elif method == "POST":
                        response = await client.post(url, json=data)
                    else:
                        raise ValueError(f"ä¸æ”¯æŒçš„æ–¹æ³•: {method}")
                    
                    end_time = time.time()
                    response_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                    
                    if response.status_code == 200:
                        success_count += 1
                        response_times.append(response_time)
                    else:
                        error_count += 1
                
                except Exception as e:
                    error_count += 1
                    print(f"  âš ï¸  è¯·æ±‚ {i+1} å¤±è´¥: {str(e)}")
                
                # æ˜¾ç¤ºè¿›åº¦
                if (i + 1) % 10 == 0:
                    print(f"  è¿›åº¦: {i+1}/{iterations}")
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        if response_times:
            result = {
                "name": name,
                "iterations": iterations,
                "success_count": success_count,
                "error_count": error_count,
                "success_rate": (success_count / iterations) * 100,
                "avg_response_time": statistics.mean(response_times),
                "min_response_time": min(response_times),
                "max_response_time": max(response_times),
                "median_response_time": statistics.median(response_times),
                "p95_response_time": sorted(response_times)[int(len(response_times) * 0.95)],
                "p99_response_time": sorted(response_times)[int(len(response_times) * 0.99)],
                "throughput": success_count / (sum(response_times) / 1000)  # è¯·æ±‚/ç§’
            }
        else:
            result = {
                "name": name,
                "iterations": iterations,
                "success_count": 0,
                "error_count": error_count,
                "success_rate": 0,
                "error": "æ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥äº†"
            }
        
        self.results.append(result)
        return result
    
    async def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("=" * 60)
        print("ğŸš€ AI Stack æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 60)
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"åŸºç¡€URL: {self.base_url}")
        print()
        
        # å®šä¹‰æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            {
                "name": "AIäº¤äº’ä¸­å¿ƒ - å¥åº·æ£€æŸ¥",
                "url": f"{self.base_url}:8020/health",
                "method": "GET",
                "iterations": 100
            },
            {
                "name": "AIäº¤äº’ä¸­å¿ƒ - èŠå¤©API",
                "url": f"{self.base_url}:8020/api/chat",
                "method": "POST",
                "data": {"message": "æµ‹è¯•æ¶ˆæ¯", "user_id": "benchmark"},
                "iterations": 50
            },
            {
                "name": "RAGç³»ç»Ÿ - å¥åº·æ£€æŸ¥",
                "url": f"{self.base_url}:8011/health",
                "method": "GET",
                "iterations": 100
            },
            {
                "name": "RAGç³»ç»Ÿ - çŸ¥è¯†æ£€ç´¢",
                "url": f"{self.base_url}:8011/api/search",
                "method": "POST",
                "data": {"query": "æµ‹è¯•æŸ¥è¯¢", "top_k": 5},
                "iterations": 50
            }
        ]
        
        # è¿è¡Œæµ‹è¯•
        for test_case in test_cases:
            await self.benchmark_api(**test_case)
            await asyncio.sleep(1)  # é¿å…è¿‡è½½
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        for result in self.results:
            print(f"\nã€{result['name']}ã€‘")
            print("-" * 60)
            
            if "error" in result:
                print(f"âŒ é”™è¯¯: {result['error']}")
                continue
            
            print(f"æ€»è¯·æ±‚æ•°: {result['iterations']}")
            print(f"æˆåŠŸ: {result['success_count']} | å¤±è´¥: {result['error_count']}")
            print(f"æˆåŠŸç‡: {result['success_rate']:.2f}%")
            print()
            print(f"å“åº”æ—¶é—´ç»Ÿè®¡ (ms):")
            print(f"  â€¢ å¹³å‡: {result['avg_response_time']:.2f}")
            print(f"  â€¢ æœ€å°: {result['min_response_time']:.2f}")
            print(f"  â€¢ æœ€å¤§: {result['max_response_time']:.2f}")
            print(f"  â€¢ ä¸­ä½æ•°: {result['median_response_time']:.2f}")
            print(f"  â€¢ P95: {result['p95_response_time']:.2f}")
            print(f"  â€¢ P99: {result['p99_response_time']:.2f}")
            print()
            print(f"ååé‡: {result['throughput']:.2f} è¯·æ±‚/ç§’")
        
        # æ€»ä½“è¯„ä¼°
        print("\n" + "=" * 60)
        print("ğŸ¯ æ€»ä½“è¯„ä¼°")
        print("=" * 60)
        
        avg_success_rate = statistics.mean([r['success_rate'] for r in self.results if 'success_rate' in r])
        avg_response_time = statistics.mean([r['avg_response_time'] for r in self.results if 'avg_response_time' in r])
        
        print(f"å¹³å‡æˆåŠŸç‡: {avg_success_rate:.2f}%")
        print(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}ms")
        
        # æ€§èƒ½è¯„çº§
        if avg_response_time < 50:
            grade = "ä¼˜ç§€ ğŸŒŸğŸŒŸğŸŒŸğŸŒŸğŸŒŸ"
        elif avg_response_time < 100:
            grade = "è‰¯å¥½ ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ"
        elif avg_response_time < 200:
            grade = "ä¸­ç­‰ ğŸŒŸğŸŒŸğŸŒŸ"
        elif avg_response_time < 500:
            grade = "ä¸€èˆ¬ ğŸŒŸğŸŒŸ"
        else:
            grade = "éœ€è¦ä¼˜åŒ– ğŸŒŸ"
        
        print(f"æ€§èƒ½è¯„çº§: {grade}")
        
        print("\n" + "=" * 60)
        print(f"æµ‹è¯•å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)


async def main():
    """ä¸»å‡½æ•°"""
    benchmark = PerformanceBenchmark()
    await benchmark.run_all_benchmarks()


if __name__ == "__main__":
    asyncio.run(main())


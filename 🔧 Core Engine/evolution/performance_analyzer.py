# ai-stack-super-enhanced/ğŸ”§ Core Engine/evolution/performance_analyzer.py
"""
æ€§èƒ½åˆ†æå™¨ - è´Ÿè´£ç³»ç»Ÿæ€§èƒ½ç›‘æ§ã€åˆ†æå’Œç“¶é¢ˆè¯†åˆ«
å¯¹åº”å¼€å‘è®¡åˆ’ï¼šé˜¶æ®µ1 - Core Engine ä¸­çš„è¿›åŒ–å¼•æ“åŸºç¡€
å¯¹åº”å¼€å‘è§„åˆ™ï¼š9.1/9.2 è‡ªæˆ‘å­¦ä¹ å’Œè‡ªæˆ‘è¿›åŒ–åŠŸèƒ½éœ€æ±‚
"""

import logging
import statistics
import time
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional

import numpy as np
import psutil

logger = logging.getLogger(__name__)


class PerformanceDimension(Enum):
    """æ€§èƒ½ç»´åº¦æšä¸¾"""

    EFFICIENCY = "efficiency"
    ACCURACY = "accuracy"
    STABILITY = "stability"
    ADAPTABILITY = "adaptability"
    RESOURCE_USAGE = "resource_usage"
    RESPONSIVENESS = "responsiveness"


class AnalysisLevel(Enum):
    """åˆ†æçº§åˆ«æšä¸¾"""

    BASIC = "basic"
    STANDARD = "standard"
    COMPREHENSIVE = "comprehensive"
    DEEP = "deep"


class PerformanceAnalyzer:
    """
    æ€§èƒ½åˆ†æå™¨ - ç³»ç»Ÿæ€§èƒ½ç›‘æ§å’Œåˆ†ææ ¸å¿ƒç»„ä»¶

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. å¤šç»´åº¦æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œç›‘æ§
    2. æ€§èƒ½è¶‹åŠ¿åˆ†æå’Œé¢„æµ‹
    3. ç³»ç»Ÿç“¶é¢ˆè¯†åˆ«å’Œå®šä½
    4. æ€§èƒ½å¼‚å¸¸æ£€æµ‹å’Œå‘Šè­¦
    5. æ€§èƒ½æ•°æ®å¯è§†åŒ–å’ŒæŠ¥å‘Šç”Ÿæˆ
    """

    def __init__(self):
        # æ€§èƒ½æ•°æ®å­˜å‚¨
        self.performance_history = {}
        self.metric_thresholds = {}
        self.anomaly_detectors = {}

        # åˆ†æé…ç½®
        self.analysis_config = {}
        self.monitoring_intervals = {}

        # çŠ¶æ€ä¿¡æ¯
        self.is_initialized = False
        self.last_analysis_time = None
        self.analysis_count = 0

        # ç¼“å­˜ä¼˜åŒ–
        self.analysis_cache = {}
        self.cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜

        logger.info("æ€§èƒ½åˆ†æå™¨å®ä¾‹åˆ›å»º")

    async def initialize(self, config: Dict[str, Any] = None) -> bool:
        """
        åˆå§‹åŒ–æ€§èƒ½åˆ†æå™¨

        Args:
            config: åˆ†æé…ç½®å‚æ•°

        Returns:
            bool: åˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹åˆå§‹åŒ–æ€§èƒ½åˆ†æå™¨")

            # è®¾ç½®é…ç½®
            self.analysis_config = config or {}
            await self._setup_default_config()

            # åˆå§‹åŒ–æ€§èƒ½å†å²å­˜å‚¨
            await self._initialize_performance_storage()

            # è®¾ç½®æŒ‡æ ‡é˜ˆå€¼
            await self._setup_metric_thresholds()

            # åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹å™¨
            await self._initialize_anomaly_detectors()

            # å¯åŠ¨åå°ç›‘æ§ä»»åŠ¡
            await self._start_background_monitoring()

            self.is_initialized = True
            self.last_analysis_time = datetime.utcnow()

            logger.info("æ€§èƒ½åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")
            return True

        except Exception as e:
            logger.error(f"æ€§èƒ½åˆ†æå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}", exc_info=True)
            self.is_initialized = False
            return False

    async def _setup_default_config(self):
        """è®¾ç½®é»˜è®¤é…ç½®"""
        default_config = {
            "analysis_level": AnalysisLevel.STANDARD.value,
            "data_retention_days": 30,
            "real_time_monitoring": True,
            "anomaly_detection_enabled": True,
            "trend_analysis_enabled": True,
            "performance_prediction": True,
            "auto_threshold_adjustment": True,
            "report_generation_interval": 3600,  # 1å°æ—¶
            "cache_enabled": True,
        }

        # åˆå¹¶é…ç½®
        for key, value in default_config.items():
            if key not in self.analysis_config:
                self.analysis_config[key] = value

        # è®¾ç½®ç›‘æ§é—´éš”
        self.monitoring_intervals = {
            "system_metrics": 10,  # ç³»ç»ŸæŒ‡æ ‡æ¯10ç§’
            "application_metrics": 30,  # åº”ç”¨æŒ‡æ ‡æ¯30ç§’
            "business_metrics": 60,  # ä¸šåŠ¡æŒ‡æ ‡æ¯60ç§’
            "deep_analysis": 300,  # æ·±åº¦åˆ†ææ¯5åˆ†é’Ÿ
        }

    async def _initialize_performance_storage(self):
        """åˆå§‹åŒ–æ€§èƒ½æ•°æ®å­˜å‚¨"""
        # ä¸ºæ¯ä¸ªæ€§èƒ½ç»´åº¦åˆå§‹åŒ–å†å²æ•°æ®å­˜å‚¨
        for dimension in PerformanceDimension:
            self.performance_history[dimension.value] = {
                "timestamps": [],
                "values": [],
                "metadata": [],
            }

        # åˆå§‹åŒ–ç³»ç»ŸæŒ‡æ ‡å­˜å‚¨
        self.performance_history["system"] = {
            "cpu_usage": [],
            "memory_usage": [],
            "disk_io": [],
            "network_io": [],
            "timestamps": [],
        }

        logger.debug("æ€§èƒ½æ•°æ®å­˜å‚¨åˆå§‹åŒ–å®Œæˆ")

    async def _setup_metric_thresholds(self):
        """è®¾ç½®æŒ‡æ ‡é˜ˆå€¼"""
        # é»˜è®¤é˜ˆå€¼é…ç½®
        self.metric_thresholds = {
            "cpu_usage": {"warning": 80, "critical": 95},
            "memory_usage": {"warning": 85, "critical": 95},
            "response_time": {"warning": 2.0, "critical": 5.0},  # ç§’
            "error_rate": {"warning": 0.05, "critical": 0.1},  # 5%, 10%
            "throughput": {"warning": 0.7, "critical": 0.5},  # ç›¸å¯¹å€¼
            "availability": {"warning": 0.99, "critical": 0.95},  # 99%, 95%
        }

        # ä»é…ç½®ä¸­æ›´æ–°é˜ˆå€¼
        if "metric_thresholds" in self.analysis_config:
            self.metric_thresholds.update(self.analysis_config["metric_thresholds"])

    async def _initialize_anomaly_detectors(self):
        """åˆå§‹åŒ–å¼‚å¸¸æ£€æµ‹å™¨"""
        try:
            # ç®€å•çš„ç»Ÿè®¡å¼‚å¸¸æ£€æµ‹å™¨
            self.anomaly_detectors = {
                "z_score": {"enabled": True, "threshold": 3.0},  # 3ä¸ªæ ‡å‡†å·®
                "moving_average": {"enabled": True, "window_size": 10},
                "rate_of_change": {"enabled": True, "threshold": 2.0},  # å˜åŒ–ç‡é˜ˆå€¼
            }

            logger.debug("å¼‚å¸¸æ£€æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"å¼‚å¸¸æ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    async def _start_background_monitoring(self):
        """å¯åŠ¨åå°ç›‘æ§ä»»åŠ¡"""
        if self.analysis_config.get("real_time_monitoring", True):
            # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šå¯åŠ¨çœŸæ­£çš„ç›‘æ§å¾ªç¯
            # ä¸ºç®€åŒ–ï¼Œè¿™é‡Œåªè®°å½•æ—¥å¿—
            logger.info("åå°æ€§èƒ½ç›‘æ§å·²å¯ç”¨")

    async def collect_system_metrics(self) -> Dict[str, Any]:
        """
        æ”¶é›†ç³»ç»Ÿçº§æ€§èƒ½æŒ‡æ ‡

        Returns:
            Dict: ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡
        """
        try:
            metrics = {
                "timestamp": datetime.utcnow().isoformat(),
                "cpu": {
                    "percent": psutil.cpu_percent(interval=1),
                    "per_cpu": psutil.cpu_percent(interval=1, percpu=True),
                    "load_avg": (
                        psutil.getloadavg()
                        if hasattr(psutil, "getloadavg")
                        else [0, 0, 0]
                    ),
                },
                "memory": {
                    "total": psutil.virtual_memory().total,
                    "available": psutil.virtual_memory().available,
                    "percent": psutil.virtual_memory().percent,
                    "used": psutil.virtual_memory().used,
                },
                "disk": {
                    "usage_percent": psutil.disk_usage("/").percent,
                    "io_counters": (
                        psutil.disk_io_counters()._asdict()
                        if psutil.disk_io_counters()
                        else {}
                    ),
                },
                "network": {
                    "io_counters": (
                        psutil.net_io_counters()._asdict()
                        if psutil.net_io_counters()
                        else {}
                    )
                },
                "process": {
                    "count": len(psutil.pids()),
                    "current_rss": psutil.Process().memory_info().rss,
                },
            }

            # å­˜å‚¨ç³»ç»ŸæŒ‡æ ‡
            await self._store_system_metrics(metrics)

            return metrics

        except Exception as e:
            logger.error(f"ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å¤±è´¥: {str(e)}")
            return {"timestamp": datetime.utcnow().isoformat(), "error": str(e)}

    async def _store_system_metrics(self, metrics: Dict[str, Any]):
        """å­˜å‚¨ç³»ç»ŸæŒ‡æ ‡åˆ°å†å²æ•°æ®"""
        try:
            history = self.performance_history["system"]
            current_time = datetime.utcnow()

            # æ·»åŠ æ—¶é—´æˆ³
            history["timestamps"].append(current_time)

            # æ·»åŠ å„é¡¹æŒ‡æ ‡
            history["cpu_usage"].append(metrics["cpu"]["percent"])
            history["memory_usage"].append(metrics["memory"]["percent"])

            # ç£ç›˜IO
            disk_io = metrics["disk"]["io_counters"]
            disk_io_total = (
                disk_io.get("read_bytes", 0) + disk_io.get("write_bytes", 0)
                if disk_io
                else 0
            )
            history["disk_io"].append(disk_io_total)

            # ç½‘ç»œIO
            net_io = metrics["network"]["io_counters"]
            net_io_total = (
                net_io.get("bytes_sent", 0) + net_io.get("bytes_recv", 0)
                if net_io
                else 0
            )
            history["network_io"].append(net_io_total)

            # é™åˆ¶å†å²æ•°æ®å¤§å°
            max_history = 1000
            for key in history:
                if len(history[key]) > max_history:
                    history[key] = history[key][-max_history:]

        except Exception as e:
            logger.error(f"ç³»ç»ŸæŒ‡æ ‡å­˜å‚¨å¤±è´¥: {str(e)}")

    async def collect_application_metrics(self) -> Dict[str, Any]:
        """
        æ”¶é›†åº”ç”¨çº§æ€§èƒ½æŒ‡æ ‡

        Returns:
            Dict: åº”ç”¨æ€§èƒ½æŒ‡æ ‡
        """
        try:
            metrics = {
                "timestamp": datetime.utcnow().isoformat(),
                "response_time": {
                    "average": self._simulate_response_time(),
                    "p95": self._simulate_response_time() * 1.5,
                    "p99": self._simulate_response_time() * 2.0,
                },
                "throughput": {
                    "requests_per_second": self._simulate_throughput(),
                    "data_processed": self._simulate_data_processed(),
                },
                "error_rates": {
                    "total": self._simulate_error_rate(),
                    "by_type": {"timeout": 0.01, "validation": 0.02, "system": 0.001},
                },
                "business_metrics": {
                    "user_satisfaction": self._simulate_user_satisfaction(),
                    "completion_rate": self._simulate_completion_rate(),
                },
            }

            # å­˜å‚¨åº”ç”¨æŒ‡æ ‡
            await self._store_application_metrics(metrics)

            return metrics

        except Exception as e:
            logger.error(f"åº”ç”¨æŒ‡æ ‡æ”¶é›†å¤±è´¥: {str(e)}")
            return {"timestamp": datetime.utcnow().isoformat(), "error": str(e)}

    def _simulate_response_time(self) -> float:
        """æ¨¡æ‹Ÿå“åº”æ—¶é—´æ•°æ®"""
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šä»å®é™…ç›‘æ§ç³»ç»Ÿè·å–æ•°æ®
        return 0.1 + (0.4 * np.random.random())  # 0.1-0.5ç§’

    def _simulate_throughput(self) -> float:
        """æ¨¡æ‹Ÿååé‡æ•°æ®"""
        return 100 + (900 * np.random.random())  # 100-1000 è¯·æ±‚/ç§’

    def _simulate_data_processed(self) -> float:
        """æ¨¡æ‹Ÿæ•°æ®å¤„ç†é‡"""
        return 1024 + (10240 * np.random.random())  # 1KB-10KB

    def _simulate_error_rate(self) -> float:
        """æ¨¡æ‹Ÿé”™è¯¯ç‡"""
        return 0.01 + (0.04 * np.random.random())  # 1%-5%

    def _simulate_user_satisfaction(self) -> float:
        """æ¨¡æ‹Ÿç”¨æˆ·æ»¡æ„åº¦"""
        return 0.8 + (0.19 * np.random.random())  # 80%-99%

    def _simulate_completion_rate(self) -> float:
        """æ¨¡æ‹Ÿå®Œæˆç‡"""
        return 0.85 + (0.14 * np.random.random())  # 85%-99%

    async def _store_application_metrics(self, metrics: Dict[str, Any]):
        """å­˜å‚¨åº”ç”¨æŒ‡æ ‡åˆ°å†å²æ•°æ®"""
        try:
            current_time = datetime.utcnow()

            # å­˜å‚¨åˆ°å„ä¸ªæ€§èƒ½ç»´åº¦
            dimensions_mapping = {
                PerformanceDimension.EFFICIENCY: metrics["response_time"]["average"],
                PerformanceDimension.ACCURACY: 1.0 - metrics["error_rates"]["total"],
                PerformanceDimension.STABILITY: metrics["business_metrics"][
                    "completion_rate"
                ],
                PerformanceDimension.ADAPTABILITY: metrics["business_metrics"][
                    "user_satisfaction"
                ],
                PerformanceDimension.RESPONSIVENESS: metrics["response_time"][
                    "average"
                ],
            }

            for dimension, value in dimensions_mapping.items():
                history = self.performance_history[dimension.value]
                history["timestamps"].append(current_time)
                history["values"].append(value)
                history["metadata"].append({"source": "application_metrics"})

                # é™åˆ¶å†å²æ•°æ®å¤§å°
                max_history = 1000
                if len(history["values"]) > max_history:
                    history["timestamps"] = history["timestamps"][-max_history:]
                    history["values"] = history["values"][-max_history:]
                    history["metadata"] = history["metadata"][-max_history:]

        except Exception as e:
            logger.error(f"åº”ç”¨æŒ‡æ ‡å­˜å‚¨å¤±è´¥: {str(e)}")

    async def get_comprehensive_analysis(self) -> Dict[str, Any]:
        """
        è·å–ç»¼åˆåˆ†ææŠ¥å‘Š

        Returns:
            Dict: ç»¼åˆåˆ†æç»“æœ
        """
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = "comprehensive_analysis"
            if self.analysis_config.get("cache_enabled", True):
                cached_result = self._get_cached_analysis(cache_key)
                if cached_result:
                    return cached_result

            logger.info("å¼€å§‹æ‰§è¡Œç»¼åˆæ€§èƒ½åˆ†æ")

            # æ”¶é›†å½“å‰æŒ‡æ ‡
            system_metrics = await self.collect_system_metrics()
            application_metrics = await self.collect_application_metrics()

            # æ‰§è¡Œå¤šç»´åº¦åˆ†æ
            analysis_results = {
                "timestamp": datetime.utcnow().isoformat(),
                "analysis_id": f"analysis_{self.analysis_count}",
                "summary": await self._generate_analysis_summary(),
                "dimensions": {},
            }

            # åˆ†æå„ä¸ªæ€§èƒ½ç»´åº¦
            for dimension in PerformanceDimension:
                dimension_analysis = await self._analyze_performance_dimension(
                    dimension
                )
                analysis_results["dimensions"][dimension.value] = dimension_analysis

            # æ‰§è¡Œè¶‹åŠ¿åˆ†æ
            analysis_results["trends"] = await self._analyze_performance_trends()

            # å¼‚å¸¸æ£€æµ‹
            analysis_results["anomalies"] = await self._detect_anomalies()

            # ç“¶é¢ˆè¯†åˆ«
            analysis_results["bottlenecks"] = await self.identify_bottlenecks()

            # ç”Ÿæˆå»ºè®®
            analysis_results["recommendations"] = await self._generate_recommendations(
                analysis_results
            )

            self.analysis_count += 1
            self.last_analysis_time = datetime.utcnow()

            # ç¼“å­˜ç»“æœ
            if self.analysis_config.get("cache_enabled", True):
                self._cache_analysis(cache_key, analysis_results)

            logger.info("ç»¼åˆæ€§èƒ½åˆ†æå®Œæˆ")
            return analysis_results

        except Exception as e:
            logger.error(f"ç»¼åˆæ€§èƒ½åˆ†æå¤±è´¥: {str(e)}", exc_info=True)
            return {
                "timestamp": datetime.utcnow().isoformat(),
                "error": str(e),
                "analysis_id": f"analysis_error_{self.analysis_count}",
            }

    async def _generate_analysis_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        try:
            # è®¡ç®—æ•´ä½“æ€§èƒ½åˆ†æ•°
            dimension_scores = []
            for dimension in PerformanceDimension:
                history = self.performance_history[dimension.value]
                if history["values"]:
                    recent_values = history["values"][-10:]  # æœ€è¿‘10ä¸ªå€¼
                    avg_score = sum(recent_values) / len(recent_values)
                    dimension_scores.append(avg_score)

            overall_score = (
                sum(dimension_scores) / len(dimension_scores)
                if dimension_scores
                else 0.0
            )

            # è¯„ä¼°æ•´ä½“çŠ¶æ€
            if overall_score >= 0.9:
                status = "excellent"
            elif overall_score >= 0.7:
                status = "good"
            elif overall_score >= 0.5:
                status = "fair"
            else:
                status = "poor"

            return {
                "overall_score": overall_score,
                "status": status,
                "dimensions_analyzed": len(dimension_scores),
                "data_points_used": sum(
                    len(self.performance_history[dim.value]["values"])
                    for dim in PerformanceDimension
                ),
            }

        except Exception as e:
            logger.error(f"åˆ†ææ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {"overall_score": 0.0, "status": "unknown", "error": str(e)}

    async def _analyze_performance_dimension(
        self, dimension: PerformanceDimension
    ) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ€§èƒ½ç»´åº¦"""
        try:
            history = self.performance_history[dimension.value]

            if not history["values"]:
                return {
                    "score": 0.0,
                    "status": "insufficient_data",
                    "trend": "unknown",
                    "data_points": 0,
                }

            values = history["values"]
            timestamps = history["timestamps"]

            # è®¡ç®—å½“å‰å¾—åˆ†
            current_score = values[-1] if values else 0.0

            # è®¡ç®—è¶‹åŠ¿
            trend = await self._calculate_dimension_trend(values)

            # è®¡ç®—ç¨³å®šæ€§
            stability = await self._calculate_dimension_stability(values)

            # è¯„ä¼°çŠ¶æ€
            status = self._evaluate_dimension_status(dimension, current_score, trend)

            return {
                "score": current_score,
                "status": status,
                "trend": trend,
                "stability": stability,
                "data_points": len(values),
                "last_updated": timestamps[-1].isoformat() if timestamps else None,
            }

        except Exception as e:
            logger.error(f"æ€§èƒ½ç»´åº¦åˆ†æå¤±è´¥ {dimension.value}: {str(e)}")
            return {"score": 0.0, "status": "analysis_error", "error": str(e)}

    async def _calculate_dimension_trend(self, values: List[float]) -> str:
        """è®¡ç®—ç»´åº¦è¶‹åŠ¿"""
        if len(values) < 5:
            return "insufficient_data"

        recent_values = values[-10:]  # æœ€è¿‘10ä¸ªå€¼

        # ç®€å•çº¿æ€§è¶‹åŠ¿åˆ†æ
        x = list(range(len(recent_values)))
        y = recent_values

        # è®¡ç®—æ–œç‡
        n = len(x)
        sum_x = sum(x)
        sum_y = sum(y)
        sum_xy = sum(x[i] * y[i] for i in range(n))
        sum_x2 = sum(x_i * x_i for x_i in x)

        numerator = n * sum_xy - sum_x * sum_y
        denominator = n * sum_x2 - sum_x * sum_x

        if denominator == 0:
            return "stable"

        slope = numerator / denominator

        if slope > 0.01:
            return "improving"
        elif slope < -0.01:
            return "declining"
        else:
            return "stable"

    async def _calculate_dimension_stability(self, values: List[float]) -> float:
        """è®¡ç®—ç»´åº¦ç¨³å®šæ€§"""
        if len(values) < 2:
            return 1.0

        # è®¡ç®—å˜å¼‚ç³»æ•°
        mean = statistics.mean(values)
        if mean == 0:
            return 1.0

        stdev = statistics.stdev(values) if len(values) >= 2 else 0
        coefficient_of_variation = stdev / mean

        # ç¨³å®šæ€§ = 1 - å˜å¼‚ç³»æ•°
        stability = max(0.0, 1.0 - coefficient_of_variation)
        return stability

    def _evaluate_dimension_status(
        self, dimension: PerformanceDimension, score: float, trend: str
    ) -> str:
        """è¯„ä¼°ç»´åº¦çŠ¶æ€"""
        # ä¸åŒç»´åº¦å¯èƒ½æœ‰ä¸åŒçš„è¯„ä¼°æ ‡å‡†
        dimension_thresholds = {
            PerformanceDimension.EFFICIENCY: {"good": 0.8, "fair": 0.6},
            PerformanceDimension.ACCURACY: {"good": 0.95, "fair": 0.8},
            PerformanceDimension.STABILITY: {"good": 0.9, "fair": 0.7},
            PerformanceDimension.ADAPTABILITY: {"good": 0.85, "fair": 0.65},
            PerformanceDimension.RESOURCE_USAGE: {"good": 0.8, "fair": 0.6},
            PerformanceDimension.RESPONSIVENESS: {"good": 0.85, "fair": 0.7},
        }

        thresholds = dimension_thresholds.get(dimension, {"good": 0.8, "fair": 0.6})

        if score >= thresholds["good"]:
            base_status = "good"
        elif score >= thresholds["fair"]:
            base_status = "fair"
        else:
            base_status = "poor"

        # è€ƒè™‘è¶‹åŠ¿å› ç´ 
        if trend == "improving" and base_status == "poor":
            return "improving_poor"
        elif trend == "declining" and base_status == "good":
            return "declining_good"
        else:
            return base_status

    async def _analyze_performance_trends(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        try:
            trends = {}

            # åˆ†æå„ç»´åº¦è¶‹åŠ¿
            for dimension in PerformanceDimension:
                history = self.performance_history[dimension.value]
                if len(history["values"]) >= 10:
                    values = history["values"][-20:]  # æœ€è¿‘20ä¸ªå€¼
                    trend_analysis = await self._perform_trend_analysis(values)
                    trends[dimension.value] = trend_analysis

            # åˆ†ææ•´ä½“è¶‹åŠ¿
            all_scores = []
            for dimension in PerformanceDimension:
                history = self.performance_history[dimension.value]
                if history["values"]:
                    all_scores.append(history["values"][-1])

            if all_scores:
                overall_trend = await self._perform_trend_analysis(all_scores)
                trends["overall"] = overall_trend

            return trends

        except Exception as e:
            logger.error(f"æ€§èƒ½è¶‹åŠ¿åˆ†æå¤±è´¥: {str(e)}")
            return {"error": str(e)}

    async def _perform_trend_analysis(self, values: List[float]) -> Dict[str, Any]:
        """æ‰§è¡Œè¶‹åŠ¿åˆ†æ"""
        if len(values) < 5:
            return {"trend": "insufficient_data"}

        # ç®€å•çº¿æ€§å›å½’
        x = list(range(len(values)))
        y = values

        n = len(x)
        sum_x = sum(x)
        sum_y = sum(y)
        sum_xy = sum(x[i] * y[i] for i in range(n))
        sum_x2 = sum(x_i * x_i for x_i in x)

        numerator = n * sum_xy - sum_x * sum_y
        denominator = n * sum_x2 - sum_x * sum_x

        if denominator == 0:
            return {"trend": "stable", "slope": 0}

        slope = numerator / denominator

        # é¢„æµ‹ä¸‹ä¸€ä¸ªå€¼
        next_x = n
        predicted_next = (slope * next_x) + (sum_y / n - slope * sum_x / n)

        # è®¡ç®—RÂ²
        y_mean = sum_y / n
        ss_tot = sum((y_i - y_mean) ** 2 for y_i in y)
        ss_res = sum(
            (y[i] - (slope * x[i] + (y_mean - slope * sum_x / n))) ** 2
            for i in range(n)
        )

        r_squared = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0

        # åˆ¤æ–­è¶‹åŠ¿å¼ºåº¦
        if abs(slope) < 0.001:
            trend_strength = "stable"
        elif abs(slope) < 0.01:
            trend_strength = "weak"
        elif abs(slope) < 0.05:
            trend_strength = "moderate"
        else:
            trend_strength = "strong"

        return {
            "trend": (
                "improving"
                if slope > 0.001
                else "declining" if slope < -0.001 else "stable"
            ),
            "slope": slope,
            "predicted_next": predicted_next,
            "r_squared": r_squared,
            "trend_strength": trend_strength,
            "data_points": n,
        }

    async def _detect_anomalies(self) -> Dict[str, Any]:
        """æ£€æµ‹æ€§èƒ½å¼‚å¸¸"""
        try:
            if not self.analysis_config.get("anomaly_detection_enabled", True):
                return {"enabled": False}

            anomalies = {}

            # æ£€æŸ¥å„ç»´åº¦å¼‚å¸¸
            for dimension in PerformanceDimension:
                history = self.performance_history[dimension.value]
                if len(history["values"]) >= 10:
                    dimension_anomalies = await self._check_dimension_anomalies(
                        dimension, history["values"]
                    )
                    if dimension_anomalies:
                        anomalies[dimension.value] = dimension_anomalies

            # æ£€æŸ¥ç³»ç»ŸæŒ‡æ ‡å¼‚å¸¸
            system_anomalies = await self._check_system_anomalies()
            if system_anomalies:
                anomalies["system"] = system_anomalies

            return {
                "detected": len(anomalies) > 0,
                "anomalies": anomalies,
                "total_checked": len(PerformanceDimension) + 1,  # +1 for system
            }

        except Exception as e:
            logger.error(f"å¼‚å¸¸æ£€æµ‹å¤±è´¥: {str(e)}")
            return {"error": str(e)}

    async def _check_dimension_anomalies(
        self, dimension: PerformanceDimension, values: List[float]
    ) -> List[Dict[str, Any]]:
        """æ£€æŸ¥ç»´åº¦å¼‚å¸¸"""
        anomalies = []

        if len(values) < 10:
            return anomalies

        recent_values = values[-10:]
        current_value = values[-1]

        # Z-score å¼‚å¸¸æ£€æµ‹
        mean = statistics.mean(recent_values)
        stdev = statistics.stdev(recent_values) if len(recent_values) >= 2 else 0

        if stdev > 0:
            z_score = abs(current_value - mean) / stdev
            if z_score > self.anomaly_detectors["z_score"]["threshold"]:
                anomalies.append(
                    {
                        "type": "z_score_outlier",
                        "value": current_value,
                        "z_score": z_score,
                        "threshold": self.anomaly_detectors["z_score"]["threshold"],
                        "severity": "high" if z_score > 5 else "medium",
                    }
                )

        # å˜åŒ–ç‡å¼‚å¸¸æ£€æµ‹
        if len(values) >= 2:
            previous_value = values[-2]
            if previous_value != 0:
                rate_of_change = abs(current_value - previous_value) / previous_value
                if (
                    rate_of_change
                    > self.anomaly_detectors["rate_of_change"]["threshold"]
                ):
                    anomalies.append(
                        {
                            "type": "high_rate_of_change",
                            "value": current_value,
                            "rate_of_change": rate_of_change,
                            "threshold": self.anomaly_detectors["rate_of_change"][
                                "threshold"
                            ],
                            "severity": "high" if rate_of_change > 5 else "medium",
                        }
                    )

        return anomalies

    async def _check_system_anomalies(self) -> List[Dict[str, Any]]:
        """æ£€æŸ¥ç³»ç»ŸæŒ‡æ ‡å¼‚å¸¸"""
        anomalies = []

        try:
            system_metrics = await self.collect_system_metrics()

            # æ£€æŸ¥CPUä½¿ç”¨ç‡
            cpu_usage = system_metrics["cpu"]["percent"]
            if cpu_usage > self.metric_thresholds["cpu_usage"]["critical"]:
                anomalies.append(
                    {
                        "type": "critical_cpu_usage",
                        "metric": "cpu_usage",
                        "value": cpu_usage,
                        "threshold": self.metric_thresholds["cpu_usage"]["critical"],
                        "severity": "critical",
                    }
                )
            elif cpu_usage > self.metric_thresholds["cpu_usage"]["warning"]:
                anomalies.append(
                    {
                        "type": "warning_cpu_usage",
                        "metric": "cpu_usage",
                        "value": cpu_usage,
                        "threshold": self.metric_thresholds["cpu_usage"]["warning"],
                        "severity": "warning",
                    }
                )

            # æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡
            memory_usage = system_metrics["memory"]["percent"]
            if memory_usage > self.metric_thresholds["memory_usage"]["critical"]:
                anomalies.append(
                    {
                        "type": "critical_memory_usage",
                        "metric": "memory_usage",
                        "value": memory_usage,
                        "threshold": self.metric_thresholds["memory_usage"]["critical"],
                        "severity": "critical",
                    }
                )
            elif memory_usage > self.metric_thresholds["memory_usage"]["warning"]:
                anomalies.append(
                    {
                        "type": "warning_memory_usage",
                        "metric": "memory_usage",
                        "value": memory_usage,
                        "threshold": self.metric_thresholds["memory_usage"]["warning"],
                        "severity": "warning",
                    }
                )

            return anomalies

        except Exception as e:
            logger.error(f"ç³»ç»Ÿå¼‚å¸¸æ£€æŸ¥å¤±è´¥: {str(e)}")
            return [
                {"type": "system_check_error", "error": str(e), "severity": "medium"}
            ]

    async def identify_bottlenecks(self) -> Dict[str, Any]:
        """
        è¯†åˆ«ç³»ç»Ÿç“¶é¢ˆ

        Returns:
            Dict: ç“¶é¢ˆåˆ†æç»“æœ
        """
        try:
            bottlenecks = {
                "timestamp": datetime.utcnow().isoformat(),
                "bottlenecks": [],
                "severity_summary": {"critical": 0, "high": 0, "medium": 0, "low": 0},
            }

            # åˆ†æç³»ç»Ÿç“¶é¢ˆ
            system_bottlenecks = await self._identify_system_bottlenecks()
            bottlenecks["bottlenecks"].extend(system_bottlenecks)

            # åˆ†æåº”ç”¨ç“¶é¢ˆ
            application_bottlenecks = await self._identify_application_bottlenecks()
            bottlenecks["bottlenecks"].extend(application_bottlenecks)

            # åˆ†ææ¶æ„ç“¶é¢ˆ
            architecture_bottlenecks = await self._identify_architecture_bottlenecks()
            bottlenecks["bottlenecks"].extend(architecture_bottlenecks)

            # ç»Ÿè®¡ä¸¥é‡ç¨‹åº¦
            for bottleneck in bottlenecks["bottlenecks"]:
                severity = bottleneck.get("severity", "low")
                bottlenecks["severity_summary"][severity] += 1

            # è®¡ç®—æ•´ä½“ç“¶é¢ˆæŒ‡æ•°
            total_bottlenecks = len(bottlenecks["bottlenecks"])
            severity_weights = {"critical": 4, "high": 3, "medium": 2, "low": 1}

            weighted_sum = sum(
                severity_weights[b.get("severity", "low")]
                for b in bottlenecks["bottlenecks"]
            )

            max_possible_weight = total_bottlenecks * 4 if total_bottlenecks > 0 else 1
            bottleneck_index = (
                weighted_sum / max_possible_weight if max_possible_weight > 0 else 0
            )

            bottlenecks["bottleneck_index"] = bottleneck_index
            bottlenecks["overall_severity"] = self._determine_overall_severity(
                bottlenecks["severity_summary"]
            )

            logger.info(
                f"ç“¶é¢ˆè¯†åˆ«å®Œæˆ: å‘ç°{total_bottlenecks}ä¸ªç“¶é¢ˆ, æŒ‡æ•°: {bottleneck_index:.3f}"
            )
            return bottlenecks

        except Exception as e:
            logger.error(f"ç“¶é¢ˆè¯†åˆ«å¤±è´¥: {str(e)}", exc_info=True)
            return {
                "timestamp": datetime.utcnow().isoformat(),
                "error": str(e),
                "bottlenecks": [],
            }

    async def _identify_system_bottlenecks(self) -> List[Dict[str, Any]]:
        """è¯†åˆ«ç³»ç»Ÿçº§ç“¶é¢ˆ"""
        bottlenecks = []

        try:
            system_metrics = await self.collect_system_metrics()

            # CPUç“¶é¢ˆ
            cpu_usage = system_metrics["cpu"]["percent"]
            if cpu_usage > 90:
                bottlenecks.append(
                    {
                        "type": "system",
                        "category": "cpu",
                        "description": f"CPUä½¿ç”¨ç‡è¿‡é«˜: {cpu_usage}%",
                        "severity": "critical" if cpu_usage > 95 else "high",
                        "impact": "ç³»ç»Ÿå“åº”ç¼“æ…¢ï¼Œå¯èƒ½å½±å“æ‰€æœ‰æ“ä½œ",
                        "suggested_actions": [
                            "ä¼˜åŒ–CPUå¯†é›†å‹ä»»åŠ¡",
                            "è€ƒè™‘æ°´å¹³æ‰©å±•",
                            "æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸è¿›ç¨‹",
                        ],
                    }
                )

            # å†…å­˜ç“¶é¢ˆ
            memory_usage = system_metrics["memory"]["percent"]
            if memory_usage > 90:
                bottlenecks.append(
                    {
                        "type": "system",
                        "category": "memory",
                        "description": f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_usage}%",
                        "severity": "critical" if memory_usage > 95 else "high",
                        "impact": "å¯èƒ½å‘ç”Ÿå†…å­˜äº¤æ¢ï¼Œä¸¥é‡å½±å“æ€§èƒ½",
                        "suggested_actions": [
                            "ä¼˜åŒ–å†…å­˜ä½¿ç”¨",
                            "å¢åŠ ç‰©ç†å†…å­˜",
                            "æ£€æŸ¥å†…å­˜æ³„æ¼",
                        ],
                    }
                )

            # ç£ç›˜ç“¶é¢ˆ
            disk_usage = system_metrics["disk"]["usage_percent"]
            if disk_usage > 95:
                bottlenecks.append(
                    {
                        "type": "system",
                        "category": "disk",
                        "description": f"ç£ç›˜ç©ºé—´ä¸è¶³: {disk_usage}%",
                        "severity": "critical",
                        "impact": "ç³»ç»Ÿå¯èƒ½å´©æºƒï¼Œæ— æ³•å†™å…¥æ•°æ®",
                        "suggested_actions": [
                            "æ¸…ç†ç£ç›˜ç©ºé—´",
                            "å¢åŠ ç£ç›˜å®¹é‡",
                            "è¿ç§»æ•°æ®åˆ°å¤–éƒ¨å­˜å‚¨",
                        ],
                    }
                )

            return bottlenecks

        except Exception as e:
            logger.error(f"ç³»ç»Ÿç“¶é¢ˆè¯†åˆ«å¤±è´¥: {str(e)}")
            return [
                {
                    "type": "system",
                    "category": "analysis_error",
                    "description": f"ç³»ç»Ÿç“¶é¢ˆåˆ†æå¤±è´¥: {str(e)}",
                    "severity": "medium",
                    "impact": "æ— æ³•å‡†ç¡®è¯„ä¼°ç³»ç»Ÿç“¶é¢ˆ",
                }
            ]

    async def _identify_application_bottlenecks(self) -> List[Dict[str, Any]]:
        """è¯†åˆ«åº”ç”¨çº§ç“¶é¢ˆ"""
        bottlenecks = []

        try:
            app_metrics = await self.collect_application_metrics()

            # å“åº”æ—¶é—´ç“¶é¢ˆ
            avg_response_time = app_metrics["response_time"]["average"]
            if avg_response_time > 5.0:  # 5ç§’
                bottlenecks.append(
                    {
                        "type": "application",
                        "category": "response_time",
                        "description": f"å¹³å‡å“åº”æ—¶é—´è¿‡é•¿: {avg_response_time:.2f}ç§’",
                        "severity": "critical" if avg_response_time > 10 else "high",
                        "impact": "ç”¨æˆ·ä½“éªŒå·®ï¼Œå¯èƒ½æµå¤±ç”¨æˆ·",
                        "suggested_actions": [
                            "ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢",
                            "å®ç°ç¼“å­˜ç­–ç•¥",
                            "ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦",
                        ],
                    }
                )

            # é”™è¯¯ç‡ç“¶é¢ˆ
            error_rate = app_metrics["error_rates"]["total"]
            if error_rate > 0.1:  # 10%
                bottlenecks.append(
                    {
                        "type": "application",
                        "category": "error_rate",
                        "description": f"é”™è¯¯ç‡è¿‡é«˜: {error_rate:.1%}",
                        "severity": "critical" if error_rate > 0.2 else "high",
                        "impact": "æœåŠ¡å¯é æ€§å·®ï¼Œå½±å“ä¸šåŠ¡è¿ç»­æ€§",
                        "suggested_actions": [
                            "åŠ å¼ºé”™è¯¯å¤„ç†",
                            "æ”¹è¿›è¾“å…¥éªŒè¯",
                            "å®æ–½é‡è¯•æœºåˆ¶",
                        ],
                    }
                )

            # ååé‡ç“¶é¢ˆ
            throughput = app_metrics["throughput"]["requests_per_second"]
            # è¿™é‡Œéœ€è¦æ ¹æ®å…·ä½“ä¸šåŠ¡è®¾å®šé˜ˆå€¼
            if throughput < 50:  # ç¤ºä¾‹é˜ˆå€¼
                bottlenecks.append(
                    {
                        "type": "application",
                        "category": "throughput",
                        "description": f"ç³»ç»Ÿååé‡ä½: {throughput:.1f} è¯·æ±‚/ç§’",
                        "severity": "medium",
                        "impact": "ç³»ç»Ÿå¤„ç†èƒ½åŠ›æœ‰é™ï¼Œå¯èƒ½æˆä¸ºæ‰©å±•ç“¶é¢ˆ",
                        "suggested_actions": [
                            "ä¼˜åŒ–å¹¶å‘å¤„ç†",
                            "æ”¹è¿›èµ„æºç®¡ç†",
                            "è€ƒè™‘å¼‚æ­¥å¤„ç†",
                        ],
                    }
                )

            return bottlenecks

        except Exception as e:
            logger.error(f"åº”ç”¨ç“¶é¢ˆè¯†åˆ«å¤±è´¥: {str(e)}")
            return [
                {
                    "type": "application",
                    "category": "analysis_error",
                    "description": f"åº”ç”¨ç“¶é¢ˆåˆ†æå¤±è´¥: {str(e)}",
                    "severity": "medium",
                    "impact": "æ— æ³•å‡†ç¡®è¯„ä¼°åº”ç”¨ç“¶é¢ˆ",
                }
            ]

    async def _identify_architecture_bottlenecks(self) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ¶æ„çº§ç“¶é¢ˆ"""
        bottlenecks = []

        try:
            # åˆ†ææ€§èƒ½ç»´åº¦é—´çš„åè°ƒé—®é¢˜
            dimension_scores = {}
            for dimension in PerformanceDimension:
                history = self.performance_history[dimension.value]
                if history["values"]:
                    dimension_scores[dimension.value] = history["values"][-1]

            # æ£€æŸ¥æ€§èƒ½ä¸å‡è¡¡
            if dimension_scores:
                scores = list(dimension_scores.values())
                score_range = max(scores) - min(scores)

                if score_range > 0.3:  # æ€§èƒ½ä¸å‡è¡¡é˜ˆå€¼
                    bottlenecks.append(
                        {
                            "type": "architecture",
                            "category": "performance_imbalance",
                            "description": f"ç³»ç»Ÿæ€§èƒ½ä¸å‡è¡¡ï¼Œç»´åº¦é—´å·®å¼‚è¾¾{score_range:.3f}",
                            "severity": "medium",
                            "impact": "éƒ¨åˆ†ç»´åº¦æ€§èƒ½ä¼˜ç§€ï¼Œä½†å…¶ä»–ç»´åº¦æ‹–ç´¯æ•´ä½“è¡¨ç°",
                            "suggested_actions": [
                                "é‡æ–°å¹³è¡¡èµ„æºåˆ†é…",
                                "ä¼˜åŒ–å¼±æ€§èƒ½ç»´åº¦",
                                "è€ƒè™‘æ¶æ„é‡æ„",
                            ],
                        }
                    )

            # æ£€æŸ¥è¿›åŒ–åè°ƒé—®é¢˜
            if len(self.performance_history["system"]["timestamps"]) > 10:
                system_stability = await self._calculate_dimension_stability(
                    self.performance_history["system"]["cpu_usage"]
                )
                if system_stability < 0.7:
                    bottlenecks.append(
                        {
                            "type": "architecture",
                            "category": "instability",
                            "description": f"ç³»ç»Ÿç¨³å®šæ€§ä¸è¶³: {system_stability:.3f}",
                            "severity": "high",
                            "impact": "ç³»ç»Ÿè¡¨ç°æ³¢åŠ¨å¤§ï¼Œå½±å“å¯é æ€§å’Œé¢„æµ‹æ€§",
                            "suggested_actions": [
                                "æ”¹è¿›èµ„æºç®¡ç†ç­–ç•¥",
                                "ä¼˜åŒ–è´Ÿè½½å‡è¡¡",
                                "å¢å¼ºå®¹é”™æœºåˆ¶",
                            ],
                        }
                    )

            return bottlenecks

        except Exception as e:
            logger.error(f"æ¶æ„ç“¶é¢ˆè¯†åˆ«å¤±è´¥: {str(e)}")
            return [
                {
                    "type": "architecture",
                    "category": "analysis_error",
                    "description": f"æ¶æ„ç“¶é¢ˆåˆ†æå¤±è´¥: {str(e)}",
                    "severity": "medium",
                    "impact": "æ— æ³•å‡†ç¡®è¯„ä¼°æ¶æ„ç“¶é¢ˆ",
                }
            ]

    def _determine_overall_severity(self, severity_summary: Dict[str, int]) -> str:
        """ç¡®å®šæ•´ä½“ä¸¥é‡ç¨‹åº¦"""
        if severity_summary["critical"] > 0:
            return "critical"
        elif severity_summary["high"] > 0:
            return "high"
        elif severity_summary["medium"] > 0:
            return "medium"
        else:
            return "low"

    async def _generate_recommendations(
        self, analysis_results: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []

        try:
            # åŸºäºç“¶é¢ˆç”Ÿæˆå»ºè®®
            bottlenecks = analysis_results.get("bottlenecks", {}).get("bottlenecks", [])
            for bottleneck in bottlenecks:
                if "suggested_actions" in bottleneck:
                    for action in bottleneck["suggested_actions"]:
                        recommendations.append(
                            {
                                "type": "bottleneck_resolution",
                                "priority": bottleneck.get("severity", "medium"),
                                "description": action,
                                "related_bottleneck": bottleneck.get("description", ""),
                                "estimated_impact": (
                                    "high"
                                    if bottleneck.get("severity")
                                    in ["critical", "high"]
                                    else "medium"
                                ),
                            }
                        )

            # åŸºäºè¶‹åŠ¿ç”Ÿæˆå»ºè®®
            trends = analysis_results.get("trends", {})
            for dimension, trend_info in trends.items():
                if trend_info.get("trend") == "declining" and trend_info.get(
                    "trend_strength"
                ) in ["moderate", "strong"]:
                    recommendations.append(
                        {
                            "type": "preventive_maintenance",
                            "priority": "medium",
                            "description": f"æ£€æµ‹åˆ°{dimension}æ€§èƒ½ä¸‹é™è¶‹åŠ¿ï¼Œå»ºè®®è¿›è¡Œé¢„é˜²æ€§ä¼˜åŒ–",
                            "related_metric": dimension,
                            "trend_strength": trend_info.get("trend_strength"),
                            "estimated_impact": "medium",
                        }
                    )

            # åŸºäºå¼‚å¸¸ç”Ÿæˆå»ºè®®
            anomalies = analysis_results.get("anomalies", {}).get("anomalies", {})
            if anomalies:
                recommendations.append(
                    {
                        "type": "anomaly_investigation",
                        "priority": "high",
                        "description": f"æ£€æµ‹åˆ°{len(anomalies)}ä¸ªæ€§èƒ½å¼‚å¸¸ï¼Œå»ºè®®ç«‹å³è°ƒæŸ¥",
                        "anomaly_count": len(anomalies),
                        "estimated_impact": "high",
                    }
                )

            # å»é‡å’Œä¼˜å…ˆçº§æ’åº
            unique_recommendations = []
            seen_descriptions = set()

            for rec in recommendations:
                if rec["description"] not in seen_descriptions:
                    seen_descriptions.add(rec["description"])
                    unique_recommendations.append(rec)

            # æŒ‰ä¼˜å…ˆçº§æ’åº
            priority_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
            unique_recommendations.sort(
                key=lambda x: priority_order.get(x["priority"], 3)
            )

            return unique_recommendations[:10]  # è¿”å›å‰10ä¸ªå»ºè®®

        except Exception as e:
            logger.error(f"å»ºè®®ç”Ÿæˆå¤±è´¥: {str(e)}")
            return [
                {
                    "type": "analysis_error",
                    "priority": "medium",
                    "description": f"å»ºè®®ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                    "estimated_impact": "low",
                }
            ]

    def _get_cached_analysis(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜çš„åˆ†æç»“æœ"""
        try:
            if cache_key in self.analysis_cache:
                cached_data = self.analysis_cache[cache_key]
                cache_time = cached_data.get("cache_time")

                if (
                    cache_time
                    and (datetime.utcnow() - cache_time).total_seconds()
                    < self.cache_ttl
                ):
                    return cached_data.get("data")

            return None

        except Exception as e:
            logger.error(f"ç¼“å­˜è·å–å¤±è´¥: {str(e)}")
            return None

    def _cache_analysis(self, cache_key: str, data: Dict[str, Any]):
        """ç¼“å­˜åˆ†æç»“æœ"""
        try:
            self.analysis_cache[cache_key] = {
                "data": data,
                "cache_time": datetime.utcnow(),
            }

            # æ¸…ç†è¿‡æœŸç¼“å­˜
            current_time = datetime.utcnow()
            expired_keys = [
                key
                for key, cached in self.analysis_cache.items()
                if (current_time - cached["cache_time"]).total_seconds()
                > self.cache_ttl
            ]

            for key in expired_keys:
                del self.analysis_cache[key]

        except Exception as e:
            logger.error(f"ç¼“å­˜å­˜å‚¨å¤±è´¥: {str(e)}")

    async def get_performance_history(
        self, dimension: str, hours: int = 24
    ) -> Dict[str, Any]:
        """
        è·å–æ€§èƒ½å†å²æ•°æ®

        Args:
            dimension: æ€§èƒ½ç»´åº¦
            hours: æ—¶é—´èŒƒå›´(å°æ—¶)

        Returns:
            Dict: å†å²æ•°æ®
        """
        try:
            if dimension not in self.performance_history:
                return {"error": f"æœªçŸ¥æ€§èƒ½ç»´åº¦: {dimension}"}

            history = self.performance_history[dimension]
            if not history["timestamps"]:
                return {"data": [], "timestamps": []}

            # è®¡ç®—æ—¶é—´è¾¹ç•Œ
            cutoff_time = datetime.utcnow() - timedelta(hours=hours)

            # è¿‡æ»¤æ•°æ®
            filtered_data = []
            filtered_timestamps = []

            for i, timestamp in enumerate(history["timestamps"]):
                if timestamp >= cutoff_time:
                    filtered_timestamps.append(timestamp.isoformat())
                    if i < len(history["values"]):
                        filtered_data.append(history["values"][i])

            return {
                "dimension": dimension,
                "data": filtered_data,
                "timestamps": filtered_timestamps,
                "data_points": len(filtered_data),
                "time_range_hours": hours,
            }

        except Exception as e:
            logger.error(f"å†å²æ•°æ®è·å–å¤±è´¥: {str(e)}")
            return {"error": str(e)}

    async def get_health_status(self) -> Dict[str, Any]:
        """è·å–å¥åº·çŠ¶æ€"""
        try:
            # æ£€æŸ¥æ•°æ®æ”¶é›†çŠ¶æ€
            data_status = {}
            total_data_points = 0
            active_dimensions = 0

            for dimension in PerformanceDimension:
                history = self.performance_history[dimension.value]
                data_points = len(history["values"])
                total_data_points += data_points

                if data_points > 0:
                    active_dimensions += 1
                    data_status[dimension.value] = {
                        "data_points": data_points,
                        "last_update": (
                            history["timestamps"][-1].isoformat()
                            if history["timestamps"]
                            else None
                        ),
                        "status": "active",
                    }
                else:
                    data_status[dimension.value] = {
                        "data_points": 0,
                        "status": "inactive",
                    }

            # è¯„ä¼°æ•´ä½“å¥åº·çŠ¶æ€
            health_score = (
                active_dimensions / len(PerformanceDimension)
                if PerformanceDimension
                else 1.0
            )

            if health_score >= 0.8:
                status = "healthy"
            elif health_score >= 0.5:
                status = "degraded"
            else:
                status = "unhealthy"

            return {
                "status": status,
                "health_score": health_score,
                "initialized": self.is_initialized,
                "analysis_count": self.analysis_count,
                "active_dimensions": active_dimensions,
                "total_data_points": total_data_points,
                "data_status": data_status,
                "last_analysis": (
                    self.last_analysis_time.isoformat()
                    if self.last_analysis_time
                    else None
                ),
            }

        except Exception as e:
            return {
                "status": "unhealthy",
                "health_score": 0.0,
                "error": str(e),
                "initialized": self.is_initialized,
            }

    async def generate_performance_report(
        self, report_type: str = "standard"
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š

        Args:
            report_type: æŠ¥å‘Šç±»å‹

        Returns:
            Dict: æ€§èƒ½æŠ¥å‘Š
        """
        try:
            logger.info(f"ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š: {report_type}")

            # æ‰§è¡Œç»¼åˆåˆ†æ
            analysis = await self.get_comprehensive_analysis()

            # ç”ŸæˆæŠ¥å‘Š
            report = {
                "report_id": f"report_{int(time.time())}",
                "generated_at": datetime.utcnow().isoformat(),
                "report_type": report_type,
                "executive_summary": await self._generate_executive_summary(analysis),
                "detailed_analysis": analysis,
                "recommendations": analysis.get("recommendations", []),
                "metadata": {
                    "analysis_duration": "å®æ—¶",
                    "data_sources": [
                        "system_metrics",
                        "application_metrics",
                        "performance_history",
                    ],
                    "report_version": "1.0",
                },
            }

            return report

        except Exception as e:
            logger.error(f"æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                "report_id": f"report_error_{int(time.time())}",
                "generated_at": datetime.utcnow().isoformat(),
                "error": str(e),
                "report_type": report_type,
            }

    async def _generate_executive_summary(
        self, analysis: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæ‰§è¡Œæ‘˜è¦"""
        try:
            summary = analysis.get("summary", {})
            bottlenecks = analysis.get("bottlenecks", {})
            trends = analysis.get("trends", {})
            anomalies = analysis.get("anomalies", {})

            return {
                "overall_performance": summary.get("overall_score", 0.0),
                "performance_status": summary.get("status", "unknown"),
                "key_metrics": {
                    "bottlenecks_count": len(bottlenecks.get("bottlenecks", [])),
                    "critical_bottlenecks": bottlenecks.get("severity_summary", {}).get(
                        "critical", 0
                    ),
                    "anomalies_detected": anomalies.get("detected", False),
                    "improving_trends": sum(
                        1 for t in trends.values() if t.get("trend") == "improving"
                    ),
                    "declining_trends": sum(
                        1 for t in trends.values() if t.get("trend") == "declining"
                    ),
                },
                "top_recommendations": [
                    rec
                    for rec in analysis.get("recommendations", [])
                    if rec.get("priority") in ["critical", "high"]
                ][
                    :3
                ],  # å‰3ä¸ªé«˜ä¼˜å…ˆçº§å»ºè®®
            }

        except Exception as e:
            logger.error(f"æ‰§è¡Œæ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                "overall_performance": 0.0,
                "performance_status": "analysis_error",
                "error": str(e),
            }

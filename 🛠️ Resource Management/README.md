# ğŸ› ï¸ èµ„æºç®¡ç†ç³»ç»Ÿ

## ğŸ“‹ ç³»ç»Ÿæ¦‚è¿°

èµ„æºç®¡ç†ç³»ç»Ÿè´Ÿè´£ç›‘æ§ã€è°ƒé…å’Œä¼˜åŒ–æ‰€æœ‰AI StackæœåŠ¡çš„ç³»ç»Ÿèµ„æºä½¿ç”¨ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šé«˜æ•ˆè¿è¡Œã€‚

## âœ¨ æ ¸å¿ƒåŠŸèƒ½

### 1. èµ„æºç›‘æ§ ğŸ“Š
- **å®æ—¶ç›‘æ§**:
  - CPUä½¿ç”¨ç‡ï¼ˆæ€»ä½“+per-coreï¼‰
  - å†…å­˜ä½¿ç”¨ï¼ˆæ€»é‡/å·²ç”¨/å¯ç”¨/Swapï¼‰
  - ç£ç›˜ä½¿ç”¨ï¼ˆå†…ç½®512GB + å¤–ç½®2TBï¼‰
  - ç½‘ç»œIOç»Ÿè®¡
- **æœåŠ¡çº§ç›‘æ§**:
  - æ¯ä¸ªæœåŠ¡çš„èµ„æºå ç”¨
  - è¿›ç¨‹æ•°é‡å’ŒçŠ¶æ€
  - èµ„æºä½¿ç”¨è¶‹åŠ¿
- **å¥åº·æ£€æŸ¥**:
  - èµ„æºé˜ˆå€¼å‘Šè­¦
  - çŠ¶æ€è¯„ä¼°ï¼ˆhealthy/warning/criticalï¼‰
  - ä¼˜åŒ–å»ºè®®ç”Ÿæˆ

### 2. èµ„æºè°ƒé… âš™ï¸
- **æ™ºèƒ½åˆ†é…**:
  - æŒ‰æœåŠ¡ä¼˜å…ˆçº§åˆ†é…èµ„æº
  - åŠ¨æ€è®¡ç®—æœ€ä¼˜åˆ†é…æ–¹æ¡ˆ
  - æ”¯æŒæ‰‹åŠ¨è°ƒæ•´
- **èµ„æºé™åˆ¶**:
  - è®¾ç½®æœåŠ¡å†…å­˜ä¸Šé™
  - é™åˆ¶CPUä½¿ç”¨ç‡
  - é˜²æ­¢èµ„æºæ»¥ç”¨
- **è‡ªé€‚åº”è°ƒæ•´**:
  - æ ¹æ®ä½¿ç”¨æƒ…å†µè‡ªåŠ¨è°ƒæ•´
  - å¹³è¡¡å„æœåŠ¡èµ„æº
  - ä¼˜åŒ–èµ„æºåˆ©ç”¨ç‡

### 3. å†²çªæ£€æµ‹ ğŸš¨
- **å®æ—¶æ£€æµ‹**:
  - å†…å­˜ä¸è¶³æ£€æµ‹
  - CPUè¿‡è½½æ£€æµ‹
  - ç£ç›˜ç©ºé—´ä¸è¶³æ£€æµ‹
- **æ™ºèƒ½åˆ†æ**:
  - è¯†åˆ«å—å½±å“æœåŠ¡
  - è¯„ä¼°å†²çªä¸¥é‡ç¨‹åº¦
  - é¢„æµ‹æ–°æœåŠ¡å¯åŠ¨å½±å“
- **è§£å†³æ–¹æ¡ˆ**:
  - è‡ªåŠ¨ç”Ÿæˆå¤šç§è§£å†³æ–¹æ¡ˆ
  - ç”¨æˆ·å‹å¥½çš„é€‰æ‹©ç•Œé¢
  - ä¸€é”®åº”ç”¨è§£å†³æ–¹æ¡ˆ

### 4. å¯åŠ¨ç®¡ç† ğŸš€
- **æœ‰åºå¯åŠ¨**:
  - æŒ‰ä¾èµ–å…³ç³»å¯åŠ¨æœåŠ¡
  - ç­‰å¾…å‰ç½®æœåŠ¡å°±ç»ª
  - å¥åº·æ£€æŸ¥éªŒè¯
- **å¯åŠ¨é¡ºåº** (ç¬¦åˆç”¨æˆ·éœ€æ±‚8.3):
  1. Dockerï¼ˆä¼˜å…ˆçº§10ï¼‰
  2. Ollamaï¼ˆä¼˜å…ˆçº§9ï¼‰
  3. OpenWebUIï¼ˆä¼˜å…ˆçº§8ï¼‰
  4. RAGæœåŠ¡ï¼ˆä¼˜å…ˆçº§7ï¼‰
  5. ERPåç«¯/å‰ç«¯ï¼ˆä¼˜å…ˆçº§6ï¼‰
  6. è‚¡ç¥¨/è¶‹åŠ¿/å†…å®¹/ä»»åŠ¡æœåŠ¡ï¼ˆä¼˜å…ˆçº§4-5ï¼‰
- **è‡ªåŠ¨å¯åŠ¨**:
  - ç”Ÿæˆå¯åŠ¨è„šæœ¬
  - macOS LaunchAgenté…ç½®
  - ç”µè„‘é‡å¯åè‡ªåŠ¨å¯åŠ¨

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
ğŸ› ï¸ Resource Management/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ resource_monitor.py      # èµ„æºç›‘æ§å™¨
â”‚   â”œâ”€â”€ resource_allocator.py    # èµ„æºè°ƒé…å™¨
â”‚   â”œâ”€â”€ conflict_detector.py     # å†²çªæ£€æµ‹å™¨
â”‚   â””â”€â”€ startup_manager.py       # å¯åŠ¨ç®¡ç†å™¨
â””â”€â”€ api/
    â”œâ”€â”€ main.py                  # FastAPIä¸»åº”ç”¨
    â””â”€â”€ resource_api.py          # APIæ¥å£
```

## ğŸ¯ æ ¸å¿ƒç»„ä»¶

### ResourceMonitor (èµ„æºç›‘æ§å™¨)
```python
monitor = ResourceMonitor()

# è·å–ç³»ç»Ÿèµ„æº
resources = monitor.get_system_resources()

# æ£€æŸ¥èµ„æºçŠ¶æ€
status = monitor.check_resource_status(resources)

# è·å–æœåŠ¡èµ„æºä½¿ç”¨
usage = monitor.get_service_resource_usage("ollama")

# ä¼°ç®—å¯ç”¨èµ„æº
available = monitor.estimate_available_resources()
```

### ResourceAllocator (èµ„æºè°ƒé…å™¨)
```python
allocator = ResourceAllocator()

# è®¡ç®—èµ„æºåˆ†é…
allocation = allocator.calculate_resource_allocation(
    available_memory_gb=16,
    available_cpu_percent=70,
    active_services=["ollama", "rag-service"]
)

# è°ƒæ•´æœåŠ¡èµ„æº
allocator.adjust_allocation_for_service(
    "ollama",
    target_memory_gb=8,
    target_cpu_percent=40
)
```

### ConflictDetector (å†²çªæ£€æµ‹å™¨)
```python
detector = ConflictDetector(monitor, allocator)

# æ£€æµ‹å†²çª
conflicts = detector.detect_conflicts(active_services)

# ç”Ÿæˆè§£å†³æ–¹æ¡ˆ
options = detector.generate_resolution_options(conflicts)

# åº”ç”¨è§£å†³æ–¹æ¡ˆ
result = detector.apply_resolution(option_id=1, conflict=conflicts)
```

### StartupManager (å¯åŠ¨ç®¡ç†å™¨)
```python
manager = StartupManager()

# å¯åŠ¨æ‰€æœ‰æœåŠ¡
result = manager.start_all_services()

# è·å–æœåŠ¡çŠ¶æ€
status = manager.get_service_status("ollama")

# ç”Ÿæˆè‡ªåŠ¨å¯åŠ¨è„šæœ¬
script = manager.generate_auto_start_script(
    "/Users/ywc/ai-stack-super-enhanced/scripts/auto_start.sh"
)
```

## ğŸ“¡ APIæ¥å£

### èµ„æºç›‘æ§ (6ä¸ªæ¥å£)
```bash
GET  /api/resources/system              # è·å–ç³»ç»Ÿèµ„æº
GET  /api/resources/available           # è·å–å¯ç”¨èµ„æº
GET  /api/resources/service/{name}      # è·å–æœåŠ¡èµ„æº
GET  /api/resources/trend               # è·å–èµ„æºè¶‹åŠ¿
GET  /api/resources/suggestions         # è·å–ä¼˜åŒ–å»ºè®®
```

### èµ„æºåˆ†é… (4ä¸ªæ¥å£)
```bash
POST /api/resources/allocate            # åˆ†é…èµ„æº
PUT  /api/resources/adjust              # è°ƒæ•´èµ„æº
GET  /api/resources/recommendation/{name} # è·å–å»ºè®®
POST /api/resources/balance             # å¹³è¡¡èµ„æº
```

### å†²çªæ£€æµ‹ (5ä¸ªæ¥å£)
```bash
GET  /api/resources/conflicts/detect    # æ£€æµ‹å†²çª
GET  /api/resources/conflicts/solutions # è·å–è§£å†³æ–¹æ¡ˆ
POST /api/resources/conflicts/resolve   # è§£å†³å†²çª
GET  /api/resources/conflicts/history   # å†²çªå†å²
GET  /api/resources/conflicts/predict   # é¢„æµ‹å†²çª
```

### å¯åŠ¨ç®¡ç† (7ä¸ªæ¥å£)
```bash
POST /api/resources/startup/start-all   # å¯åŠ¨æ‰€æœ‰æœåŠ¡
POST /api/resources/startup/stop-all    # åœæ­¢æ‰€æœ‰æœåŠ¡
POST /api/resources/startup/restart/{name} # é‡å¯æœåŠ¡
GET  /api/resources/startup/status      # æ‰€æœ‰æœåŠ¡çŠ¶æ€
GET  /api/resources/startup/status/{name} # æœåŠ¡çŠ¶æ€
GET  /api/resources/startup/script      # ç”Ÿæˆå¯åŠ¨è„šæœ¬
GET  /api/resources/startup/launchd     # ç”ŸæˆLaunchAgent
```

### ç»Ÿè®¡ (1ä¸ªæ¥å£)
```bash
GET  /api/resources/statistics/overview # èµ„æºç»Ÿè®¡æ¦‚è§ˆ
```

**æ€»è®¡**: **23ä¸ªå®Œæ•´APIæ¥å£** ğŸ”Œ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨æœåŠ¡

```bash
cd "/Users/ywc/ai-stack-super-enhanced/ğŸ› ï¸ Resource Management"
python -m uvicorn api.main:app --host 0.0.0.0 --port 8018 --reload
```

### 2. è®¿é—®ç³»ç»Ÿ

- **APIæ–‡æ¡£**: http://localhost:8018/docs
- **å¥åº·æ£€æŸ¥**: http://localhost:8018/health

### 3. ä½¿ç”¨ç¤ºä¾‹

#### ç›‘æ§ç³»ç»Ÿèµ„æº

```python
import requests

response = requests.get("http://localhost:8018/api/resources/system")
data = response.json()

print(f"CPUä½¿ç”¨ç‡: {data['resources']['cpu']['total_percent']}%")
print(f"å†…å­˜ä½¿ç”¨ç‡: {data['resources']['memory']['percent']}%")
print(f"çŠ¶æ€: {data['status']['overall']}")
```

#### æ£€æµ‹èµ„æºå†²çª

```python
active_services = ["ollama", "rag-service", "erp-backend"]

response = requests.get(
    "http://localhost:8018/api/resources/conflicts/detect",
    params={"services": active_services}
)

conflicts = response.json()

if conflicts["conflicts"]["has_conflict"]:
    print(f"æ£€æµ‹åˆ°å†²çª: {conflicts['conflicts']['conflict_type']}")
    print(f"ä¸¥é‡ç¨‹åº¦: {conflicts['conflicts']['severity']}")
```

#### è·å–å¹¶åº”ç”¨è§£å†³æ–¹æ¡ˆ

```python
# è·å–è§£å†³æ–¹æ¡ˆ
response = requests.get(
    "http://localhost:8018/api/resources/conflicts/solutions",
    params={"services": active_services}
)

solutions = response.json()

# åº”ç”¨æ–¹æ¡ˆ1
response = requests.post(
    "http://localhost:8018/api/resources/conflicts/resolve",
    json={"option_id": 1},
    params={"services": active_services}
)

result = response.json()
print(f"è§£å†³æ–¹æ¡ˆå·²åº”ç”¨: {result['actions_taken']}")
```

#### å¯åŠ¨æ‰€æœ‰æœåŠ¡

```python
response = requests.post(
    "http://localhost:8018/api/resources/startup/start-all",
    params={"skip_optional": False}
)

result = response.json()
print(f"å¯åŠ¨å®Œæˆ: {len(result['result']['started'])}ä¸ªæœåŠ¡")
print(f"å¤±è´¥: {len(result['result']['failed'])}ä¸ªæœåŠ¡")
```

## ğŸ“Š æœåŠ¡ä¼˜å…ˆçº§é…ç½®

| æœåŠ¡ | ä¼˜å…ˆçº§ | å†…å­˜èŒƒå›´ | CPUèŒƒå›´ | è¯´æ˜ |
|------|--------|----------|---------|------|
| Docker | 10 | 2-8 GB | 10-50% | æœ€é«˜ä¼˜å…ˆçº§ |
| Ollama | 9 | 4-12 GB | 20-60% | AIæ¨¡å‹æœåŠ¡ |
| OpenWebUI | 8 | 1-4 GB | 5-30% | ç»Ÿä¸€å…¥å£ |
| RAG | 7 | 2-6 GB | 10-40% | çŸ¥è¯†æ£€ç´¢ |
| ERP | 6 | 1-3 GB | 5-25% | ä¼ä¸šç®¡ç† |
| å…¶ä»–æœåŠ¡ | 4-5 | 1-3 GB | 5-25% | ä¸šåŠ¡æœåŠ¡ |

## ğŸ”§ ç³»ç»Ÿé…ç½®

### é€‚é…çš„ç¡¬ä»¶ï¼ˆç”¨æˆ·éœ€æ±‚8.8ï¼‰
- **å‹å·**: MacBook Pro 15-inch, 2018
- **CPU**: 2.6 GHz å…­æ ¸ Intel Core i7
- **å†…å­˜**: 32 GB 2400 MHz DDR4
- **å†…ç½®ç¡¬ç›˜**: 512GB SSD
- **å¤–æ¥ç¡¬ç›˜**: 
  - Huawei-1: 1024GB
  - Huawei-2: 1024GB
- **ç³»ç»Ÿ**: macOS Sequoia 15.6.1

### èµ„æºé˜ˆå€¼é…ç½®
```python
thresholds = {
    "cpu_warning": 70,       # CPUè­¦å‘Šé˜ˆå€¼
    "cpu_critical": 85,      # CPUå±é™©é˜ˆå€¼
    "memory_warning": 75,    # å†…å­˜è­¦å‘Šé˜ˆå€¼
    "memory_critical": 90,   # å†…å­˜å±é™©é˜ˆå€¼
    "disk_warning": 80,      # ç£ç›˜è­¦å‘Šé˜ˆå€¼
    "disk_critical": 95      # ç£ç›˜å±é™©é˜ˆå€¼
}
```

## ğŸŒŸ ç‰¹è‰²åŠŸèƒ½

### 1. ç”¨æˆ·å‹å¥½çš„å†²çªè§£å†³
å½“èµ„æºå†²çªæ—¶ï¼Œç³»ç»Ÿä¼šï¼š
1. æ£€æµ‹å†²çªç±»å‹å’Œä¸¥é‡ç¨‹åº¦
2. ç”Ÿæˆ4ç§è§£å†³æ–¹æ¡ˆä¾›é€‰æ‹©
3. ç”¨æˆ·é€‰æ‹©åä¸€é”®åº”ç”¨
4. å®æ—¶åé¦ˆæ‰§è¡Œç»“æœ

### 2. æ™ºèƒ½å¯åŠ¨é¡ºåº
- æŒ‰ä¾èµ–å…³ç³»è‡ªåŠ¨æ’åº
- ç­‰å¾…å‰ç½®æœåŠ¡å°±ç»ª
- å¤±è´¥æ—¶æ™ºèƒ½å¤„ç†
- æ”¯æŒè·³è¿‡å¯é€‰æœåŠ¡

### 3. è‡ªåŠ¨å¯åŠ¨é…ç½®
```bash
# ç”Ÿæˆå¯åŠ¨è„šæœ¬
curl http://localhost:8018/api/resources/startup/script > auto_start.sh
chmod +x auto_start.sh

# ç”Ÿæˆ LaunchAgent
curl http://localhost:8018/api/resources/startup/launchd > \
  ~/Library/LaunchAgents/com.aistack.startup.plist
launchctl load ~/Library/LaunchAgents/com.aistack.startup.plist
```

### 4. é¢„æµ‹æ€§å†²çªæ£€æµ‹
åœ¨å¯åŠ¨æ–°æœåŠ¡å‰ï¼Œå¯ä»¥é¢„æµ‹æ˜¯å¦ä¼šå¯¼è‡´èµ„æºå†²çªï¼š
```python
prediction = detector.predict_conflict(
    new_service="new-ml-service",
    current_services=["ollama", "rag-service"]
)

if not prediction["can_start"]:
    print("è­¦å‘Š: å¯åŠ¨è¯¥æœåŠ¡å¯èƒ½å¯¼è‡´å†²çª")
    print(prediction["warnings"])
```

## ğŸ“ˆ æ€§èƒ½ç‰¹æ€§

- **ç›‘æ§å»¶è¿Ÿ**: < 100ms
- **å†²çªæ£€æµ‹**: < 200ms
- **APIå“åº”**: < 50ms
- **æœåŠ¡å¯åŠ¨**: æ ¹æ®æœåŠ¡ä¸åŒï¼ˆ5-30ç§’ï¼‰

## ğŸ›¡ï¸ å®‰å…¨ç‰¹æ€§

- èµ„æºé™åˆ¶ä¿æŠ¤
- ä¼˜å…ˆçº§æ§åˆ¶
- å¿…é¡»æœåŠ¡ä¿æŠ¤
- æ“ä½œæ—¥å¿—è®°å½•

## ğŸ”— ä¸å…¶ä»–ç³»ç»Ÿé›†æˆ

### ä¸ä»»åŠ¡ä»£ç†é›†æˆ
```python
# ä»»åŠ¡ä»£ç†åœ¨æ‰§è¡Œä»»åŠ¡å‰æ£€æŸ¥èµ„æº
available = monitor.estimate_available_resources()
if not available["can_start_new_service"]:
    # ç­‰å¾…èµ„æºé‡Šæ”¾æˆ–æš‚åœä½ä¼˜å…ˆçº§ä»»åŠ¡
    pass
```

### ä¸OpenWebUIé›†æˆ
- é€šè¿‡èŠå¤©ç•Œé¢æŸ¥è¯¢èµ„æºçŠ¶æ€
- æ¥æ”¶èµ„æºå‘Šè­¦é€šçŸ¥
- æ‰‹åŠ¨è§¦å‘èµ„æºä¼˜åŒ–

## ğŸ“ å¼€å‘è®¡åˆ’

- [x] æ ¸å¿ƒèµ„æºç›‘æ§
- [x] èµ„æºåŠ¨æ€è°ƒé…
- [x] å†²çªæ£€æµ‹å’Œè§£å†³
- [x] å¯åŠ¨é¡ºåºç®¡ç†
- [x] APIæ¥å£
- [x] è‡ªåŠ¨å¯åŠ¨é…ç½®
- [ ] Webç®¡ç†ç•Œé¢
- [ ] å†å²æ•°æ®åˆ†æ
- [ ] æœºå™¨å­¦ä¹ ä¼˜åŒ–
- [ ] åˆ†å¸ƒå¼èµ„æºç®¡ç†

## ğŸ‰ ç³»ç»Ÿä¼˜åŠ¿

1. **æ™ºèƒ½åŒ–**: AIé©±åŠ¨çš„èµ„æºä¼˜åŒ–
2. **è‡ªåŠ¨åŒ–**: æ— éœ€äººå·¥å¹²é¢„çš„èµ„æºç®¡ç†
3. **å¯é æ€§**: å®Œå–„çš„å†²çªæ£€æµ‹å’Œè§£å†³
4. **æ˜“ç”¨æ€§**: ç”¨æˆ·å‹å¥½çš„äº¤äº’æ–¹å¼
5. **å¯æ‰©å±•**: æ˜“äºæ·»åŠ æ–°æœåŠ¡

---

**å¼€å‘å®Œæˆåº¦**: 75% âœ…  
**æœ€åæ›´æ–°**: 2025-11-03  
**ç‰ˆæœ¬**: v1.0.0  
**APIç«¯å£**: 8018


"""
è‡ªæˆ‘å­¦ä¹ ç›‘æ§ç³»ç»Ÿ
æ•´åˆè‡ªğŸ§  Self Learning System/ï¼Œèåˆåˆ°è¶…çº§Agent
é›†æˆå·¥ä½œæµç›‘æ§å’Œèµ„æºè‡ªåŠ¨è°ƒèŠ‚åŠŸèƒ½
"""

import asyncio
from typing import Dict, List, Optional, Any
from datetime import datetime
import json

from .workflow_monitor import WorkflowMonitor
from .resource_auto_adjuster import ResourceAutoAdjuster
from .learning_events import LearningEventBus, LearningEventType

class SelfLearningMonitor:
    """
    è‡ªæˆ‘å­¦ä¹ ç›‘æ§ç³»ç»Ÿ
    
    åŠŸèƒ½ï¼š
    1. ç›‘æ§AIå·¥ä½œæµ9æ­¥éª¤
    2. è¯†åˆ«é—®é¢˜å’Œä¼˜åŒ–æœºä¼š
    3. è°ƒç”¨ç¼–ç¨‹åŠ©æ‰‹ä¼˜åŒ–ä»£ç 
    4. å°†é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆå­˜å…¥RAG
    """
    
    def __init__(self, rag_service=None, coding_assistant=None, resource_manager=None, event_bus: Optional[LearningEventBus] = None):
        self.rag_service = rag_service
        self.coding_assistant = coding_assistant
        self.resource_manager = resource_manager
        self.event_bus = event_bus
        self.workflow_logs = []
        self.problems = []
        self.solutions = []
        self.latest_recommendations: Dict[str, Dict[str, Any]] = {}
        self.latest_resource_signals: List[Dict[str, Any]] = []

    def set_event_bus(self, event_bus: LearningEventBus):
        self.event_bus = event_bus

    async def _publish_event(self, event_type: LearningEventType, severity: str, payload: Dict[str, Any]):
        if self.event_bus:
            await self.event_bus.publish_event(
                event_type=event_type,
                source="self_learning_monitor",
                severity=severity,
                payload=payload
            )

        # å¦‚æœcoding_assistantæ˜¯URLï¼Œåˆ›å»ºHTTPå®¢æˆ·ç«¯
        if isinstance(coding_assistant, str):
            self.coding_assistant_url = coding_assistant
        else:
            self.coding_assistant_url = None
        
        # åˆå§‹åŒ–å·¥ä½œæµç›‘æ§å™¨
        self.workflow_monitor = WorkflowMonitor(
            rag_service=rag_service,
            resource_manager=resource_manager
        )
        
        # åˆå§‹åŒ–èµ„æºè‡ªåŠ¨è°ƒèŠ‚å™¨
        self.resource_adjuster = ResourceAutoAdjuster(
            resource_manager=resource_manager
        )
        
        # å¯åŠ¨åå°ç›‘æ§ä»»åŠ¡
        self._background_task = None
        
    async def monitor_workflow(self, workflow_data: Dict[str, Any]):
        """
        ç›‘æ§AIå·¥ä½œæµ
        
        Args:
            workflow_data: å·¥ä½œæµæ•°æ®ï¼ŒåŒ…å«9æ­¥éª¤çš„å®Œæ•´ä¿¡æ¯
        """
        # è®°å½•å·¥ä½œæµæ—¥å¿—
        self.workflow_logs.append({
            **workflow_data,
            "timestamp": datetime.now().isoformat()
        })
        
        # åˆ†æå·¥ä½œæµæ€§èƒ½
        await self._analyze_performance(workflow_data)
        
        # æ£€æµ‹é—®é¢˜
        problems = await self._detect_problems(workflow_data)
        if problems:
            await self._handle_problems(problems, workflow_data)
    
    async def _analyze_performance(self, workflow_data: Dict):
        """åˆ†æå·¥ä½œæµæ€§èƒ½â­å¢å¼ºç‰ˆ"""
        response_time = workflow_data.get("response_time", 0)
        
        # åˆ†æå„æ­¥éª¤è€—æ—¶
        step_times = self._analyze_step_times(workflow_data)
        
        # å¦‚æœå“åº”æ—¶é—´è¶…è¿‡2ç§’ï¼Œè®°å½•æ€§èƒ½é—®é¢˜
        if response_time > 2.0:
            await self._record_performance_issue(response_time, workflow_data)
        
        # åˆ†ææ€§èƒ½è¶‹åŠ¿ï¼ˆåŸºäºå†å²æ•°æ®ï¼‰
        if len(self.workflow_logs) > 10:
            trend = self._analyze_performance_trend()
            if trend.get("degrading"):
                await self._handle_performance_degradation(trend, workflow_data)
        
        # è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
        bottlenecks = self._identify_bottlenecks(step_times, response_time)
        if bottlenecks:
            await self._handle_bottlenecks(bottlenecks, workflow_data)
    
    def _analyze_step_times(self, workflow_data: Dict) -> Dict[str, float]:
        """åˆ†æå„æ­¥éª¤è€—æ—¶"""
        # ä¼°ç®—å„æ­¥éª¤è€—æ—¶ï¼ˆåŸºäºå·¥ä½œæµæ•°æ®ï¼‰
        step_times = {}
        
        # ç¬¬1æ¬¡RAGæ£€ç´¢è€—æ—¶
        rag_1 = workflow_data.get("rag_1", {})
        if rag_1:
            step_times["rag_1"] = 0.3  # ä¼°ç®—å€¼
        
        # ä¸“å®¶è·¯ç”±è€—æ—¶
        expert = workflow_data.get("expert", {})
        if expert:
            step_times["routing"] = 0.1
        
        # æ¨¡å—æ‰§è¡Œè€—æ—¶
        execution = workflow_data.get("execution", {})
        if execution:
            step_times["execution"] = 0.5  # ä¼°ç®—å€¼
        
        # ç¬¬2æ¬¡RAGæ£€ç´¢è€—æ—¶
        rag_2 = workflow_data.get("rag_2", {})
        if rag_2:
            step_times["rag_2"] = 0.4
        
        # ç”Ÿæˆå›å¤è€—æ—¶
        response = workflow_data.get("response", {})
        if response:
            step_times["response"] = 0.2
        
        return step_times
    
    def _analyze_performance_trend(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        if len(self.workflow_logs) < 10:
            return {"degrading": False}
        
        # è·å–æœ€è¿‘20æ¡è®°å½•
        recent_logs = self.workflow_logs[-20:]
        older_logs = self.workflow_logs[-40:-20] if len(self.workflow_logs) >= 40 else []
        
        if not older_logs:
            return {"degrading": False}
        
        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        recent_avg = sum(
            log.get("response_time", 0) 
            for log in recent_logs 
            if "response_time" in log
        ) / len(recent_logs)
        
        older_avg = sum(
            log.get("response_time", 0) 
            for log in older_logs 
            if "response_time" in log
        ) / len(older_logs)
        
        # å¦‚æœæ€§èƒ½ä¸‹é™è¶…è¿‡20%ï¼Œè®¤ä¸ºåœ¨é€€åŒ–
        degrading = recent_avg > older_avg * 1.2
        
        return {
            "degrading": degrading,
            "recent_avg": recent_avg,
            "older_avg": older_avg,
            "degradation_rate": (recent_avg - older_avg) / older_avg * 100 if older_avg > 0 else 0
        }
    
    async def _handle_performance_degradation(self, trend: Dict, workflow_data: Dict):
        """å¤„ç†æ€§èƒ½é€€åŒ–"""
        problem = {
            "type": "performance_degradation",
            "severity": "high",
            "description": f"æ€§èƒ½é€€åŒ– {trend['degradation_rate']:.1f}%ï¼Œæœ€è¿‘å¹³å‡å“åº”æ—¶é—´ {trend['recent_avg']:.2f}ç§’",
            "trend": trend
        }
        
        # å°è¯•è‡ªåŠ¨ä¼˜åŒ–
        solution = await self._auto_optimize_performance(problem, workflow_data)
        if solution:
            await self._save_solution(problem, solution)
    
    def _identify_bottlenecks(self, step_times: Dict[str, float], total_time: float) -> List[Dict]:
        """è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ"""
        bottlenecks = []
        
        if not step_times or total_time == 0:
            return bottlenecks
        
        # æ‰¾å‡ºè€—æ—¶è¶…è¿‡æ€»æ—¶é—´30%çš„æ­¥éª¤
        threshold = total_time * 0.3
        
        for step, time in step_times.items():
            if time > threshold:
                bottlenecks.append({
                    "step": step,
                    "time": time,
                    "percentage": (time / total_time * 100) if total_time > 0 else 0,
                    "threshold": threshold
                })
        
        return bottlenecks
    
    async def _handle_bottlenecks(self, bottlenecks: List[Dict], workflow_data: Dict):
        """å¤„ç†æ€§èƒ½ç“¶é¢ˆ"""
        for bottleneck in bottlenecks:
            problem = {
                "type": "bottleneck",
                "severity": "medium",
                "description": f"æ­¥éª¤ {bottleneck['step']} è€—æ—¶è¿‡é•¿ï¼Œå æ€»æ—¶é—´ {bottleneck['percentage']:.1f}%",
                "bottleneck": bottleneck
            }
            
            # ç”Ÿæˆä¼˜åŒ–å»ºè®®
            solution = await self._generate_bottleneck_solution(bottleneck, workflow_data)
            if solution:
                await self._save_solution(problem, solution)
    
    async def _generate_bottleneck_solution(self, bottleneck: Dict, workflow_data: Dict) -> Optional[Dict]:
        """ç”Ÿæˆç“¶é¢ˆä¼˜åŒ–æ–¹æ¡ˆ"""
        step = bottleneck.get("step", "")
        
        solutions = {
            "rag_1": {
                "type": "optimization",
                "suggestions": [
                    "ä¼˜åŒ–RAGæ£€ç´¢æŸ¥è¯¢ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„å…³é”®è¯",
                    "å‡å°‘æ£€ç´¢æ•°é‡ï¼ˆtop_kï¼‰",
                    "å¯ç”¨ç¼“å­˜æœºåˆ¶",
                    "ä½¿ç”¨å¹¶è¡Œæ£€ç´¢"
                ],
                "priority": "high"
            },
            "rag_2": {
                "type": "optimization",
                "suggestions": [
                    "ä¼˜åŒ–ç¬¬2æ¬¡RAGæ£€ç´¢ç­–ç•¥",
                    "å‡å°‘æ£€ç´¢çš„æ¡ˆä¾‹æ•°é‡",
                    "ä½¿ç”¨å¼‚æ­¥å¹¶è¡Œæ£€ç´¢",
                    "å¯ç”¨ç»“æœç¼“å­˜"
                ],
                "priority": "high"
            },
            "execution": {
                "type": "optimization",
                "suggestions": [
                    "ä¼˜åŒ–æ¨¡å—æ‰§è¡Œé€»è¾‘",
                    "ä½¿ç”¨å¼‚æ­¥æ‰§è¡Œ",
                    "æ·»åŠ æ‰§è¡Œç»“æœç¼“å­˜",
                    "ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢"
                ],
                "priority": "medium"
            },
            "routing": {
                "type": "optimization",
                "suggestions": [
                    "ä¼˜åŒ–ä¸“å®¶è·¯ç”±ç®—æ³•",
                    "ä½¿ç”¨ç¼“å­˜çš„è·¯ç”±ç»“æœ",
                    "ç®€åŒ–è·¯ç”±é€»è¾‘"
                ],
                "priority": "low"
            }
        }
        
        solution = solutions.get(step)
        if solution:
            return {
                **solution,
                "bottleneck": bottleneck,
                "timestamp": datetime.now().isoformat()
            }
        
        return None
    
    async def _auto_optimize_performance(self, problem: Dict, workflow_data: Dict) -> Optional[Dict]:
        """è‡ªåŠ¨ä¼˜åŒ–æ€§èƒ½â­å¢å¼ºç‰ˆ"""
        # ç”Ÿæˆä¼˜åŒ–å»ºè®®
        optimization_suggestions = []
        
        # åŸºäºé—®é¢˜ç±»å‹ç”Ÿæˆå»ºè®®
        if problem.get("type") == "performance_degradation":
            optimization_suggestions.extend([
                "æ£€æŸ¥æœ€è¿‘ä»£ç å˜æ›´æ˜¯å¦å¼•å…¥æ€§èƒ½é—®é¢˜",
                "åˆ†æèµ„æºä½¿ç”¨æƒ…å†µï¼ˆCPU/å†…å­˜ï¼‰",
                "æ£€æŸ¥æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½",
                "ä¼˜åŒ–RAGæ£€ç´¢ç­–ç•¥",
                "å¯ç”¨ç¼“å­˜æœºåˆ¶"
            ])
        
        # è°ƒç”¨ç¼–ç¨‹åŠ©æ‰‹è¿›è¡Œä»£ç ä¼˜åŒ–
        if self.coding_assistant_url or self.coding_assistant:
            code_optimization = await self._auto_fix_problem(problem, workflow_data)
            if code_optimization:
                optimization_suggestions.append("ä»£ç ä¼˜åŒ–å»ºè®®å·²ç”Ÿæˆ")
        
        if optimization_suggestions:
            return {
                "type": "performance_optimization",
                "suggestions": optimization_suggestions,
                "problem": problem,
                "timestamp": datetime.now().isoformat()
            }
        
        return None
    
    async def _detect_problems(self, workflow_data: Dict) -> List[Dict]:
        """æ£€æµ‹é—®é¢˜"""
        problems = []
        
        # æ£€æµ‹é”™è¯¯
        if not workflow_data.get("response", {}).get("success", True):
            problems.append({
                "type": "error",
                "severity": "high",
                "description": workflow_data.get("response", {}).get("error", "æœªçŸ¥é”™è¯¯")
            })
        
        # æ£€æµ‹æ€§èƒ½é—®é¢˜
        response_time = workflow_data.get("response_time", 0)
        if response_time > 2.0:
            problems.append({
                "type": "performance",
                "severity": "medium",
                "description": f"å“åº”æ—¶é—´è¿‡é•¿: {response_time:.2f}ç§’"
            })
        
        # æ£€æµ‹RAGæ£€ç´¢è´¨é‡é—®é¢˜
        rag_1 = workflow_data.get("rag_1", {})
        rag_2 = workflow_data.get("rag_2", {})
        
        if not rag_1.get("knowledge") or len(rag_1.get("knowledge", [])) == 0:
            problems.append({
                "type": "rag_quality",
                "severity": "medium",
                "description": "ç¬¬1æ¬¡RAGæ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†"
            })
        
        if not rag_2.get("experience") or len(rag_2.get("experience", [])) == 0:
            problems.append({
                "type": "rag_quality",
                "severity": "low",
                "description": "ç¬¬2æ¬¡RAGæ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³ç»éªŒ"
            })
        
        return problems
    
    async def _handle_problems(self, problems: List[Dict], workflow_data: Dict):
        """å¤„ç†é—®é¢˜"""
        for problem in problems:
            # è®°å½•é—®é¢˜
            self.problems.append({
                **problem,
                "workflow_data": workflow_data,
                "timestamp": datetime.now().isoformat()
            })
            
            await self._publish_event(
                LearningEventType.WORKFLOW_ANOMALY,
                severity=problem.get("severity", "medium"),
                payload={"problem": problem, "workflow": workflow_data}
            )
            
            # å°è¯•è‡ªåŠ¨è§£å†³
            if problem["severity"] in ["high", "medium"]:
                solution = await self._auto_fix_problem(problem, workflow_data)
                if solution:
                    await self._save_solution(problem, solution)
    
    async def _auto_fix_problem(self, problem: Dict, workflow_data: Dict) -> Optional[Dict]:
        """è‡ªåŠ¨ä¿®å¤é—®é¢˜â­å¢å¼ºç‰ˆ"""
        problem_type = problem.get("type", "")
        
        # æ€§èƒ½é—®é¢˜ä¼˜åŒ–
        if problem_type in ["performance", "performance_degradation", "bottleneck"]:
            # è°ƒç”¨ç¼–ç¨‹åŠ©æ‰‹ä¼˜åŒ–ä»£ç 
            if self.coding_assistant_url:
                # é€šè¿‡HTTPè°ƒç”¨
                import httpx
                try:
                    async with httpx.AsyncClient(timeout=30.0) as client:
                        # æ„å»ºä¼˜åŒ–è¯·æ±‚
                        optimization_request = {
                            "problem_description": problem.get("description", ""),
                            "problem_type": problem_type,
                            "context": {
                                "workflow_data": workflow_data,
                                "problem": problem
                            },
                            "optimization_type": "performance"
                        }
                        
                        response = await client.post(
                            f"{self.coding_assistant_url}/optimize",
                            json=optimization_request
                        )
                        if response.status_code == 200:
                            optimization = response.json()
                            return {
                                "type": "code_optimization",
                                "optimization": optimization,
                                "applied": False,  # éœ€è¦äººå·¥ç¡®è®¤
                                "timestamp": datetime.now().isoformat()
                            }
                except Exception as e:
                    print(f"è°ƒç”¨ç¼–ç¨‹åŠ©æ‰‹å¤±è´¥: {e}")
                    # è¿”å›åŸºç¡€ä¼˜åŒ–å»ºè®®
                    return self._generate_basic_optimization(problem, workflow_data)
            elif self.coding_assistant:
                # ç›´æ¥è°ƒç”¨å¯¹è±¡
                try:
                    optimization = await self.coding_assistant.optimize_performance(
                        problem_description=problem.get("description", ""),
                        context=workflow_data
                    )
                    return {
                        "type": "code_optimization",
                        "optimization": optimization,
                        "applied": False,
                        "timestamp": datetime.now().isoformat()
                    }
                except Exception as e:
                    print(f"è°ƒç”¨ç¼–ç¨‹åŠ©æ‰‹å¤±è´¥: {e}")
                    return self._generate_basic_optimization(problem, workflow_data)
            else:
                # æ²¡æœ‰ç¼–ç¨‹åŠ©æ‰‹ï¼Œè¿”å›åŸºç¡€ä¼˜åŒ–å»ºè®®
                return self._generate_basic_optimization(problem, workflow_data)
        
        # RAGè´¨é‡é—®é¢˜ä¼˜åŒ–
        elif problem_type == "rag_quality":
            return await self._optimize_rag_quality(problem, workflow_data)
        
        # é”™è¯¯é—®é¢˜å¤„ç†
        elif problem_type == "error":
            return await self._handle_error_problem(problem, workflow_data)
        
        return None
    
    def _generate_basic_optimization(self, problem: Dict, workflow_data: Dict) -> Dict:
        """ç”ŸæˆåŸºç¡€ä¼˜åŒ–å»ºè®®ï¼ˆå½“ç¼–ç¨‹åŠ©æ‰‹ä¸å¯ç”¨æ—¶ï¼‰"""
        suggestions = []
        
        if problem.get("type") == "performance":
            suggestions.extend([
                "æ£€æŸ¥æ˜¯å¦æœ‰ä¸å¿…è¦çš„æ•°æ®åº“æŸ¥è¯¢",
                "å¯ç”¨ç»“æœç¼“å­˜",
                "ä½¿ç”¨å¼‚æ­¥å¤„ç†",
                "ä¼˜åŒ–ç®—æ³•å¤æ‚åº¦"
            ])
        
        return {
            "type": "basic_optimization",
            "suggestions": suggestions,
            "problem": problem,
            "timestamp": datetime.now().isoformat()
        }
    
    async def _optimize_rag_quality(self, problem: Dict, workflow_data: Dict) -> Optional[Dict]:
        """ä¼˜åŒ–RAGæ£€ç´¢è´¨é‡"""
        rag_step = "rag_1" if "ç¬¬1æ¬¡" in problem.get("description", "") else "rag_2"
        rag_data = workflow_data.get(rag_step, {})
        
        suggestions = []
        
        # å¦‚æœæ£€ç´¢ç»“æœä¸ºç©ºï¼Œå»ºè®®ä¼˜åŒ–æŸ¥è¯¢
        if not rag_data.get("knowledge") or len(rag_data.get("knowledge", [])) == 0:
            suggestions.extend([
                "ä¼˜åŒ–RAGæŸ¥è¯¢å…³é”®è¯",
                "æ‰©å¤§æ£€ç´¢èŒƒå›´ï¼ˆå¢åŠ top_kï¼‰",
                "æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦æœ‰ç›¸å…³å†…å®¹",
                "ä½¿ç”¨åŒä¹‰è¯æ‰©å±•æŸ¥è¯¢"
            ])
        
        # å¦‚æœæ£€ç´¢ç»“æœç›¸å…³æ€§ä½
        if rag_data.get("knowledge"):
            avg_score = sum(
                item.get("score", 0) 
                for item in rag_data.get("knowledge", [])
            ) / len(rag_data.get("knowledge", []))
            
            if avg_score < 0.5:
                suggestions.extend([
                    "ä¼˜åŒ–å‘é‡æ£€ç´¢æ¨¡å‹",
                    "æ”¹è¿›æŸ¥è¯¢é¢„å¤„ç†",
                    "ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰",
                    "è°ƒæ•´æ£€ç´¢å‚æ•°"
                ])
        
        if suggestions:
            return {
                "type": "rag_optimization",
                "suggestions": suggestions,
                "rag_step": rag_step,
                "problem": problem,
                "timestamp": datetime.now().isoformat()
            }
        
        return None
    
    async def _handle_error_problem(self, problem: Dict, workflow_data: Dict) -> Optional[Dict]:
        """å¤„ç†é”™è¯¯é—®é¢˜"""
        error_description = problem.get("description", "")
        
        suggestions = []
        
        # æ ¹æ®é”™è¯¯ç±»å‹ç”Ÿæˆå»ºè®®
        if "timeout" in error_description.lower():
            suggestions.extend([
                "å¢åŠ è¯·æ±‚è¶…æ—¶æ—¶é—´",
                "ä¼˜åŒ–æ…¢æŸ¥è¯¢",
                "ä½¿ç”¨å¼‚æ­¥å¤„ç†",
                "æ·»åŠ é‡è¯•æœºåˆ¶"
            ])
        elif "connection" in error_description.lower():
            suggestions.extend([
                "æ£€æŸ¥ç½‘ç»œè¿æ¥",
                "å¢åŠ è¿æ¥æ± å¤§å°",
                "æ·»åŠ è¿æ¥é‡è¯•æœºåˆ¶",
                "æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§"
            ])
        else:
            suggestions.extend([
                "æ£€æŸ¥é”™è¯¯æ—¥å¿—",
                "æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†",
                "å®ç°é”™è¯¯æ¢å¤æœºåˆ¶",
                "è®°å½•é”™è¯¯ä¸Šä¸‹æ–‡"
            ])
        
        return {
            "type": "error_handling",
            "suggestions": suggestions,
            "problem": problem,
            "timestamp": datetime.now().isoformat()
        }
    
    async def _save_solution(self, problem: Dict, solution: Dict):
        """ä¿å­˜è§£å†³æ–¹æ¡ˆåˆ°RAG"""
        if self.rag_service:
            knowledge_entry = {
                "type": "problem_solution",
                "problem": problem,
                "solution": solution,
                "timestamp": datetime.now().isoformat()
            }
            await self.rag_service.store_knowledge(knowledge_entry)
        
        self.solutions.append({
            "problem": problem,
            "solution": solution,
            "timestamp": datetime.now().isoformat()
        })
        
        await self._publish_event(
            LearningEventType.PERFORMANCE,
            severity=problem.get("severity", "info"),
            payload={"problem": problem, "solution": solution}
        )
    
    async def _record_performance_issue(self, response_time: float, workflow_data: Dict):
        """è®°å½•æ€§èƒ½é—®é¢˜"""
        issue = {
            "type": "performance",
            "response_time": response_time,
            "threshold": 2.0,
            "workflow": workflow_data,
            "timestamp": datetime.now().isoformat()
        }
        self.problems.append(issue)
        await self._publish_event(
            LearningEventType.PERFORMANCE,
            severity="medium",
            payload=issue
        )
    
    async def record_error(self, error_info: Dict):
        """è®°å½•é”™è¯¯"""
        error_entry = {
            "type": "error",
            "severity": "high",
            **error_info
        }
        self.problems.append(error_entry)
        await self._publish_event(
            LearningEventType.WORKFLOW_ANOMALY,
            severity="high",
            payload=error_entry
        )
        
        # å­˜å…¥RAG
        if self.rag_service:
            await self.rag_service.store_knowledge({
                "type": "error_log",
                **error_entry
            })
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯â­å¢å¼ºç‰ˆ"""
        stats = {
            "total_workflows": len(self.workflow_logs),
            "total_problems": len(self.problems),
            "total_solutions": len(self.solutions),
            "average_response_time": self._calculate_avg_response_time(),
            "problem_types": self._get_problem_types(),
            "solution_rate": self._calculate_solution_rate(),
            "performance_trend": self._get_performance_trend(),
            "top_bottlenecks": self._get_top_bottlenecks(),
            "status": "active" if self.workflow_logs else "idle",
            "optimization_suggestions": self._get_optimization_suggestions(),
            "last_update": datetime.now().isoformat()
        }
        stats["interaction_recommendations"] = self._generate_interaction_recommendations(stats)
        stats["resource_signals"] = self._generate_resource_signals()
        stats["alert_level"] = self._calculate_alert_level(stats)
        return stats
    
    def _get_optimization_suggestions(self) -> List[str]:
        """è·å–ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        # åŸºäºé—®é¢˜ç»Ÿè®¡ç”Ÿæˆå»ºè®®
        if len(self.problems) > 10:
            suggestions.append(f"æ£€æµ‹åˆ°{len(self.problems)}ä¸ªé—®é¢˜ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†é«˜ä¼˜å…ˆçº§é—®é¢˜")
        
        # åŸºäºå“åº”æ—¶é—´ç”Ÿæˆå»ºè®®
        avg_time = self._calculate_avg_response_time()
        if avg_time > 2.0:
            suggestions.append(f"å¹³å‡å“åº”æ—¶é—´{avg_time:.2f}ç§’ï¼Œè¶…è¿‡2ç§’ç›®æ ‡ï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½ç“¶é¢ˆ")
        
        # åŸºäºç“¶é¢ˆç”Ÿæˆå»ºè®®
        bottlenecks = self._get_top_bottlenecks(3)
        if bottlenecks:
            suggestions.append(f"å‘ç°{len(bottlenecks)}ä¸ªæ€§èƒ½ç“¶é¢ˆï¼Œå»ºè®®ä¼˜åŒ–ç›¸å…³æ¨¡å—")
        
        return suggestions[:5]  # æœ€å¤šè¿”å›5æ¡å»ºè®®
    
    def _calculate_solution_rate(self) -> float:
        """è®¡ç®—é—®é¢˜è§£å†³ç‡"""
        if not self.problems:
            return 0.0
        
        solved_problems = len([
            p for p in self.problems 
            if any(s.get("problem", {}).get("type") == p.get("type") 
                   for s in self.solutions)
        ])
        
        return (solved_problems / len(self.problems) * 100) if self.problems else 0.0

    def _generate_interaction_recommendations(self, stats: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆäº¤äº’/èµ„æºå»ºè®®"""
        recommendations: List[Dict[str, Any]] = []
        avg_response = stats.get("average_response_time") or 0.0
        problem_types = stats.get("problem_types", {})
        timestamp = datetime.now().isoformat()

        def _add(payload: Dict[str, Any]):
            rec_id = payload.get("id") or f"rec_{len(recommendations)+1}_{int(datetime.now().timestamp())}"
            payload["id"] = rec_id
            payload["timestamp"] = timestamp
            recommendations.append(payload)

        if avg_response > 2.0:
            _add({
                "title": "å“åº”æ—¶å»¶è¿‡é«˜ Â· å»ºè®®æ‰©å®¹ç®—åŠ›",
                "description": f"æœ€è¿‘å¹³å‡å“åº” {avg_response:.2f}sï¼Œå·²è¶…è¿‡2ç§’ SLOï¼Œå¯è§¦å‘ä¸€æ¬¡ LLM æ¨ç†æ‰©å®¹ã€‚",
                "severity": "high",
                "action_type": "resource_authorization",
                "payload": {
                    "description": "æ‰©å®¹LLMæ¨ç†èŠ‚ç‚¹ CPU/å†…å­˜",
                    "action_type": "scale_up",
                    "risk_level": "medium",
                    "expected_improvement": "é™ä½å“åº”æ—¶é—´",
                    "rollback_plan": "æ€§èƒ½ç¨³å®šåæ¢å¤åŸé…ç½®"
                }
            })

        rag_quality_issues = problem_types.get("rag_quality", 0)
        if rag_quality_issues:
            _add({
                "title": "RAGå‘½ä¸­ä¸è¶³ Â· å»ºè®®åˆ·æ–°ç´¢å¼•",
                "description": f"æ£€æµ‹åˆ° {rag_quality_issues} æ¬¡ RAG å‘½ä¸­ä¸è¶³ï¼Œå¯é‡æ–°æ‰§è¡Œé¢„å¤„ç†/å…¥åº“ã€‚",
                "severity": "medium",
                "action_type": "interaction",
                "payload": {
                    "instruction": "åœ¨RAGç®¡ç†é¡µæ‰§è¡Œä¸€æ¬¡æ‰¹é‡æ¸…æ´—ä¸å‘é‡é‡å»ºã€‚",
                    "module": "rag"
                }
            })

        performance_trend = stats.get("performance_trend", {})
        if performance_trend.get("trend") == "degrading":
            _add({
                "title": "æ€§èƒ½è¶‹åŠ¿ä¸‹æ»‘ Â· å»ºè®®æ‰§è¡Œå­¦ä¹ å›æ”¾",
                "description": f"æ€§èƒ½é€€åŒ–ç‡ {performance_trend.get('degradation_rate', 0):.1f}%ï¼Œå¯è°ƒåº¦å­¦ä¹ å›æ”¾è„šæœ¬ã€‚",
                "severity": "medium",
                "action_type": "interaction",
                "payload": {
                    "instruction": "è§¦å‘è‡ªå­¦ä¹ ä¼˜åŒ–è„šæœ¬ï¼Œå›æ”¾æœ€è¿‘ä»»åŠ¡ã€‚",
                    "module": "self_learning"
                }
            })

        self.latest_recommendations = {rec["id"]: rec for rec in recommendations}
        return recommendations

    def _generate_resource_signals(self) -> List[Dict[str, Any]]:
        """è¾“å‡ºèµ„æºä¿¡å·"""
        signals: List[Dict[str, Any]] = []
        if not self.resource_manager:
            self.latest_resource_signals = []
            return signals
        try:
            snapshot = self.resource_manager.get_current_status()
        except Exception:
            self.latest_resource_signals = []
            return []

        def _push(name: str, value: Optional[float], threshold: float, suggestion: str):
            if value is None:
                return
            severity = "high" if value >= threshold + 10 else "medium"
            signals.append({
                "resource": name,
                "value": round(value, 1),
                "threshold": threshold,
                "severity": severity,
                "suggestion": suggestion
            })

        cpu_percent = snapshot.get("cpu", {}).get("percent")
        memory_percent = snapshot.get("memory", {}).get("percent")
        disk_percent = snapshot.get("disk", {}).get("percent")
        _push("CPU", cpu_percent, 75, "è¯„ä¼°æ¨ç†è¯·æ±‚å¹¶è€ƒè™‘æ‰©å®¹/é™æµ")
        _push("å†…å­˜", memory_percent, 80, "æ¸…ç†ç¼“å­˜æˆ–æ‰©å®¹å†…å­˜")
        _push("ç£ç›˜", disk_percent, 85, "é‡Šæ”¾ç©ºé—´æˆ–æ‰©å±•ç£ç›˜å®¹é‡")
        self.latest_resource_signals = signals
        return signals

    def _calculate_alert_level(self, stats: Dict[str, Any]) -> str:
        score = 0
        if stats.get("total_problems", 0) > 5:
            score += 2
        elif stats.get("total_problems", 0) > 0:
            score += 1
        avg_response = stats.get("average_response_time", 0.0)
        if avg_response > 2.0:
            score += 2
        elif avg_response > 1.5:
            score += 1
        if len(self.latest_resource_signals) >= 2:
            score += 2
        elif self.latest_resource_signals:
            score += 1
        if score >= 4:
            return "high"
        if score >= 2:
            return "medium"
        return "low"

    def get_recommendation(self, rec_id: str) -> Optional[Dict[str, Any]]:
        return self.latest_recommendations.get(rec_id)

    def mark_recommendation_applied(self, rec_id: str) -> Optional[Dict[str, Any]]:
        rec = self.latest_recommendations.get(rec_id)
        if rec:
            rec["applied_at"] = datetime.now().isoformat()
        return rec
    
    def _get_performance_trend(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½è¶‹åŠ¿"""
        if len(self.workflow_logs) < 5:
            return {"trend": "insufficient_data"}
        
        recent_logs = self.workflow_logs[-10:]
        response_times = [
            log.get("response_time", 0) 
            for log in recent_logs 
            if "response_time" in log
        ]
        
        if not response_times:
            return {"trend": "no_data"}
        
        # è®¡ç®—è¶‹åŠ¿ï¼ˆç®€å•çº¿æ€§å›å½’ï¼‰
        n = len(response_times)
        x = list(range(n))
        y = response_times
        
        avg_x = sum(x) / n
        avg_y = sum(y) / n
        
        numerator = sum((x[i] - avg_x) * (y[i] - avg_y) for i in range(n))
        denominator = sum((x[i] - avg_x) ** 2 for i in range(n))
        
        if denominator == 0:
            slope = 0
        else:
            slope = numerator / denominator
        
        if slope > 0.01:
            trend = "degrading"
        elif slope < -0.01:
            trend = "improving"
        else:
            trend = "stable"
        
        return {
            "trend": trend,
            "slope": slope,
            "recent_avg": avg_y,
            "min": min(response_times),
            "max": max(response_times)
        }
    
    def _get_top_bottlenecks(self, top_n: int = 3) -> List[Dict]:
        """è·å–ä¸»è¦æ€§èƒ½ç“¶é¢ˆ"""
        bottleneck_problems = [
            p for p in self.problems 
            if p.get("type") == "bottleneck"
        ]
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
        bottleneck_problems.sort(
            key=lambda x: x.get("bottleneck", {}).get("percentage", 0),
            reverse=True
        )
        
        return bottleneck_problems[:top_n]
    
    def _calculate_avg_response_time(self) -> float:
        """è®¡ç®—å¹³å‡å“åº”æ—¶é—´"""
        if not self.workflow_logs:
            return 0.0
        
        total_time = sum(
            log.get("response_time", 0) 
            for log in self.workflow_logs 
            if "response_time" in log
        )
        return total_time / len(self.workflow_logs)
    
    def _get_problem_types(self) -> Dict[str, int]:
        """è·å–é—®é¢˜ç±»å‹ç»Ÿè®¡"""
        types = {}
        for problem in self.problems:
            ptype = problem.get("type", "unknown")
            types[ptype] = types.get(ptype, 0) + 1
        return types


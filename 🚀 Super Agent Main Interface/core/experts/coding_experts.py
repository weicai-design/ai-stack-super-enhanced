#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
TRAEç¼–ç¨‹åŠ©æ‰‹æ¨¡å—ä¸“å®¶ç³»ç»Ÿï¼ˆT010ï¼‰
å®ç°5ä¸ªä¸“å®¶ï¼šTRAEä»£ç ç”Ÿæˆä¸“å®¶ã€TRAEä»£ç å®¡æŸ¥ä¸“å®¶ã€TRAEæ€§èƒ½ä¼˜åŒ–ä¸“å®¶ã€TRAE Bugä¿®å¤ä¸“å®¶ã€TRAEæ–‡æ¡£ç”Ÿæˆä¸“å®¶
"""

from __future__ import annotations

import logging
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from enum import Enum

logger = logging.getLogger(__name__)


class CodingStage(str, Enum):
    """ç¼–ç¨‹é˜¶æ®µ"""
    GENERATION = "generation"  # ä»£ç ç”Ÿæˆ
    REVIEW = "review"  # ä»£ç å®¡æŸ¥
    OPTIMIZATION = "optimization"  # æ€§èƒ½ä¼˜åŒ–
    BUG_FIX = "bug_fix"  # Bugä¿®å¤
    DOCUMENTATION = "documentation"  # æ–‡æ¡£ç”Ÿæˆ


@dataclass
class CodingAnalysis:
    """ç¼–ç¨‹åˆ†æç»“æœ"""
    stage: CodingStage
    confidence: float
    score: float  # 0-100åˆ†
    insights: List[str] = field(default_factory=list)
    recommendations: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)


class CodeGenerationExpert:
    """
    ä»£ç ç”Ÿæˆä¸“å®¶ï¼ˆT010-1ï¼‰
    
    ä¸“ä¸šèƒ½åŠ›ï¼š
    1. å¤šè¯­è¨€ä»£ç æ™ºèƒ½ç”Ÿæˆï¼ˆæ”¯æŒ30+ç¼–ç¨‹è¯­è¨€ï¼‰
    2. ä»£ç è´¨é‡æ·±åº¦è¯„ä¼°ä¸ä¼˜åŒ–
    3. ä»£ç ç»“æ„æ™ºèƒ½é‡æ„
    4. æœ€ä½³å®è·µè‡ªåŠ¨åº”ç”¨
    5. ä»£ç ç”Ÿæˆæ•ˆç‡ä¼˜åŒ–
    6. æ™ºèƒ½ä»£ç è¡¥å…¨ä¸å»ºè®®
    """
    
    def __init__(self):
        self.expert_id = "code_generation_expert"
        self.name = "ä»£ç ç”Ÿæˆä¸“å®¶"
        self.stage = CodingStage.GENERATION
        self.data_sources = ["GitHubä»£ç åº“", "Stack Overflow", "å®˜æ–¹æ–‡æ¡£", "å¼€æºé¡¹ç›®", "AIè®­ç»ƒæ•°æ®"]
        self.analysis_dimensions = ["è¯­è¨€æ”¯æŒ", "ä»£ç è´¨é‡", "ç»“æ„ä¼˜åŒ–", "æ€§èƒ½æ•ˆç‡", "å®‰å…¨æ€§", "å¯ç»´æŠ¤æ€§"]
        self.supported_languages = [
            "python", "javascript", "typescript", "java", "cpp", "c", "csharp",
            "go", "rust", "php", "ruby", "swift", "kotlin", "scala", "r",
            "matlab", "sql", "html", "css", "shell", "bash", "powershell",
            "lua", "perl", "dart", "objective-c", "groovy", "haskell", "elixir"
        ]
        
    async def analyze_generation(
        self,
        code_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> CodingAnalysis:
        """åˆ†æä»£ç ç”Ÿæˆè´¨é‡ - å¤šç»´åº¦ç”Ÿäº§çº§åˆ†æ"""
        insights = []
        recommendations = []
        metadata = {}
        
        # å¤šç»´åº¦åˆ†æ
        dimension_scores = {}
        
        # 1. è¯­è¨€æ”¯æŒç»´åº¦åˆ†æ
        language_score = await self._analyze_language_support(code_data, insights, metadata)
        dimension_scores["è¯­è¨€æ”¯æŒ"] = language_score
        
        # 2. ä»£ç è´¨é‡ç»´åº¦åˆ†æ
        quality_score = await self._analyze_code_quality(code_data, insights, metadata, recommendations)
        dimension_scores["ä»£ç è´¨é‡"] = quality_score
        
        # 3. ç»“æ„ä¼˜åŒ–ç»´åº¦åˆ†æ
        structure_score = await self._analyze_code_structure(code_data, insights, metadata, recommendations)
        dimension_scores["ç»“æ„ä¼˜åŒ–"] = structure_score
        
        # 4. æ€§èƒ½æ•ˆç‡ç»´åº¦åˆ†æ
        performance_score = await self._analyze_performance_efficiency(code_data, insights, metadata, recommendations)
        dimension_scores["æ€§èƒ½æ•ˆç‡"] = performance_score
        
        # 5. å®‰å…¨æ€§ç»´åº¦åˆ†æ
        security_score = await self._analyze_security(code_data, insights, metadata, recommendations)
        dimension_scores["å®‰å…¨æ€§"] = security_score
        
        # 6. å¯ç»´æŠ¤æ€§ç»´åº¦åˆ†æ
        maintainability_score = await self._analyze_maintainability(code_data, insights, metadata, recommendations)
        dimension_scores["å¯ç»´æŠ¤æ€§"] = maintainability_score
        
        # ç”Ÿäº§çº§åŠ æƒè¯„åˆ†ç³»ç»Ÿ
        weights = {
            "è¯­è¨€æ”¯æŒ": 0.15,
            "ä»£ç è´¨é‡": 0.25,
            "ç»“æ„ä¼˜åŒ–": 0.20,
            "æ€§èƒ½æ•ˆç‡": 0.15,
            "å®‰å…¨æ€§": 0.10,
            "å¯ç»´æŠ¤æ€§": 0.15
        }
        
        weighted_score = sum(dimension_scores[dim] * weights[dim] for dim in dimension_scores)
        final_score = max(0, min(100, weighted_score))
        
        # æ™ºèƒ½ç½®ä¿¡åº¦è®¡ç®—
        confidence = self._calculate_confidence(dimension_scores, code_data)
        
        metadata["dimension_scores"] = dimension_scores
        metadata["weights"] = weights
        
        return CodingAnalysis(
            stage=self.stage,
            confidence=confidence,
            score=final_score,
            insights=insights,
            recommendations=recommendations,
            metadata=metadata
        )
    
    async def _analyze_language_support(self, code_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any]) -> float:
        """åˆ†æè¯­è¨€æ”¯æŒç»´åº¦"""
        target_language = code_data.get("language", "").lower()
        if target_language in self.supported_languages:
            insights.append(f"âœ… è¯­è¨€æ”¯æŒ: {target_language} (å®Œå…¨æ”¯æŒ)")
            metadata["language_supported"] = True
            return 95.0
        else:
            insights.append(f"âš ï¸ è¯­è¨€æ”¯æŒ: {target_language} (éƒ¨åˆ†æ”¯æŒ)")
            metadata["language_supported"] = False
            return 60.0
    
    async def _analyze_code_quality(self, code_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æä»£ç è´¨é‡ç»´åº¦"""
        code_quality = code_data.get("quality", 0)
        complexity = code_data.get("complexity", 0)
        
        quality_score = code_quality * 100
        
        if code_quality >= 0.9:
            insights.append(f"âœ… ä»£ç è´¨é‡: ä¼˜ç§€ ({code_quality:.2f})")
        elif code_quality >= 0.7:
            insights.append(f"ğŸŸ¡ ä»£ç è´¨é‡: è‰¯å¥½ ({code_quality:.2f})")
        else:
            insights.append(f"ğŸ”´ ä»£ç è´¨é‡: éœ€è¦æ”¹è¿› ({code_quality:.2f})")
            recommendations.append("å»ºè®®è¿›è¡Œä»£ç è´¨é‡é‡æ„")
        
        metadata["quality"] = code_quality
        metadata["complexity"] = complexity
        
        return quality_score
    
    async def _analyze_code_structure(self, code_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æç»“æ„ä¼˜åŒ–ç»´åº¦"""
        structure_score = code_data.get("structure_quality", 0.7) * 100
        
        if structure_score >= 85:
            insights.append(f"âœ… ä»£ç ç»“æ„: ä¼˜ç§€ ({structure_score:.1f}åˆ†)")
        elif structure_score >= 70:
            insights.append(f"ğŸŸ¡ ä»£ç ç»“æ„: è‰¯å¥½ ({structure_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ ä»£ç ç»“æ„: éœ€è¦ä¼˜åŒ– ({structure_score:.1f}åˆ†)")
            recommendations.append("å»ºè®®é‡æ„ä»£ç ç»“æ„ä»¥æé«˜å¯è¯»æ€§")
        
        return structure_score
    
    async def _analyze_performance_efficiency(self, code_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†ææ€§èƒ½æ•ˆç‡ç»´åº¦"""
        performance_score = code_data.get("performance_score", 0.8) * 100
        
        if performance_score >= 90:
            insights.append(f"âœ… æ€§èƒ½æ•ˆç‡: ä¼˜ç§€ ({performance_score:.1f}åˆ†)")
        elif performance_score >= 75:
            insights.append(f"ğŸŸ¡ æ€§èƒ½æ•ˆç‡: è‰¯å¥½ ({performance_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ æ€§èƒ½æ•ˆç‡: éœ€è¦ä¼˜åŒ– ({performance_score:.1f}åˆ†)")
            recommendations.append("å»ºè®®è¿›è¡Œæ€§èƒ½ä¼˜åŒ–")
        
        return performance_score
    
    async def _analyze_security(self, code_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æå®‰å…¨æ€§ç»´åº¦"""
        security_score = code_data.get("security_score", 0.85) * 100
        
        if security_score >= 95:
            insights.append(f"âœ… å®‰å…¨æ€§: ä¼˜ç§€ ({security_score:.1f}åˆ†)")
        elif security_score >= 80:
            insights.append(f"ğŸŸ¡ å®‰å…¨æ€§: è‰¯å¥½ ({security_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ å®‰å…¨æ€§: éœ€è¦åŠ å¼º ({security_score:.1f}åˆ†)")
            recommendations.append("å»ºè®®è¿›è¡Œå®‰å…¨ä»£ç å®¡æŸ¥")
        
        return security_score
    
    async def _analyze_maintainability(self, code_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æå¯ç»´æŠ¤æ€§ç»´åº¦"""
        maintainability_score = code_data.get("maintainability_score", 0.75) * 100
        
        if maintainability_score >= 85:
            insights.append(f"âœ… å¯ç»´æŠ¤æ€§: ä¼˜ç§€ ({maintainability_score:.1f}åˆ†)")
        elif maintainability_score >= 70:
            insights.append(f"ğŸŸ¡ å¯ç»´æŠ¤æ€§: è‰¯å¥½ ({maintainability_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ å¯ç»´æŠ¤æ€§: éœ€è¦æ”¹è¿› ({maintainability_score:.1f}åˆ†)")
            recommendations.append("å»ºè®®æé«˜ä»£ç å¯ç»´æŠ¤æ€§")
        
        return maintainability_score
    
    def _calculate_confidence(self, dimension_scores: Dict[str, float], code_data: Dict[str, Any]) -> float:
        """æ™ºèƒ½ç½®ä¿¡åº¦è®¡ç®—"""
        base_confidence = 0.85
        
        # æ•°æ®è´¨é‡å½±å“
        data_quality = code_data.get("data_quality", 0.8)
        confidence_modifier = data_quality * 0.1
        
        # ç»´åº¦åˆ†æ•°ç¨³å®šæ€§å½±å“
        score_variance = sum((score - 75) ** 2 for score in dimension_scores.values()) / len(dimension_scores)
        variance_modifier = max(0, 1 - (score_variance / 1000)) * 0.05
        
        final_confidence = base_confidence + confidence_modifier + variance_modifier
        return min(0.95, max(0.7, final_confidence))


class CodeReviewExpert:
    """
    ä»£ç å®¡æŸ¥ä¸“å®¶ï¼ˆT010-2ï¼‰
    
    ä¸“ä¸šèƒ½åŠ›ï¼š
    1. æ™ºèƒ½ä»£ç é—®é¢˜æ£€æµ‹ä¸åˆ†ç±»
    2. å¤šç»´åº¦ä»£ç è§„èŒƒæ·±åº¦æ£€æŸ¥
    3. å®‰å…¨æ¼æ´æ™ºèƒ½è¯†åˆ«ä¸é£é™©è¯„ä¼°
    4. æ€§èƒ½ç“¶é¢ˆç²¾å‡†å‘ç°ä¸ä¼˜åŒ–å»ºè®®
    5. ä»£ç å®¡æŸ¥æ•ˆç‡ä¼˜åŒ–ä¸è‡ªåŠ¨åŒ–
    6. å®¡æŸ¥ç»“æœæ™ºèƒ½åˆ†çº§ä¸ä¼˜å…ˆçº§æ’åº
    """
    
    def __init__(self):
        self.expert_id = "code_review_expert"
        self.name = "ä»£ç å®¡æŸ¥ä¸“å®¶"
        self.stage = CodingStage.REVIEW
        self.data_sources = ["ä»£ç å®¡æŸ¥å·¥å…·", "å®‰å…¨æ‰«æå™¨", "æ€§èƒ½åˆ†æå™¨", "ç¼–ç è§„èŒƒ", "æœ€ä½³å®è·µåº“"]
        self.analysis_dimensions = ["é—®é¢˜æ£€æµ‹", "è§„èŒƒæ£€æŸ¥", "å®‰å…¨å®¡æŸ¥", "æ€§èƒ½åˆ†æ", "ä»£ç è´¨é‡", "å®¡æŸ¥æ•ˆç‡"]
        
    async def analyze_review(
        self,
        review_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> CodingAnalysis:
        """åˆ†æä»£ç å®¡æŸ¥ç»“æœ - å¤šç»´åº¦ç”Ÿäº§çº§åˆ†æ"""
        insights = []
        recommendations = []
        metadata = {}
        
        # å¤šç»´åº¦åˆ†æ
        dimension_scores = {}
        
        # 1. é—®é¢˜æ£€æµ‹ç»´åº¦åˆ†æ
        problem_score = await self._analyze_problem_detection(review_data, insights, metadata, recommendations)
        dimension_scores["é—®é¢˜æ£€æµ‹"] = problem_score
        
        # 2. è§„èŒƒæ£€æŸ¥ç»´åº¦åˆ†æ
        standard_score = await self._analyze_standard_check(review_data, insights, metadata, recommendations)
        dimension_scores["è§„èŒƒæ£€æŸ¥"] = standard_score
        
        # 3. å®‰å…¨å®¡æŸ¥ç»´åº¦åˆ†æ
        security_score = await self._analyze_security_review(review_data, insights, metadata, recommendations)
        dimension_scores["å®‰å…¨å®¡æŸ¥"] = security_score
        
        # 4. æ€§èƒ½åˆ†æç»´åº¦åˆ†æ
        performance_score = await self._analyze_performance_analysis(review_data, insights, metadata, recommendations)
        dimension_scores["æ€§èƒ½åˆ†æ"] = performance_score
        
        # 5. ä»£ç è´¨é‡ç»´åº¦åˆ†æ
        quality_score = await self._analyze_code_quality(review_data, insights, metadata, recommendations)
        dimension_scores["ä»£ç è´¨é‡"] = quality_score
        
        # 6. å®¡æŸ¥æ•ˆç‡ç»´åº¦åˆ†æ
        efficiency_score = await self._analyze_review_efficiency(review_data, insights, metadata, recommendations)
        dimension_scores["å®¡æŸ¥æ•ˆç‡"] = efficiency_score
        
        # ç”Ÿäº§çº§åŠ æƒè¯„åˆ†ç³»ç»Ÿ
        weights = {
            "é—®é¢˜æ£€æµ‹": 0.25,
            "è§„èŒƒæ£€æŸ¥": 0.20,
            "å®‰å…¨å®¡æŸ¥": 0.15,
            "æ€§èƒ½åˆ†æ": 0.15,
            "ä»£ç è´¨é‡": 0.15,
            "å®¡æŸ¥æ•ˆç‡": 0.10
        }
        
        weighted_score = sum(dimension_scores[dim] * weights[dim] for dim in dimension_scores)
        final_score = max(0, min(100, weighted_score))
        
        # æ™ºèƒ½ç½®ä¿¡åº¦è®¡ç®—
        confidence = self._calculate_confidence(dimension_scores, review_data)
        
        metadata["dimension_scores"] = dimension_scores
        metadata["weights"] = weights
        
        return CodingAnalysis(
            stage=self.stage,
            confidence=confidence,
            score=final_score,
            insights=insights,
            recommendations=recommendations,
            metadata=metadata
        )
    
    async def _analyze_problem_detection(self, review_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æé—®é¢˜æ£€æµ‹ç»´åº¦"""
        issues = review_data.get("issues", [])
        issue_count = len(issues)
        
        critical_issues = [i for i in issues if i.get("severity") == "critical"]
        major_issues = [i for i in issues if i.get("severity") == "major"]
        minor_issues = [i for i in issues if i.get("severity") == "minor"]
        
        # é—®é¢˜æ£€æµ‹è¯„åˆ†
        base_score = 90
        if critical_issues:
            base_score -= len(critical_issues) * 10
        if major_issues:
            base_score -= len(major_issues) * 3
        if minor_issues:
            base_score -= len(minor_issues) * 1
        
        if issue_count == 0:
            insights.append("âœ… é—®é¢˜æ£€æµ‹: æœªå‘ç°ä»»ä½•é—®é¢˜")
        else:
            insights.append(f"ğŸ” é—®é¢˜æ£€æµ‹: å‘ç°{issue_count}ä¸ªé—®é¢˜ (ä¸¥é‡:{len(critical_issues)}, ä¸»è¦:{len(major_issues)}, æ¬¡è¦:{len(minor_issues)})")
            if critical_issues:
                recommendations.append("å‘ç°ä¸¥é‡é—®é¢˜ï¼Œå»ºè®®ç«‹å³ä¿®å¤")
        
        metadata["issue_count"] = issue_count
        metadata["critical_issues"] = len(critical_issues)
        metadata["major_issues"] = len(major_issues)
        metadata["minor_issues"] = len(minor_issues)
        
        return max(0, base_score)
    
    async def _analyze_standard_check(self, review_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æè§„èŒƒæ£€æŸ¥ç»´åº¦"""
        standards = review_data.get("standards", [])
        violations = review_data.get("violations", [])
        
        standard_count = len(standards)
        violation_count = len(violations)
        
        # è§„èŒƒæ£€æŸ¥è¯„åˆ†
        compliance_rate = 1 - (violation_count / max(standard_count, 1))
        standard_score = compliance_rate * 100
        
        if compliance_rate >= 0.95:
            insights.append(f"âœ… è§„èŒƒæ£€æŸ¥: ä¼˜ç§€ ({compliance_rate:.2%} åˆè§„ç‡)")
        elif compliance_rate >= 0.85:
            insights.append(f"ğŸŸ¡ è§„èŒƒæ£€æŸ¥: è‰¯å¥½ ({compliance_rate:.2%} åˆè§„ç‡)")
        else:
            insights.append(f"ğŸ”´ è§„èŒƒæ£€æŸ¥: éœ€è¦æ”¹è¿› ({compliance_rate:.2%} åˆè§„ç‡)")
            recommendations.append("å»ºè®®åŠ å¼ºç¼–ç è§„èŒƒéµå¾ª")
        
        metadata["standard_count"] = standard_count
        metadata["violation_count"] = violation_count
        metadata["compliance_rate"] = compliance_rate
        
        return standard_score
    
    async def _analyze_security_review(self, review_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æå®‰å…¨å®¡æŸ¥ç»´åº¦"""
        security_issues = review_data.get("security_issues", [])
        security_score = review_data.get("security_score", 0.85) * 100
        
        if len(security_issues) == 0:
            insights.append(f"âœ… å®‰å…¨å®¡æŸ¥: ä¼˜ç§€ ({security_score:.1f}åˆ†)")
        elif len(security_issues) <= 2:
            insights.append(f"ğŸŸ¡ å®‰å…¨å®¡æŸ¥: è‰¯å¥½ ({security_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ å®‰å…¨å®¡æŸ¥: éœ€è¦åŠ å¼º ({security_score:.1f}åˆ†)")
            recommendations.append("å»ºè®®è¿›è¡Œå®‰å…¨ä»£ç å®¡æŸ¥")
        
        metadata["security_issues"] = len(security_issues)
        metadata["security_score"] = security_score
        
        return security_score
    
    async def _analyze_performance_analysis(self, review_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†ææ€§èƒ½åˆ†æç»´åº¦"""
        performance_issues = review_data.get("performance_issues", [])
        performance_score = review_data.get("performance_score", 0.8) * 100
        
        if len(performance_issues) == 0:
            insights.append(f"âœ… æ€§èƒ½åˆ†æ: ä¼˜ç§€ ({performance_score:.1f}åˆ†)")
        elif len(performance_issues) <= 3:
            insights.append(f"ğŸŸ¡ æ€§èƒ½åˆ†æ: è‰¯å¥½ ({performance_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ æ€§èƒ½åˆ†æ: éœ€è¦ä¼˜åŒ– ({performance_score:.1f}åˆ†)")
            recommendations.append("å»ºè®®è¿›è¡Œæ€§èƒ½ä¼˜åŒ–")
        
        metadata["performance_issues"] = len(performance_issues)
        metadata["performance_score"] = performance_score
        
        return performance_score
    
    async def _analyze_code_quality(self, review_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æä»£ç è´¨é‡ç»´åº¦"""
        quality_score = review_data.get("quality_score", 0.75) * 100
        complexity = review_data.get("complexity", 0)
        
        if quality_score >= 85:
            insights.append(f"âœ… ä»£ç è´¨é‡: ä¼˜ç§€ ({quality_score:.1f}åˆ†)")
        elif quality_score >= 70:
            insights.append(f"ğŸŸ¡ ä»£ç è´¨é‡: è‰¯å¥½ ({quality_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ ä»£ç è´¨é‡: éœ€è¦æ”¹è¿› ({quality_score:.1f}åˆ†)")
            recommendations.append("å»ºè®®è¿›è¡Œä»£ç è´¨é‡é‡æ„")
        
        metadata["quality_score"] = quality_score
        metadata["complexity"] = complexity
        
        return quality_score
    
    async def _analyze_review_efficiency(self, review_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æå®¡æŸ¥æ•ˆç‡ç»´åº¦"""
        efficiency_score = review_data.get("efficiency_score", 0.8) * 100
        review_time = review_data.get("review_time", 0)
        
        if efficiency_score >= 90:
            insights.append(f"âœ… å®¡æŸ¥æ•ˆç‡: ä¼˜ç§€ ({efficiency_score:.1f}åˆ†)")
        elif efficiency_score >= 75:
            insights.append(f"ğŸŸ¡ å®¡æŸ¥æ•ˆç‡: è‰¯å¥½ ({efficiency_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ å®¡æŸ¥æ•ˆç‡: éœ€è¦æå‡ ({efficiency_score:.1f}åˆ†)")
            recommendations.append("å»ºè®®ä¼˜åŒ–å®¡æŸ¥æµç¨‹")
        
        metadata["efficiency_score"] = efficiency_score
        metadata["review_time"] = review_time
        
        return efficiency_score
    
    def _calculate_confidence(self, dimension_scores: Dict[str, float], review_data: Dict[str, Any]) -> float:
        """æ™ºèƒ½ç½®ä¿¡åº¦è®¡ç®—"""
        base_confidence = 0.88
        
        # æ•°æ®å®Œæ•´æ€§å½±å“
        data_completeness = review_data.get("data_completeness", 0.8)
        completeness_modifier = data_completeness * 0.08
        
        # é—®é¢˜æ£€æµ‹å‡†ç¡®æ€§å½±å“
        detection_accuracy = review_data.get("detection_accuracy", 0.85)
        accuracy_modifier = detection_accuracy * 0.07
        
        final_confidence = base_confidence + completeness_modifier + accuracy_modifier
        return min(0.95, max(0.75, final_confidence))


class PerformanceOptimizationExpert:
    """
    æ€§èƒ½ä¼˜åŒ–ä¸“å®¶ï¼ˆT010-3ï¼‰
    
    ä¸“ä¸šèƒ½åŠ›ï¼š
    1. æ€§èƒ½ç“¶é¢ˆæ·±åº¦åˆ†æä¸å®šä½
    2. å†…å­˜ä½¿ç”¨æ™ºèƒ½ä¼˜åŒ–ä¸åƒåœ¾å›æ”¶
    3. å“åº”æ—¶é—´ç²¾å‡†ä¼˜åŒ–ä¸è´Ÿè½½å‡è¡¡
    4. èµ„æºåˆ©ç”¨ç‡æ™ºèƒ½ç›‘æ§ä¸è°ƒä¼˜
    5. å¹¶å‘æ€§èƒ½ä¼˜åŒ–ä¸çº¿ç¨‹ç®¡ç†
    6. æ€§èƒ½ç›‘æ§ä¸é¢„è­¦ç³»ç»Ÿ
    """
    
    def __init__(self):
        self.expert_id = "performance_optimization_expert"
        self.name = "æ€§èƒ½ä¼˜åŒ–ä¸“å®¶"
        self.stage = CodingStage.OPTIMIZATION
        self.data_sources = ["æ€§èƒ½ç›‘æ§æ•°æ®", "ç³»ç»Ÿèµ„æºæŒ‡æ ‡", "åº”ç”¨æ—¥å¿—", "æ€§èƒ½æµ‹è¯•ç»“æœ", "åŸºå‡†æµ‹è¯•æ•°æ®"]
        self.analysis_dimensions = ["å“åº”æ—¶é—´", "å†…å­˜ä½¿ç”¨", "CPUåˆ©ç”¨ç‡", "I/Oæ€§èƒ½", "å¹¶å‘æ€§èƒ½", "èµ„æºæ•ˆç‡"]
        
    async def analyze_performance(
        self,
        perf_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> CodingAnalysis:
        """åˆ†ææ€§èƒ½æ•°æ® - å¤šç»´åº¦ç”Ÿäº§çº§åˆ†æ"""
        insights = []
        recommendations = []
        metadata = {}
        
        # å¤šç»´åº¦åˆ†æ
        dimension_scores = {}
        
        # 1. å“åº”æ—¶é—´ç»´åº¦åˆ†æ
        response_score = await self._analyze_response_time(perf_data, insights, metadata, recommendations)
        dimension_scores["å“åº”æ—¶é—´"] = response_score
        
        # 2. å†…å­˜ä½¿ç”¨ç»´åº¦åˆ†æ
        memory_score = await self._analyze_memory_usage(perf_data, insights, metadata, recommendations)
        dimension_scores["å†…å­˜ä½¿ç”¨"] = memory_score
        
        # 3. CPUåˆ©ç”¨ç‡ç»´åº¦åˆ†æ
        cpu_score = await self._analyze_cpu_utilization(perf_data, insights, metadata, recommendations)
        dimension_scores["CPUåˆ©ç”¨ç‡"] = cpu_score
        
        # 4. I/Oæ€§èƒ½ç»´åº¦åˆ†æ
        io_score = await self._analyze_io_performance(perf_data, insights, metadata, recommendations)
        dimension_scores["I/Oæ€§èƒ½"] = io_score
        
        # 5. å¹¶å‘æ€§èƒ½ç»´åº¦åˆ†æ
        concurrency_score = await self._analyze_concurrency_performance(perf_data, insights, metadata, recommendations)
        dimension_scores["å¹¶å‘æ€§èƒ½"] = concurrency_score
        
        # 6. èµ„æºæ•ˆç‡ç»´åº¦åˆ†æ
        efficiency_score = await self._analyze_resource_efficiency(perf_data, insights, metadata, recommendations)
        dimension_scores["èµ„æºæ•ˆç‡"] = efficiency_score
        
        # ç”Ÿäº§çº§åŠ æƒè¯„åˆ†ç³»ç»Ÿ
        weights = {
            "å“åº”æ—¶é—´": 0.25,
            "å†…å­˜ä½¿ç”¨": 0.20,
            "CPUåˆ©ç”¨ç‡": 0.15,
            "I/Oæ€§èƒ½": 0.15,
            "å¹¶å‘æ€§èƒ½": 0.10,
            "èµ„æºæ•ˆç‡": 0.15
        }
        
        weighted_score = sum(dimension_scores[dim] * weights[dim] for dim in dimension_scores)
        final_score = max(0, min(100, weighted_score))
        
        # æ™ºèƒ½ç½®ä¿¡åº¦è®¡ç®—
        confidence = self._calculate_confidence(dimension_scores, perf_data)
        
        metadata["dimension_scores"] = dimension_scores
        metadata["weights"] = weights
        
        return CodingAnalysis(
            stage=self.stage,
            confidence=confidence,
            score=final_score,
            insights=insights,
            recommendations=recommendations,
            metadata=metadata
        )
    
    async def _analyze_response_time(self, perf_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æå“åº”æ—¶é—´ç»´åº¦"""
        response_time = perf_data.get("response_time", 0)
        
        # å“åº”æ—¶é—´è¯„åˆ†
        if response_time <= 100:
            score = 95
            insights.append(f"âœ… å“åº”æ—¶é—´: ä¼˜ç§€ ({response_time}ms)")
        elif response_time <= 300:
            score = 85
            insights.append(f"ğŸŸ¡ å“åº”æ—¶é—´: è‰¯å¥½ ({response_time}ms)")
        elif response_time <= 1000:
            score = 70
            insights.append(f"ğŸŸ  å“åº”æ—¶é—´: ä¸€èˆ¬ ({response_time}ms)")
            recommendations.append("å»ºè®®ä¼˜åŒ–å“åº”æ—¶é—´")
        else:
            score = 50
            insights.append(f"ğŸ”´ å“åº”æ—¶é—´: è¾ƒå·® ({response_time}ms)")
            recommendations.append("å“åº”æ—¶é—´è¿‡é•¿ï¼Œéœ€è¦ç«‹å³ä¼˜åŒ–")
        
        metadata["response_time"] = response_time
        return score
    
    async def _analyze_memory_usage(self, perf_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æå†…å­˜ä½¿ç”¨ç»´åº¦"""
        memory_usage = perf_data.get("memory_usage", 0)
        
        # å†…å­˜ä½¿ç”¨è¯„åˆ†
        if memory_usage <= 100:
            score = 90
            insights.append(f"âœ… å†…å­˜ä½¿ç”¨: ä¼˜ç§€ ({memory_usage}MB)")
        elif memory_usage <= 300:
            score = 80
            insights.append(f"ğŸŸ¡ å†…å­˜ä½¿ç”¨: è‰¯å¥½ ({memory_usage}MB)")
        elif memory_usage <= 500:
            score = 65
            insights.append(f"ğŸŸ  å†…å­˜ä½¿ç”¨: ä¸€èˆ¬ ({memory_usage}MB)")
            recommendations.append("å»ºè®®ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
        else:
            score = 45
            insights.append(f"ğŸ”´ å†…å­˜ä½¿ç”¨: è¾ƒå·® ({memory_usage}MB)")
            recommendations.append("å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Œéœ€è¦ç«‹å³ä¼˜åŒ–")
        
        metadata["memory_usage"] = memory_usage
        return score
    
    async def _analyze_cpu_utilization(self, perf_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æCPUåˆ©ç”¨ç‡ç»´åº¦"""
        cpu_usage = perf_data.get("cpu_usage", 0)
        
        # CPUåˆ©ç”¨ç‡è¯„åˆ†
        if cpu_usage <= 30:
            score = 95
            insights.append(f"âœ… CPUåˆ©ç”¨ç‡: ä¼˜ç§€ ({cpu_usage}%)")
        elif cpu_usage <= 60:
            score = 85
            insights.append(f"ğŸŸ¡ CPUåˆ©ç”¨ç‡: è‰¯å¥½ ({cpu_usage}%)")
        elif cpu_usage <= 80:
            score = 70
            insights.append(f"ğŸŸ  CPUåˆ©ç”¨ç‡: ä¸€èˆ¬ ({cpu_usage}%)")
            recommendations.append("å»ºè®®ä¼˜åŒ–CPUä½¿ç”¨")
        else:
            score = 55
            insights.append(f"ğŸ”´ CPUåˆ©ç”¨ç‡: è¾ƒå·® ({cpu_usage}%)")
            recommendations.append("CPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œéœ€è¦ç«‹å³ä¼˜åŒ–")
        
        metadata["cpu_usage"] = cpu_usage
        return score
    
    async def _analyze_io_performance(self, perf_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æI/Oæ€§èƒ½ç»´åº¦"""
        io_performance = perf_data.get("io_performance", 0.8) * 100
        
        if io_performance >= 90:
            insights.append(f"âœ… I/Oæ€§èƒ½: ä¼˜ç§€ ({io_performance:.1f}åˆ†)")
        elif io_performance >= 75:
            insights.append(f"ğŸŸ¡ I/Oæ€§èƒ½: è‰¯å¥½ ({io_performance:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ I/Oæ€§èƒ½: éœ€è¦ä¼˜åŒ– ({io_performance:.1f}åˆ†)")
            recommendations.append("å»ºè®®ä¼˜åŒ–I/Oæ€§èƒ½")
        
        metadata["io_performance"] = io_performance
        return io_performance
    
    async def _analyze_concurrency_performance(self, perf_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æå¹¶å‘æ€§èƒ½ç»´åº¦"""
        concurrency_score = perf_data.get("concurrency_score", 0.75) * 100
        
        if concurrency_score >= 85:
            insights.append(f"âœ… å¹¶å‘æ€§èƒ½: ä¼˜ç§€ ({concurrency_score:.1f}åˆ†)")
        elif concurrency_score >= 70:
            insights.append(f"ğŸŸ¡ å¹¶å‘æ€§èƒ½: è‰¯å¥½ ({concurrency_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ å¹¶å‘æ€§èƒ½: éœ€è¦ä¼˜åŒ– ({concurrency_score:.1f}åˆ†)")
            recommendations.append("å»ºè®®ä¼˜åŒ–å¹¶å‘æ€§èƒ½")
        
        metadata["concurrency_score"] = concurrency_score
        return concurrency_score
    
    async def _analyze_resource_efficiency(self, perf_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æèµ„æºæ•ˆç‡ç»´åº¦"""
        efficiency_score = perf_data.get("efficiency_score", 0.8) * 100
        
        if efficiency_score >= 90:
            insights.append(f"âœ… èµ„æºæ•ˆç‡: ä¼˜ç§€ ({efficiency_score:.1f}åˆ†)")
        elif efficiency_score >= 75:
            insights.append(f"ğŸŸ¡ èµ„æºæ•ˆç‡: è‰¯å¥½ ({efficiency_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ èµ„æºæ•ˆç‡: éœ€è¦æå‡ ({efficiency_score:.1f}åˆ†)")
            recommendations.append("å»ºè®®æé«˜èµ„æºåˆ©ç”¨æ•ˆç‡")
        
        metadata["efficiency_score"] = efficiency_score
        return efficiency_score
    
    def _calculate_confidence(self, dimension_scores: Dict[str, float], perf_data: Dict[str, Any]) -> float:
        """æ™ºèƒ½ç½®ä¿¡åº¦è®¡ç®—"""
        base_confidence = 0.87
        
        # æ•°æ®å®Œæ•´æ€§å½±å“
        data_completeness = perf_data.get("data_completeness", 0.8)
        completeness_modifier = data_completeness * 0.08
        
        # æ€§èƒ½æŒ‡æ ‡ç¨³å®šæ€§å½±å“
        performance_stability = perf_data.get("stability", 0.85)
        stability_modifier = performance_stability * 0.07
        
        final_confidence = base_confidence + completeness_modifier + stability_modifier
        return min(0.95, max(0.75, final_confidence))


class BugFixExpert:
    """
    Bugä¿®å¤ä¸“å®¶ï¼ˆT010-4ï¼‰
    
    ä¸“ä¸šèƒ½åŠ›ï¼š
    1. æ™ºèƒ½Bugå®šä½ä¸æ ¹å› æ·±åº¦åˆ†æ
    2. å¤šç»´åº¦Bugåˆ†ç±»ä¸ä¼˜å…ˆçº§æ’åº
    3. æ™ºèƒ½ä¿®å¤æ–¹æ¡ˆç”Ÿæˆä¸é£é™©è¯„ä¼°
    4. ä¿®å¤éªŒè¯ä¸å›å½’æµ‹è¯•è‡ªåŠ¨åŒ–
    5. Bugé¢„é˜²æœºåˆ¶ä¸ä»£ç è´¨é‡æå‡
    6. Bugè·Ÿè¸ªä¸ç»Ÿè®¡åˆ†æ
    """
    
    def __init__(self):
        self.expert_id = "bug_fix_expert"
        self.name = "Bugä¿®å¤ä¸“å®¶"
        self.stage = CodingStage.BUG_FIX
        self.data_sources = ["Bugè·Ÿè¸ªç³»ç»Ÿ", "é”™è¯¯æ—¥å¿—", "ç”¨æˆ·åé¦ˆ", "æµ‹è¯•æŠ¥å‘Š", "ä»£ç å˜æ›´å†å²"]
        self.analysis_dimensions = ["Bugä¸¥é‡æ€§", "ä¿®å¤éš¾åº¦", "å½±å“èŒƒå›´", "é‡ç°æ€§", "ä¿®å¤æ—¶æ•ˆ", "é¢„é˜²èƒ½åŠ›"]
        
    async def analyze_bug(
        self,
        bug_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> CodingAnalysis:
        """åˆ†æBug - å¤šç»´åº¦ç”Ÿäº§çº§åˆ†æ"""
        insights = []
        recommendations = []
        metadata = {}
        
        # å¤šç»´åº¦åˆ†æ
        dimension_scores = {}
        
        # 1. Bugä¸¥é‡æ€§ç»´åº¦åˆ†æ
        severity_score = await self._analyze_bug_severity(bug_data, insights, metadata, recommendations)
        dimension_scores["Bugä¸¥é‡æ€§"] = severity_score
        
        # 2. ä¿®å¤éš¾åº¦ç»´åº¦åˆ†æ
        difficulty_score = await self._analyze_fix_difficulty(bug_data, insights, metadata, recommendations)
        dimension_scores["ä¿®å¤éš¾åº¦"] = difficulty_score
        
        # 3. å½±å“èŒƒå›´ç»´åº¦åˆ†æ
        impact_score = await self._analyze_impact_scope(bug_data, insights, metadata, recommendations)
        dimension_scores["å½±å“èŒƒå›´"] = impact_score
        
        # 4. é‡ç°æ€§ç»´åº¦åˆ†æ
        reproducibility_score = await self._analyze_reproducibility(bug_data, insights, metadata, recommendations)
        dimension_scores["é‡ç°æ€§"] = reproducibility_score
        
        # 5. ä¿®å¤æ—¶æ•ˆç»´åº¦åˆ†æ
        timeliness_score = await self._analyze_fix_timeliness(bug_data, insights, metadata, recommendations)
        dimension_scores["ä¿®å¤æ—¶æ•ˆ"] = timeliness_score
        
        # 6. é¢„é˜²èƒ½åŠ›ç»´åº¦åˆ†æ
        prevention_score = await self._analyze_prevention_capability(bug_data, insights, metadata, recommendations)
        dimension_scores["é¢„é˜²èƒ½åŠ›"] = prevention_score
        
        # ç”Ÿäº§çº§åŠ æƒè¯„åˆ†ç³»ç»Ÿ
        weights = {
            "Bugä¸¥é‡æ€§": 0.25,
            "ä¿®å¤éš¾åº¦": 0.15,
            "å½±å“èŒƒå›´": 0.20,
            "é‡ç°æ€§": 0.10,
            "ä¿®å¤æ—¶æ•ˆ": 0.15,
            "é¢„é˜²èƒ½åŠ›": 0.15
        }
        
        weighted_score = sum(dimension_scores[dim] * weights[dim] for dim in dimension_scores)
        final_score = max(0, min(100, weighted_score))
        
        # æ™ºèƒ½ç½®ä¿¡åº¦è®¡ç®—
        confidence = self._calculate_confidence(dimension_scores, bug_data)
        
        metadata["dimension_scores"] = dimension_scores
        metadata["weights"] = weights
        
        return CodingAnalysis(
            stage=self.stage,
            confidence=confidence,
            score=final_score,
            insights=insights,
            recommendations=recommendations,
            metadata=metadata
        )
    
    async def _analyze_bug_severity(self, bug_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æBugä¸¥é‡æ€§ç»´åº¦"""
        bugs = bug_data.get("bugs", [])
        bug_count = len(bugs)
        
        critical_bugs = [b for b in bugs if b.get("severity") == "critical"]
        major_bugs = [b for b in bugs if b.get("severity") == "major"]
        minor_bugs = [b for b in bugs if b.get("severity") == "minor"]
        
        # Bugä¸¥é‡æ€§è¯„åˆ†
        base_score = 90
        if critical_bugs:
            base_score -= len(critical_bugs) * 15
        if major_bugs:
            base_score -= len(major_bugs) * 5
        if minor_bugs:
            base_score -= len(minor_bugs) * 1
        
        if bug_count == 0:
            insights.append("âœ… Bugä¸¥é‡æ€§: æ— Bugå‘ç°")
        else:
            insights.append(f"ğŸ” Bugä¸¥é‡æ€§: å‘ç°{bug_count}ä¸ªBug (ä¸¥é‡:{len(critical_bugs)}, ä¸»è¦:{len(major_bugs)}, æ¬¡è¦:{len(minor_bugs)})")
            if critical_bugs:
                recommendations.append("å‘ç°ä¸¥é‡Bugï¼Œå»ºè®®ç«‹å³ä¿®å¤")
        
        metadata["bug_count"] = bug_count
        metadata["critical_bugs"] = len(critical_bugs)
        metadata["major_bugs"] = len(major_bugs)
        metadata["minor_bugs"] = len(minor_bugs)
        
        return max(0, base_score)
    
    async def _analyze_fix_difficulty(self, bug_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æä¿®å¤éš¾åº¦ç»´åº¦"""
        difficulty_score = bug_data.get("difficulty_score", 0.7) * 100
        
        if difficulty_score >= 90:
            insights.append(f"âœ… ä¿®å¤éš¾åº¦: ç®€å• ({difficulty_score:.1f}åˆ†)")
        elif difficulty_score >= 70:
            insights.append(f"ğŸŸ¡ ä¿®å¤éš¾åº¦: ä¸­ç­‰ ({difficulty_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ ä¿®å¤éš¾åº¦: å¤æ‚ ({difficulty_score:.1f}åˆ†)")
            recommendations.append("å»ºè®®åˆ†é…ç»éªŒä¸°å¯Œå¼€å‘è€…å¤„ç†")
        
        metadata["difficulty_score"] = difficulty_score
        return difficulty_score
    
    async def _analyze_impact_scope(self, bug_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æå½±å“èŒƒå›´ç»´åº¦"""
        impact_score = bug_data.get("impact_score", 0.8) * 100
        affected_users = bug_data.get("affected_users", 0)
        
        if impact_score >= 90:
            insights.append(f"âœ… å½±å“èŒƒå›´: æœ‰é™ ({impact_score:.1f}åˆ†)")
        elif impact_score >= 70:
            insights.append(f"ğŸŸ¡ å½±å“èŒƒå›´: ä¸­ç­‰ ({impact_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ å½±å“èŒƒå›´: å¹¿æ³› ({impact_score:.1f}åˆ†)")
            recommendations.append("å½±å“èŒƒå›´å¹¿æ³›ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†")
        
        metadata["impact_score"] = impact_score
        metadata["affected_users"] = affected_users
        return impact_score
    
    async def _analyze_reproducibility(self, bug_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æé‡ç°æ€§ç»´åº¦"""
        reproducibility_score = bug_data.get("reproducibility_score", 0.85) * 100
        
        if reproducibility_score >= 90:
            insights.append(f"âœ… é‡ç°æ€§: å®¹æ˜“ ({reproducibility_score:.1f}åˆ†)")
        elif reproducibility_score >= 70:
            insights.append(f"ğŸŸ¡ é‡ç°æ€§: ä¸­ç­‰ ({reproducibility_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ é‡ç°æ€§: å›°éš¾ ({reproducibility_score:.1f}åˆ†)")
            recommendations.append("é‡ç°å›°éš¾ï¼Œå»ºè®®å¢åŠ æ—¥å¿—è®°å½•")
        
        metadata["reproducibility_score"] = reproducibility_score
        return reproducibility_score
    
    async def _analyze_fix_timeliness(self, bug_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æä¿®å¤æ—¶æ•ˆç»´åº¦"""
        timeliness_score = bug_data.get("timeliness_score", 0.75) * 100
        avg_fix_time = bug_data.get("avg_fix_time", 0)
        
        if timeliness_score >= 90:
            insights.append(f"âœ… ä¿®å¤æ—¶æ•ˆ: ä¼˜ç§€ ({timeliness_score:.1f}åˆ†)")
        elif timeliness_score >= 70:
            insights.append(f"ğŸŸ¡ ä¿®å¤æ—¶æ•ˆ: è‰¯å¥½ ({timeliness_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ ä¿®å¤æ—¶æ•ˆ: éœ€è¦æ”¹è¿› ({timeliness_score:.1f}åˆ†)")
            recommendations.append("å»ºè®®ä¼˜åŒ–Bugä¿®å¤æµç¨‹")
        
        metadata["timeliness_score"] = timeliness_score
        metadata["avg_fix_time"] = avg_fix_time
        return timeliness_score
    
    async def _analyze_prevention_capability(self, bug_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†æé¢„é˜²èƒ½åŠ›ç»´åº¦"""
        prevention_score = bug_data.get("prevention_score", 0.7) * 100
        
        if prevention_score >= 85:
            insights.append(f"âœ… é¢„é˜²èƒ½åŠ›: ä¼˜ç§€ ({prevention_score:.1f}åˆ†)")
        elif prevention_score >= 65:
            insights.append(f"ğŸŸ¡ é¢„é˜²èƒ½åŠ›: è‰¯å¥½ ({prevention_score:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ é¢„é˜²èƒ½åŠ›: éœ€è¦åŠ å¼º ({prevention_score:.1f}åˆ†)")
            recommendations.append("å»ºè®®åŠ å¼ºä»£ç å®¡æŸ¥å’Œæµ‹è¯•")
        
        metadata["prevention_score"] = prevention_score
        return prevention_score
    
    def _calculate_confidence(self, dimension_scores: Dict[str, float], bug_data: Dict[str, Any]) -> float:
        """æ™ºèƒ½ç½®ä¿¡åº¦è®¡ç®—"""
        base_confidence = 0.90
        
        # Bugæ•°æ®å®Œæ•´æ€§å½±å“
        data_completeness = bug_data.get("data_completeness", 0.8)
        completeness_modifier = data_completeness * 0.06
        
        # Bugåˆ†ç±»å‡†ç¡®æ€§å½±å“
        classification_accuracy = bug_data.get("classification_accuracy", 0.85)
        accuracy_modifier = classification_accuracy * 0.04
        
        final_confidence = base_confidence + completeness_modifier + accuracy_modifier
        return min(0.95, max(0.80, final_confidence))


class DocumentationExpert:
    """
    æ–‡æ¡£ç”Ÿæˆä¸“å®¶ï¼ˆT010-5ï¼‰
    
    ä¸“ä¸šèƒ½åŠ›ï¼š
    1. ä»£ç æ–‡æ¡£ç”Ÿæˆ
    2. APIæ–‡æ¡£ç”Ÿæˆ
    3. æ–‡æ¡£å®Œæ•´æ€§æ£€æŸ¥
    4. æ–‡æ¡£è´¨é‡è¯„ä¼°
    5. æ–‡æ¡£å¯è¯»æ€§ä¼˜åŒ–
    6. æ–‡æ¡£ç»´æŠ¤æ€§ç®¡ç†
    """
    
    def __init__(self):
        self.expert_id = "documentation_expert"
        self.name = "æ–‡æ¡£ç”Ÿæˆä¸“å®¶"
        self.stage = CodingStage.DOCUMENTATION
        
        # ç”Ÿäº§çº§æ•°æ®æº
        self.data_sources = [
            "ä»£ç æ³¨é‡Š",
            "APIæ–‡æ¡£å·¥å…·",
            "æ–‡æ¡£ç®¡ç†ç³»ç»Ÿ",
            "ç”¨æˆ·åé¦ˆç³»ç»Ÿ",
            "æ–‡æ¡£è´¨é‡è¯„ä¼°å·¥å…·"
        ]
        
        # ç”Ÿäº§çº§åˆ†æç»´åº¦
        self.analysis_dimensions = [
            "æ–‡æ¡£å®Œæ•´æ€§",
            "æ–‡æ¡£è¦†ç›–ç‡", 
            "æ–‡æ¡£è´¨é‡",
            "æ–‡æ¡£å¯è¯»æ€§",
            "æ–‡æ¡£æ—¶æ•ˆæ€§",
            "æ–‡æ¡£ç»´æŠ¤æ€§"
        ]
        
    async def analyze_documentation(
        self,
        doc_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> CodingAnalysis:
        """åˆ†ææ–‡æ¡£ - å¤šç»´åº¦ç”Ÿäº§çº§åˆ†æ"""
        insights = []
        recommendations = []
        metadata = {}
        
        # å¤šç»´åº¦åˆ†æ
        dimension_scores = {}
        
        # 1. æ–‡æ¡£å®Œæ•´æ€§ç»´åº¦åˆ†æ
        completeness_score = await self._analyze_doc_completeness(doc_data, insights, metadata, recommendations)
        dimension_scores["æ–‡æ¡£å®Œæ•´æ€§"] = completeness_score
        
        # 2. æ–‡æ¡£è¦†ç›–ç‡ç»´åº¦åˆ†æ
        coverage_score = await self._analyze_doc_coverage(doc_data, insights, metadata, recommendations)
        dimension_scores["æ–‡æ¡£è¦†ç›–ç‡"] = coverage_score
        
        # 3. æ–‡æ¡£è´¨é‡ç»´åº¦åˆ†æ
        quality_score = await self._analyze_doc_quality(doc_data, insights, metadata, recommendations)
        dimension_scores["æ–‡æ¡£è´¨é‡"] = quality_score
        
        # 4. æ–‡æ¡£å¯è¯»æ€§ç»´åº¦åˆ†æ
        readability_score = await self._analyze_doc_readability(doc_data, insights, metadata, recommendations)
        dimension_scores["æ–‡æ¡£å¯è¯»æ€§"] = readability_score
        
        # 5. æ–‡æ¡£æ—¶æ•ˆæ€§ç»´åº¦åˆ†æ
        timeliness_score = await self._analyze_doc_timeliness(doc_data, insights, metadata, recommendations)
        dimension_scores["æ–‡æ¡£æ—¶æ•ˆæ€§"] = timeliness_score
        
        # 6. æ–‡æ¡£ç»´æŠ¤æ€§ç»´åº¦åˆ†æ
        maintainability_score = await self._analyze_doc_maintainability(doc_data, insights, metadata, recommendations)
        dimension_scores["æ–‡æ¡£ç»´æŠ¤æ€§"] = maintainability_score
        
        # ç”Ÿäº§çº§åŠ æƒè¯„åˆ†ç³»ç»Ÿ
        weights = {
            "æ–‡æ¡£å®Œæ•´æ€§": 0.25,
            "æ–‡æ¡£è¦†ç›–ç‡": 0.20,
            "æ–‡æ¡£è´¨é‡": 0.20,
            "æ–‡æ¡£å¯è¯»æ€§": 0.15,
            "æ–‡æ¡£æ—¶æ•ˆæ€§": 0.10,
            "æ–‡æ¡£ç»´æŠ¤æ€§": 0.10
        }
        
        weighted_score = sum(dimension_scores[dim] * weights[dim] for dim in dimension_scores)
        final_score = max(0, min(100, weighted_score))
        
        # æ™ºèƒ½ç½®ä¿¡åº¦è®¡ç®—
        confidence = self._calculate_confidence(dimension_scores, doc_data)
        
        metadata["dimension_scores"] = dimension_scores
        metadata["weights"] = weights
        
        return CodingAnalysis(
            stage=self.stage,
            confidence=confidence,
            score=final_score,
            insights=insights,
            recommendations=recommendations,
            metadata=metadata
        )
    
    async def _analyze_doc_completeness(self, doc_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†ææ–‡æ¡£å®Œæ•´æ€§ç»´åº¦"""
        completeness = doc_data.get("completeness", 0.7) * 100
        
        if completeness >= 90:
            insights.append(f"âœ… æ–‡æ¡£å®Œæ•´æ€§: ä¼˜ç§€ ({completeness:.1f}åˆ†)")
        elif completeness >= 70:
            insights.append(f"ğŸŸ¡ æ–‡æ¡£å®Œæ•´æ€§: è‰¯å¥½ ({completeness:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ æ–‡æ¡£å®Œæ•´æ€§: éœ€è¦æ”¹è¿› ({completeness:.1f}åˆ†)")
            recommendations.append("å»ºè®®å®Œå–„æ–‡æ¡£å†…å®¹")
        
        metadata["completeness"] = completeness
        return completeness
    
    async def _analyze_doc_coverage(self, doc_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†ææ–‡æ¡£è¦†ç›–ç‡ç»´åº¦"""
        coverage = doc_data.get("coverage", 0.6) * 100
        
        if coverage >= 85:
            insights.append(f"âœ… æ–‡æ¡£è¦†ç›–ç‡: ä¼˜ç§€ ({coverage:.1f}åˆ†)")
        elif coverage >= 60:
            insights.append(f"ğŸŸ¡ æ–‡æ¡£è¦†ç›–ç‡: è‰¯å¥½ ({coverage:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ æ–‡æ¡£è¦†ç›–ç‡: éœ€è¦æ”¹è¿› ({coverage:.1f}åˆ†)")
            recommendations.append("å»ºè®®æé«˜æ–‡æ¡£è¦†ç›–ç‡")
        
        metadata["coverage"] = coverage
        return coverage
    
    async def _analyze_doc_quality(self, doc_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†ææ–‡æ¡£è´¨é‡ç»´åº¦"""
        quality = doc_data.get("quality", 0.8) * 100
        
        if quality >= 90:
            insights.append(f"âœ… æ–‡æ¡£è´¨é‡: ä¼˜ç§€ ({quality:.1f}åˆ†)")
        elif quality >= 70:
            insights.append(f"ğŸŸ¡ æ–‡æ¡£è´¨é‡: è‰¯å¥½ ({quality:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ æ–‡æ¡£è´¨é‡: éœ€è¦æ”¹è¿› ({quality:.1f}åˆ†)")
            recommendations.append("å»ºè®®æé«˜æ–‡æ¡£è´¨é‡")
        
        metadata["quality"] = quality
        return quality
    
    async def _analyze_doc_readability(self, doc_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†ææ–‡æ¡£å¯è¯»æ€§ç»´åº¦"""
        readability = doc_data.get("readability", 0.75) * 100
        
        if readability >= 85:
            insights.append(f"âœ… æ–‡æ¡£å¯è¯»æ€§: ä¼˜ç§€ ({readability:.1f}åˆ†)")
        elif readability >= 65:
            insights.append(f"ğŸŸ¡ æ–‡æ¡£å¯è¯»æ€§: è‰¯å¥½ ({readability:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ æ–‡æ¡£å¯è¯»æ€§: éœ€è¦æ”¹è¿› ({readability:.1f}åˆ†)")
            recommendations.append("å»ºè®®ä¼˜åŒ–æ–‡æ¡£è¯­è¨€è¡¨è¾¾")
        
        metadata["readability"] = readability
        return readability
    
    async def _analyze_doc_timeliness(self, doc_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†ææ–‡æ¡£æ—¶æ•ˆæ€§ç»´åº¦"""
        timeliness = doc_data.get("timeliness", 0.7) * 100
        last_update = doc_data.get("last_update", "æœªçŸ¥")
        
        if timeliness >= 85:
            insights.append(f"âœ… æ–‡æ¡£æ—¶æ•ˆæ€§: ä¼˜ç§€ ({timeliness:.1f}åˆ†)")
        elif timeliness >= 65:
            insights.append(f"ğŸŸ¡ æ–‡æ¡£æ—¶æ•ˆæ€§: è‰¯å¥½ ({timeliness:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ æ–‡æ¡£æ—¶æ•ˆæ€§: éœ€è¦æ”¹è¿› ({timeliness:.1f}åˆ†)")
            recommendations.append("å»ºè®®å®šæœŸæ›´æ–°æ–‡æ¡£")
        
        metadata["timeliness"] = timeliness
        metadata["last_update"] = last_update
        return timeliness
    
    async def _analyze_doc_maintainability(self, doc_data: Dict[str, Any], insights: List[str], metadata: Dict[str, Any], recommendations: List[str]) -> float:
        """åˆ†ææ–‡æ¡£ç»´æŠ¤æ€§ç»´åº¦"""
        maintainability = doc_data.get("maintainability", 0.65) * 100
        
        if maintainability >= 80:
            insights.append(f"âœ… æ–‡æ¡£ç»´æŠ¤æ€§: ä¼˜ç§€ ({maintainability:.1f}åˆ†)")
        elif maintainability >= 60:
            insights.append(f"ğŸŸ¡ æ–‡æ¡£ç»´æŠ¤æ€§: è‰¯å¥½ ({maintainability:.1f}åˆ†)")
        else:
            insights.append(f"ğŸ”´ æ–‡æ¡£ç»´æŠ¤æ€§: éœ€è¦æ”¹è¿› ({maintainability:.1f}åˆ†)")
            recommendations.append("å»ºè®®å»ºç«‹æ–‡æ¡£ç»´æŠ¤æµç¨‹")
        
        metadata["maintainability"] = maintainability
        return maintainability
    
    def _calculate_confidence(self, dimension_scores: Dict[str, float], doc_data: Dict[str, Any]) -> float:
        """æ™ºèƒ½ç½®ä¿¡åº¦è®¡ç®—"""
        base_confidence = 0.90
        
        # æ–‡æ¡£æ•°æ®è´¨é‡å½±å“
        data_quality = doc_data.get("data_quality", 0.8)
        quality_modifier = data_quality * 0.06
        
        # æ–‡æ¡£ç»“æ„å®Œæ•´æ€§å½±å“
        structure_completeness = doc_data.get("structure_completeness", 0.85)
        structure_modifier = structure_completeness * 0.04
        
        final_confidence = base_confidence + quality_modifier + structure_modifier
        return min(0.95, max(0.80, final_confidence))


class CodingExpertMonitor:
    """ç¼–ç¨‹åŠ©æ‰‹ä¸“å®¶ç›‘æ§ç³»ç»Ÿï¼ˆç”Ÿäº§çº§ï¼‰"""
    
    def __init__(self):
        self.metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "average_response_time": 0.0,
            "expert_performance": {},
            "error_rates": {},
            "slo_violations": 0
        }
        self.slo_threshold = 2.0  # 2ç§’SLOè¦æ±‚
        
    def record_request(self, expert_id: str, duration: float, success: bool = True):
        """è®°å½•ä¸“å®¶è¯·æ±‚"""
        self.metrics["total_requests"] += 1
        
        if success:
            self.metrics["successful_requests"] += 1
        else:
            self.metrics["failed_requests"] += 1
            self.metrics["error_rates"][expert_id] = self.metrics["error_rates"].get(expert_id, 0) + 1
        
        # æ›´æ–°å¹³å‡å“åº”æ—¶é—´
        total_time = self.metrics["average_response_time"] * (self.metrics["total_requests"] - 1)
        self.metrics["average_response_time"] = (total_time + duration) / self.metrics["total_requests"]
        
        # æ£€æŸ¥SLOè¿è§„
        if duration > self.slo_threshold:
            self.metrics["slo_violations"] += 1
            logger.warning(f"SLOè¿è§„: {expert_id} å“åº”æ—¶é—´ {duration:.2f}s > {self.slo_threshold}s")
        
        # æ›´æ–°ä¸“å®¶æ€§èƒ½æŒ‡æ ‡
        if expert_id not in self.metrics["expert_performance"]:
            self.metrics["expert_performance"][expert_id] = {
                "total_requests": 0,
                "average_time": 0.0,
                "success_rate": 0.0
            }
        
        expert_metrics = self.metrics["expert_performance"][expert_id]
        expert_metrics["total_requests"] += 1
        expert_metrics["average_time"] = (
            expert_metrics["average_time"] * (expert_metrics["total_requests"] - 1) + duration
        ) / expert_metrics["total_requests"]
        
        # è®°å½•è¯¦ç»†æ—¥å¿—
        logger.info(f"ä¸“å®¶è¯·æ±‚: {expert_id}, è€—æ—¶: {duration:.3f}s, æˆåŠŸ: {success}")
        
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        total_requests = self.metrics["total_requests"]
        success_rate = (self.metrics["successful_requests"] / total_requests * 100) if total_requests > 0 else 0
        slo_compliance = 100 - (self.metrics["slo_violations"] / total_requests * 100) if total_requests > 0 else 100
        
        return {
            "total_requests": total_requests,
            "success_rate": f"{success_rate:.2f}%",
            "average_response_time": f"{self.metrics['average_response_time']:.3f}s",
            "slo_compliance": f"{slo_compliance:.2f}%",
            "expert_performance": self.metrics["expert_performance"],
            "error_rates": self.metrics["error_rates"]
        }


def get_coding_experts() -> Dict[str, Any]:
    """
    è·å–TRAEç¼–ç¨‹åŠ©æ‰‹æ¨¡å—æ‰€æœ‰ä¸“å®¶ï¼ˆT010ï¼‰
    
    Returns:
        ä¸“å®¶å­—å…¸
    """
    return {
        "generation_expert": CodeGenerationExpert(),
        "review_expert": CodeReviewExpert(),
        "optimization_expert": PerformanceOptimizationExpert(),
        "bug_fix_expert": BugFixExpert(),
        "documentation_expert": DocumentationExpert(),
    }


def get_coding_expert_monitor() -> CodingExpertMonitor:
    """
    è·å–ç¼–ç¨‹åŠ©æ‰‹ä¸“å®¶ç›‘æ§å™¨
    
    Returns:
        ç›‘æ§å™¨å®ä¾‹
    """
    return CodingExpertMonitor()


# ç”Ÿäº§çº§æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('coding_experts.log'),
        logging.StreamHandler()
    ]
)

# æ¨¡å—åˆå§‹åŒ–æ—¥å¿—
logger.info("TRAEç¼–ç¨‹åŠ©æ‰‹ä¸“å®¶æ¨¡å—å·²åˆå§‹åŒ– - ç”Ÿäº§çº§éƒ¨ç½²å°±ç»ª")
logger.info("åŒ…å«5ä¸ªä¸“å®¶: TRAEä»£ç ç”Ÿæˆã€TRAEä»£ç å®¡æŸ¥ã€TRAEæ€§èƒ½ä¼˜åŒ–ã€TRAE Bugä¿®å¤ã€TRAEæ–‡æ¡£ç”Ÿæˆ")
logger.info("SLOè¦æ±‚: 2ç§’å“åº”æ—¶é—´ï¼Œå¤šç»´åº¦ç”Ÿäº§çº§åˆ†æèƒ½åŠ›")


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å·¥ä½œæµéªŒè¯æµ‹è¯•è„šæœ¬

åŠŸèƒ½ï¼š
1. æ‰§è¡Œå®Œæ•´çš„å·¥ä½œæµéªŒè¯æµ‹è¯•
2. æ”¯æŒå¤šç§æµ‹è¯•åœºæ™¯
3. ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
4. ä¸éªŒè¯ç›‘æ§å™¨é›†æˆ
"""

from __future__ import annotations

import asyncio
import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).resolve().parents[2]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from core.workflow_validation_monitor import (
    WorkflowValidationMonitor,
    get_workflow_validation_monitor,
    ValidationStatus,
    WorkflowValidationResult,
)

logger = logging.getLogger(__name__)


class WorkflowValidationTestRunner:
    """å·¥ä½œæµéªŒè¯æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, monitor: Optional[WorkflowValidationMonitor] = None):
        self.monitor = monitor or get_workflow_validation_monitor()
        self.test_scenarios: List[Dict[str, Any]] = []
        self._setup_test_scenarios()
    
    def _setup_test_scenarios(self):
        """è®¾ç½®æµ‹è¯•åœºæ™¯"""
        self.test_scenarios = [
            {
                "name": "ERPè®¢å•æŸ¥è¯¢å·¥ä½œæµ",
                "description": "æµ‹è¯•ERPç³»ç»Ÿä¸­çš„è®¢å•æŸ¥è¯¢å®Œæ•´é“¾è·¯",
                "input": {
                    "query": "æŸ¥è¯¢æœ€è¿‘3å¤©çš„è®¢å•çŠ¶æ€",
                    "workflow_type": "intelligent",
                    "expected_steps": ["RAGæ£€ç´¢", "ä¸“å®¶è·¯ç”±", "æ¨¡å—æ‰§è¡Œ", "ä¸“å®¶æ•´åˆ", "RAGå­˜å‚¨"],
                },
                "expected_output": {
                    "status": ValidationStatus.PASSED,
                    "min_steps": 4,
                    "max_duration": 3.0,  # ç§’
                }
            },
            {
                "name": "å†…å®¹åˆ›ä½œå»ºè®®å·¥ä½œæµ",
                "description": "æµ‹è¯•å†…å®¹åˆ›ä½œç³»ç»Ÿçš„å»ºè®®ç”Ÿæˆé“¾è·¯",
                "input": {
                    "query": "ä¸ºæ–°äº§å“ç”Ÿæˆè¥é”€å†…å®¹å»ºè®®",
                    "workflow_type": "intelligent",
                    "expected_steps": ["RAGæ£€ç´¢", "ç­–åˆ’ä¸“å®¶", "ç”Ÿæˆä¸“å®¶", "å»AIåŒ–ä¸“å®¶", "RAGå­˜å‚¨"],
                },
                "expected_output": {
                    "status": ValidationStatus.PASSED,
                    "min_steps": 4,
                    "max_duration": 3.0,
                }
            },
            {
                "name": "è‚¡ç¥¨è¶‹åŠ¿åˆ†æå·¥ä½œæµ",
                "description": "æµ‹è¯•è‚¡ç¥¨é‡åŒ–ç³»ç»Ÿçš„è¶‹åŠ¿åˆ†æé“¾è·¯",
                "input": {
                    "query": "åˆ†æAAPLè‚¡ç¥¨æœ€è¿‘ä¸€å‘¨çš„è¶‹åŠ¿",
                    "workflow_type": "intelligent",
                    "expected_steps": ["RAGæ£€ç´¢", "æŠ€æœ¯åˆ†æä¸“å®¶", "åŸºæœ¬é¢ä¸“å®¶", "é£é™©åˆ†æä¸“å®¶", "RAGå­˜å‚¨"],
                },
                "expected_output": {
                    "status": ValidationStatus.PASSED,
                    "min_steps": 4,
                    "max_duration": 3.0,
                }
            },
            {
                "name": "ç›´æ¥æ“ä½œå·¥ä½œæµ",
                "description": "æµ‹è¯•ç›´æ¥æ“ä½œå·¥ä½œæµçš„æ‰§è¡Œé“¾è·¯",
                "input": {
                    "query": "æ‰§è¡Œç³»ç»ŸçŠ¶æ€æ£€æŸ¥",
                    "workflow_type": "direct",
                    "expected_steps": ["æ¨¡å—æ‰§è¡Œ", "ç»“æœè¿”å›"],
                },
                "expected_output": {
                    "status": ValidationStatus.PASSED,
                    "min_steps": 2,
                    "max_duration": 1.5,
                }
            },
            {
                "name": "é”™è¯¯å¤„ç†å·¥ä½œæµ",
                "description": "æµ‹è¯•å·¥ä½œæµé”™è¯¯å¤„ç†æœºåˆ¶",
                "input": {
                    "query": "æ‰§è¡Œæ— æ•ˆæ“ä½œ",
                    "workflow_type": "intelligent",
                    "expected_steps": ["RAGæ£€ç´¢", "é”™è¯¯å¤„ç†"],
                },
                "expected_output": {
                    "status": ValidationStatus.FAILED,
                    "min_steps": 1,
                    "max_duration": 2.0,
                }
            }
        ]
    
    async def run_single_test(self, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•åœºæ™¯"""
        logger.info(f"å¼€å§‹æµ‹è¯•: {scenario['name']}")
        
        start_time = time.time()
        
        try:
            # æ¨¡æ‹Ÿå·¥ä½œæµæ‰§è¡Œ
            workflow_id = f"test_{int(time.time())}_{scenario['name'][:10]}"
            
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„å·¥ä½œæµæ‰§è¡Œå™¨
            # ç›®å‰å…ˆæ¨¡æ‹Ÿæ‰§è¡Œ
            await self._simulate_workflow_execution(scenario)
            
            # è®°å½•éªŒè¯ç»“æœ
            result = WorkflowValidationResult(
                workflow_id=workflow_id,
                workflow_type=scenario['input']['workflow_type'],
                user_input=scenario['input']['query'],
                status=ValidationStatus.PASSED,  # æ¨¡æ‹ŸæˆåŠŸ
                duration_seconds=time.time() - start_time,
                steps_count=len(scenario['input']['expected_steps']),
                successful_steps=len(scenario['input']['expected_steps']),
                rag_calls=2,  # æ¨¡æ‹Ÿä¸¤æ¬¡RAGè°ƒç”¨
                validation_details={
                    "scenario": scenario['name'],
                    "input": scenario['input'],
                    "simulated": True,
                },
                timestamp=datetime.now(),
            )
            
            # éªŒè¯ç»“æœä¼šè‡ªåŠ¨æ·»åŠ åˆ°ç›‘æ§å™¨ä¸­
            
            # æ£€æŸ¥æ˜¯å¦ç¬¦åˆé¢„æœŸ
            expected = scenario['expected_output']
            is_success = (
                result.status == expected['status'] and
                result.successful_steps >= expected['min_steps'] and
                result.duration_seconds <= expected['max_duration']
            )
            
            test_result = {
                "scenario_name": scenario['name'],
                "status": "PASSED" if is_success else "FAILED",
                "duration": result.duration_seconds,
                "steps_completed": f"{result.successful_steps}/{result.steps_count}",
                "expected_status": expected['status'].value,
                "actual_status": result.status.value,
                "details": {
                    "workflow_id": workflow_id,
                    "simulated": True,
                }
            }
            
            logger.info(f"æµ‹è¯•å®Œæˆ: {scenario['name']} - {test_result['status']}")
            return test_result
            
        except Exception as e:
            logger.error(f"æµ‹è¯•å¤±è´¥: {scenario['name']} - {e}")
            
            # è®°å½•å¤±è´¥ç»“æœ
            result = WorkflowValidationResult(
                workflow_id=f"test_{int(time.time())}_{scenario['name'][:10]}",
                workflow_type=scenario['input']['workflow_type'],
                user_input=scenario['input']['query'],
                status=ValidationStatus.FAILED,
                duration_seconds=time.time() - start_time,
                steps_count=len(scenario['input']['expected_steps']),
                successful_steps=0,
                rag_calls=0,
                validation_details={
                    "scenario": scenario['name'],
                    "error": str(e),
                    "simulated": True,
                },
                timestamp=datetime.now(),
                error=str(e),
            )
            
            # éªŒè¯ç»“æœä¼šè‡ªåŠ¨æ·»åŠ åˆ°ç›‘æ§å™¨ä¸­
            
            return {
                "scenario_name": scenario['name'],
                "status": "FAILED",
                "duration": result.duration_seconds,
                "steps_completed": "0/0",
                "error": str(e),
                "details": {"workflow_id": result.workflow_id}
            }
    
    async def _simulate_workflow_execution(self, scenario: Dict[str, Any]):
        """æ¨¡æ‹Ÿå·¥ä½œæµæ‰§è¡Œ"""
        # æ¨¡æ‹Ÿæ‰§è¡Œæ—¶é—´
        execution_time = 0.5 + (hash(scenario['name']) % 100) / 1000  # 0.5-0.6ç§’
        await asyncio.sleep(execution_time)
        
        # æ¨¡æ‹Ÿå¯èƒ½çš„å¤±è´¥
        if "é”™è¯¯å¤„ç†" in scenario['name']:
            raise Exception("æ¨¡æ‹Ÿå·¥ä½œæµæ‰§è¡Œé”™è¯¯")
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•åœºæ™¯"""
        logger.info("å¼€å§‹è¿è¡Œæ‰€æœ‰å·¥ä½œæµéªŒè¯æµ‹è¯•...")
        
        start_time = time.time()
        test_results = []
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        for scenario in self.test_scenarios:
            result = await self.run_single_test(scenario)
            test_results.append(result)
        
        # ç»Ÿè®¡ç»“æœ
        total_tests = len(test_results)
        passed_tests = sum(1 for r in test_results if r['status'] == "PASSED")
        failed_tests = total_tests - passed_tests
        
        # è®¡ç®—å¹³å‡å“åº”æ—¶é—´
        avg_duration = sum(r['duration'] for r in test_results) / total_tests
        
        summary = {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "pass_rate": passed_tests / total_tests,
            "average_duration": avg_duration,
            "total_duration": time.time() - start_time,
            "test_results": test_results,
            "timestamp": datetime.now().isoformat(),
        }
        
        logger.info(f"æ‰€æœ‰æµ‹è¯•å®Œæˆ: {passed_tests}/{total_tests} é€šè¿‡")
        return summary
    
    def generate_test_report(self, summary: Dict[str, Any]) -> str:
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        report = f"""
# AI-STACK å·¥ä½œæµéªŒè¯æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ‘˜è¦
- æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- æ€»æµ‹è¯•åœºæ™¯: {summary['total_tests']}
- é€šè¿‡æµ‹è¯•: {summary['passed_tests']}
- å¤±è´¥æµ‹è¯•: {summary['failed_tests']}
- é€šè¿‡ç‡: {summary['pass_rate']:.1%}
- å¹³å‡å“åº”æ—¶é—´: {summary['average_duration']:.3f}ç§’
- æ€»æµ‹è¯•æ—¶é•¿: {summary['total_duration']:.2f}ç§’

## è¯¦ç»†ç»“æœ
"""
        
        # æ·»åŠ æ¯ä¸ªæµ‹è¯•çš„è¯¦ç»†ç»“æœ
        for i, result in enumerate(summary['test_results'], 1):
            status_icon = "âœ…" if result['status'] == "PASSED" else "âŒ"
            report += f"\n### {i}. {result['scenario_name']} {status_icon}\n"
            report += f"- çŠ¶æ€: {result['status']}\n"
            report += f"- å“åº”æ—¶é—´: {result['duration']:.3f}ç§’\n"
            report += f"- æ­¥éª¤å®Œæˆ: {result['steps_completed']}\n"
            
            if 'error' in result:
                report += f"- é”™è¯¯ä¿¡æ¯: {result['error']}\n"
            
            if 'expected_status' in result:
                report += f"- é¢„æœŸçŠ¶æ€: {result['expected_status']}\n"
                report += f"- å®é™…çŠ¶æ€: {result['actual_status']}\n"
        
        # æ·»åŠ å»ºè®®
        report += "\n## æµ‹è¯•å»ºè®®\n"
        
        if summary['pass_rate'] == 1.0:
            report += "- âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Œå·¥ä½œæµéªŒè¯æœºåˆ¶è¿è¡Œæ­£å¸¸\n"
        elif summary['pass_rate'] >= 0.8:
            report += "- âš ï¸  å¤§éƒ¨åˆ†æµ‹è¯•é€šè¿‡ï¼Œå»ºè®®æ£€æŸ¥å¤±è´¥åœºæ™¯\n"
        else:
            report += "- â— é€šè¿‡ç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥å·¥ä½œæµå®ç°\n"
        
        if summary['average_duration'] > 1.0:
            report += "- â±ï¸  å“åº”æ—¶é—´è¾ƒé•¿ï¼Œå»ºè®®ä¼˜åŒ–æ€§èƒ½\n"
        else:
            report += "- âš¡ å“åº”æ—¶é—´è‰¯å¥½\n"
        
        report += "- ğŸ“Š å»ºè®®å®šæœŸè¿è¡ŒéªŒè¯æµ‹è¯•\n"
        report += "- ğŸ”„ æŒç»­ç›‘æ§å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€\n"
        
        return report
    
    def save_test_report(self, summary: Dict[str, Any], report_dir: Path = Path("validation_reports")):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        report_dir.mkdir(exist_ok=True)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_content = self.generate_test_report(summary)
        
        # ä¿å­˜æŠ¥å‘Šæ–‡ä»¶
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = report_dir / f"workflow_test_report_{timestamp}.md"
        
        report_file.write_text(report_content, encoding="utf-8")
        logger.info(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        return report_file


async def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    test_runner = WorkflowValidationTestRunner()
    
    try:
        print("ğŸš€ å¼€å§‹å·¥ä½œæµéªŒè¯æµ‹è¯•...")
        print("=" * 50)
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        summary = await test_runner.run_all_tests()
        
        # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
        print("\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
        print(f"   æ€»æµ‹è¯•åœºæ™¯: {summary['total_tests']}")
        print(f"   é€šè¿‡æµ‹è¯•: {summary['passed_tests']}")
        print(f"   å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
        print(f"   é€šè¿‡ç‡: {summary['pass_rate']:.1%}")
        print(f"   å¹³å‡å“åº”æ—¶é—´: {summary['average_duration']:.3f}ç§’")
        
        # æ˜¾ç¤ºè¯¦ç»†ç»“æœ
        print("\nğŸ” è¯¦ç»†ç»“æœ:")
        for result in summary['test_results']:
            status_icon = "âœ…" if result['status'] == "PASSED" else "âŒ"
            print(f"   {status_icon} {result['scenario_name']}: {result['duration']:.3f}ç§’")
        
        # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
        report_file = test_runner.save_test_report(summary)
        print(f"\nğŸ“„ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # å¯åŠ¨ä»ªè¡¨æ¿ï¼ˆå¯é€‰ï¼‰
        if summary['passed_tests'] > 0:
            print("\nğŸ’¡ æç¤º: å¯ä»¥è¿è¡Œ 'python tools/workflow_validation_dashboard.py' å¯åŠ¨å®æ—¶ç›‘æ§ä»ªè¡¨æ¿")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•è¿è¡Œå¤±è´¥: {e}")
        print(f"âŒ æµ‹è¯•è¿è¡Œå¤±è´¥: {e}")


if __name__ == "__main__":
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è¿è¡Œä¸»å‡½æ•°
    asyncio.run(main())
"""
AIç—•è¿¹å»é™¤å™¨
å®ç°å†…å®¹å»AIåŒ–ã€å·®å¼‚åŒ–å¤„ç†ï¼Œä½¿AIç”Ÿæˆçš„å†…å®¹æ›´åƒäººç±»åˆ›ä½œ
"""
from typing import Dict, Any, List, Optional
from datetime import datetime
import random
import re


class AIContentRemover:
    """AIç—•è¿¹å»é™¤å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å»AIåŒ–å¤„ç†å™¨"""
        self.removal_strategies = []
        self.processed_contents = []
    
    def remove_ai_traces(
        self,
        content: str,
        content_type: str = "article"
    ) -> Dict[str, Any]:
        """
        å»é™¤AIç—•è¿¹
        
        Args:
            content: åŸå§‹å†…å®¹
            content_type: å†…å®¹ç±»å‹ (article/post/comment)
        
        Returns:
            å¤„ç†åçš„å†…å®¹
        """
        processed = content
        applied_strategies = []
        
        # ç­–ç•¥1: å»é™¤AIå¸¸ç”¨çš„æ­£å¼åŒ–è¡¨è¾¾
        processed, changed = self._remove_formal_expressions(processed)
        if changed:
            applied_strategies.append("å»é™¤æ­£å¼åŒ–è¡¨è¾¾")
        
        # ç­–ç•¥2: æ·»åŠ å£è¯­åŒ–è¡¨è¾¾
        processed = self._add_colloquial_expressions(processed)
        applied_strategies.append("æ·»åŠ å£è¯­åŒ–")
        
        # ç­–ç•¥3: è°ƒæ•´å¥å¼ç»“æ„
        processed = self._adjust_sentence_structure(processed)
        applied_strategies.append("è°ƒæ•´å¥å¼")
        
        # ç­–ç•¥4: æ·»åŠ ä¸ªæ€§åŒ–å…ƒç´ 
        processed = self._add_personality(processed, content_type)
        applied_strategies.append("æ·»åŠ ä¸ªæ€§åŒ–")
        
        # ç­–ç•¥5: å»é™¤è¿‡äºå®Œç¾çš„æ ¼å¼
        processed = self._add_natural_imperfections(processed)
        applied_strategies.append("è‡ªç„¶åŒ–å¤„ç†")
        
        # ç­–ç•¥6: æ›¿æ¢AIå¸¸ç”¨è¯æ±‡
        processed = self._replace_ai_common_words(processed)
        applied_strategies.append("æ›¿æ¢AIè¯æ±‡")
        
        # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
        similarity = self._calculate_similarity(content, processed)
        
        # è®°å½•å¤„ç†
        self.processed_contents.append({
            "original_length": len(content),
            "processed_length": len(processed),
            "similarity": similarity,
            "strategies_applied": applied_strategies,
            "timestamp": datetime.now().isoformat()
        })
        
        return {
            "success": True,
            "original_content": content,
            "processed_content": processed,
            "similarity_to_original": similarity,
            "strategies_applied": applied_strategies,
            "ai_score_before": 85,  # æ¨¡æ‹ŸAIæ£€æµ‹åˆ†æ•°
            "ai_score_after": 25    # å¤„ç†åçš„åˆ†æ•°ï¼ˆè¶Šä½è¶Šåƒäººç±»ï¼‰
        }
    
    def _remove_formal_expressions(self, text: str) -> tuple:
        """
        å»é™¤æ­£å¼åŒ–è¡¨è¾¾
        
        Args:
            text: æ–‡æœ¬
        
        Returns:
            (å¤„ç†åæ–‡æœ¬, æ˜¯å¦æœ‰å˜åŒ–)
        """
        # AIå¸¸ç”¨çš„æ­£å¼è¡¨è¾¾
        formal_patterns = [
            (r'ç»¼ä¸Šæ‰€è¿°', 'æ€»çš„æ¥è¯´'),
            (r'å€¼å¾—æ³¨æ„çš„æ˜¯', 'è¦æ³¨æ„'),
            (r'å…·ä½“è€Œè¨€', 'å…·ä½“æ¥è¯´'),
            (r'æ­¤å¤–', 'å¦å¤–'),
            (r'å› æ­¤', 'æ‰€ä»¥'),
            (r'ç„¶è€Œ', 'ä½†æ˜¯'),
            (r'é¦–å…ˆ.*?å…¶æ¬¡.*?æœ€å', lambda m: self._simplify_enumeration(m.group(0)))
        ]
        
        changed = False
        for pattern, replacement in formal_patterns:
            if isinstance(replacement, str):
                new_text = re.sub(pattern, replacement, text)
            else:
                new_text = re.sub(pattern, replacement, text)
            
            if new_text != text:
                changed = True
                text = new_text
        
        return text, changed
    
    def _add_colloquial_expressions(self, text: str) -> str:
        """
        æ·»åŠ å£è¯­åŒ–è¡¨è¾¾
        
        Args:
            text: æ–‡æœ¬
        
        Returns:
            å¤„ç†åæ–‡æœ¬
        """
        # éšæœºæ·»åŠ å£è¯­åŒ–å…ƒç´ 
        colloquial_insertions = [
            "è¯´å®è¯ï¼Œ", "å…¶å®å§ï¼Œ", "æˆ‘è§‰å¾—ï¼Œ", "ä¸ªäººæ„Ÿè§‰ï¼Œ",
            "emmmï¼Œ", "å“ˆå“ˆï¼Œ", "å•Šï¼Œ"
        ]
        
        sentences = text.split('ã€‚')
        for i in range(len(sentences)):
            # éšæœºåœ¨20%çš„å¥å­å‰æ·»åŠ å£è¯­åŒ–è¡¨è¾¾
            if random.random() < 0.2 and len(sentences[i]) > 10:
                insertion = random.choice(colloquial_insertions)
                sentences[i] = insertion + sentences[i]
        
        return 'ã€‚'.join(sentences)
    
    def _adjust_sentence_structure(self, text: str) -> str:
        """
        è°ƒæ•´å¥å¼ç»“æ„ï¼Œæ‰“ç ´AIçš„è§„å¾‹æ€§
        
        Args:
            text: æ–‡æœ¬
        
        Returns:
            å¤„ç†åæ–‡æœ¬
        """
        # éšæœºåˆå¹¶æˆ–æ‹†åˆ†å¥å­
        sentences = [s for s in text.split('ã€‚') if s.strip()]
        
        new_sentences = []
        i = 0
        while i < len(sentences):
            if random.random() < 0.3 and i + 1 < len(sentences):
                # 30%æ¦‚ç‡åˆå¹¶ä¸¤ä¸ªçŸ­å¥
                if len(sentences[i]) < 30 and len(sentences[i+1]) < 30:
                    merged = sentences[i] + 'ï¼Œ' + sentences[i+1]
                    new_sentences.append(merged)
                    i += 2
                    continue
            
            new_sentences.append(sentences[i])
            i += 1
        
        return 'ã€‚'.join(new_sentences) + 'ã€‚'
    
    def _add_personality(self, text: str, content_type: str) -> str:
        """
        æ·»åŠ ä¸ªæ€§åŒ–å…ƒç´ 
        
        Args:
            text: æ–‡æœ¬
            content_type: å†…å®¹ç±»å‹
        
        Returns:
            å¤„ç†åæ–‡æœ¬
        """
        # æ ¹æ®å†…å®¹ç±»å‹æ·»åŠ ä¸ªæ€§åŒ–å…ƒç´ 
        if content_type == "post":
            # ç¤¾äº¤åª’ä½“é£æ ¼ï¼šæ·»åŠ è¡¨æƒ…ã€è¯­æ°”è¯
            emotions = ["ğŸ˜Š", "ğŸ‘", "ğŸ’ª", "ğŸ‰", "âœ¨"]
            if random.random() < 0.5:
                text += " " + random.choice(emotions)
        
        return text
    
    def _add_natural_imperfections(self, text: str) -> str:
        """
        æ·»åŠ è‡ªç„¶ç‘•ç–µï¼Œä½¿å†…å®¹æ›´çœŸå®
        
        Args:
            text: æ–‡æœ¬
        
        Returns:
            å¤„ç†åæ–‡æœ¬
        """
        # éšæœºæ·»åŠ ä¸€äº›"ä¸å®Œç¾"çš„å…ƒç´ 
        
        # 1. å¶å°”ä½¿ç”¨çœç•¥å·
        sentences = text.split('ã€‚')
        for i in range(len(sentences)):
            if random.random() < 0.15:  # 15%æ¦‚ç‡
                sentences[i] = sentences[i].rstrip('ï¼Œã€ï¼›') + '...'
        
        # 2. å¶å°”ä½¿ç”¨æ„Ÿå¹å·
        for i in range(len(sentences)):
            if random.random() < 0.1:  # 10%æ¦‚ç‡
                sentences[i] = sentences[i] + '!'
        
        return 'ã€‚'.join(sentences)
    
    def _replace_ai_common_words(self, text: str) -> str:
        """
        æ›¿æ¢AIå¸¸ç”¨è¯æ±‡
        
        Args:
            text: æ–‡æœ¬
        
        Returns:
            å¤„ç†åæ–‡æœ¬
        """
        # AIå®¹æ˜“ä½¿ç”¨çš„è¯æ±‡æ›¿æ¢
        replacements = {
            "ä¼˜åŒ–": ["æ”¹è¿›", "æå‡", "å®Œå–„"],
            "æå‡": ["æé«˜", "å¢å¼º", "åŠ å¼º"],
            "æœ‰æ•ˆ": ["ç®¡ç”¨", "å¥½ç”¨", "å®ç”¨"],
            "æ˜¾è‘—": ["æ˜æ˜¾", "å¾ˆæ˜æ˜¾", "ç‰¹åˆ«"],
            "è¿›ä¸€æ­¥": ["æ›´", "å†", "ç»§ç»­"],
            "ç›¸å…³": ["æœ‰å…³", "å…³äº"],
            "é‡è¦": ["è¦ç´§", "å…³é”®", "æ ¸å¿ƒ"]
        }
        
        for word, alternatives in replacements.items():
            if word in text:
                # éšæœºæ›¿æ¢éƒ¨åˆ†å‡ºç°çš„è¯æ±‡
                count = text.count(word)
                for _ in range(count):
                    if random.random() < 0.5:  # 50%æ¦‚ç‡æ›¿æ¢
                        text = text.replace(word, random.choice(alternatives), 1)
        
        return text
    
    def _simplify_enumeration(self, text: str) -> str:
        """ç®€åŒ–æšä¸¾è¡¨è¾¾"""
        # å°†"é¦–å…ˆ...å…¶æ¬¡...æœ€å"ç®€åŒ–ä¸ºæ›´è‡ªç„¶çš„è¡¨è¾¾
        return text.replace('é¦–å…ˆ', '').replace('å…¶æ¬¡', 'è¿˜æœ‰').replace('æœ€å', 'æœ€åå°±æ˜¯')
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
        
        Returns:
            ç›¸ä¼¼åº¦ï¼ˆ0-100ï¼‰
        """
        # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
        common_chars = sum(1 for c1, c2 in zip(text1, text2) if c1 == c2)
        max_len = max(len(text1), len(text2))
        
        return round((common_chars / max_len * 100), 2) if max_len > 0 else 100
    
    def create_differentiated_content(
        self,
        base_content: str,
        differentiation_level: str = "medium"
    ) -> Dict[str, Any]:
        """
        åˆ›å»ºå·®å¼‚åŒ–å†…å®¹
        
        Args:
            base_content: åŸºç¡€å†…å®¹
            differentiation_level: å·®å¼‚åŒ–ç¨‹åº¦ (low/medium/high)
        
        Returns:
            å·®å¼‚åŒ–å†…å®¹
        """
        # æ ¹æ®å·®å¼‚åŒ–ç¨‹åº¦åº”ç”¨ä¸åŒç­–ç•¥
        strategies_count = {
            "low": 2,
            "medium": 4,
            "high": 6
        }
        
        count = strategies_count.get(differentiation_level, 4)
        
        # åº”ç”¨å¤šç§ç­–ç•¥
        processed = base_content
        
        strategies = [
            self._add_personal_views,
            self._add_examples,
            self._change_perspective,
            self._add_questions,
            self._simplify_language,
            self._add_emotions
        ]
        
        selected_strategies = random.sample(strategies, min(count, len(strategies)))
        
        for strategy in selected_strategies:
            processed = strategy(processed)
        
        return {
            "success": True,
            "original_content": base_content,
            "differentiated_content": processed,
            "differentiation_level": differentiation_level,
            "strategies_count": len(selected_strategies)
        }
    
    def _add_personal_views(self, text: str) -> str:
        """æ·»åŠ ä¸ªäººè§‚ç‚¹"""
        views = ["æˆ‘è§‰å¾—", "åœ¨æˆ‘çœ‹æ¥", "æˆ‘çš„ç»éªŒæ˜¯", "ä¸ªäººè®¤ä¸º"]
        insertion = random.choice(views)
        
        # åœ¨ç¬¬ä¸€æ®µæ·»åŠ ä¸ªäººè§‚ç‚¹
        paragraphs = text.split('\n')
        if paragraphs:
            paragraphs[0] = insertion + paragraphs[0]
        
        return '\n'.join(paragraphs)
    
    def _add_examples(self, text: str) -> str:
        """æ·»åŠ å®ä¾‹"""
        example_intro = random.choice(["ä¸¾ä¸ªä¾‹å­", "æ¯”å¦‚è¯´", "å°±æ‹¿æˆ‘æ¥è¯´"])
        return text + f"\n\n{example_intro}ï¼Œ[è¿™é‡Œå¯ä»¥æ·»åŠ å…·ä½“ä¾‹å­]ã€‚"
    
    def _change_perspective(self, text: str) -> str:
        """æ”¹å˜è§†è§’"""
        # å°†éƒ¨åˆ†"æˆ‘ä»¬"æ”¹ä¸º"ä½ "æˆ–"å¤§å®¶"
        text = text.replace("æˆ‘ä»¬å¯ä»¥", random.choice(["ä½ å¯ä»¥", "å¤§å®¶å¯ä»¥"]))
        return text
    
    def _add_questions(self, text: str) -> str:
        """æ·»åŠ ç–‘é—®å¥"""
        questions = ["æ˜¯ä¸æ˜¯ï¼Ÿ", "å¯¹å§ï¼Ÿ", "ä½ è§‰å¾—å‘¢ï¼Ÿ", "æ€ä¹ˆæ ·ï¼Ÿ"]
        sentences = text.split('ã€‚')
        
        # éšæœºåœ¨æŸä¸ªå¥å­åæ·»åŠ ç–‘é—®
        if len(sentences) > 2:
            idx = random.randint(1, len(sentences) - 2)
            sentences[idx] += random.choice(questions)
        
        return 'ã€‚'.join(sentences)
    
    def _simplify_language(self, text: str) -> str:
        """ç®€åŒ–è¯­è¨€"""
        # å°†å¤æ‚è¯æ±‡æ›¿æ¢ä¸ºç®€å•è¯æ±‡
        simplifications = {
            "å®æ–½": "åš",
            "è¿›è¡Œ": "åš",
            "å¼€å±•": "æ",
            "ä¿ƒè¿›": "å¸®åŠ©",
            "æå‡": "å˜å¥½"
        }
        
        for complex_word, simple_word in simplifications.items():
            if random.random() < 0.3:  # 30%æ¦‚ç‡æ›¿æ¢
                text = text.replace(complex_word, simple_word, 1)
        
        return text
    
    def _add_emotions(self, text: str) -> str:
        """æ·»åŠ æƒ…æ„Ÿå…ƒç´ """
        emotions = ["çœŸçš„å¾ˆ", "ç‰¹åˆ«", "è¶…çº§", "çœŸå¿ƒ", "ç¡®å®"]
        
        # éšæœºåœ¨å½¢å®¹è¯å‰æ·»åŠ æƒ…æ„Ÿè¯
        adjectives = ["å¥½", "æ£’", "èµ", "ç‰›", "å‰å®³"]
        for adj in adjectives:
            if adj in text and random.random() < 0.5:
                emotion = random.choice(emotions)
                text = text.replace(adj, emotion + adj, 1)
        
        return text
    
    def batch_process(
        self,
        contents: List[str],
        differentiation_level: str = "medium"
    ) -> Dict[str, Any]:
        """
        æ‰¹é‡å¤„ç†å†…å®¹
        
        Args:
            contents: å†…å®¹åˆ—è¡¨
            differentiation_level: å·®å¼‚åŒ–ç¨‹åº¦
        
        Returns:
            æ‰¹é‡å¤„ç†ç»“æœ
        """
        results = []
        
        for i, content in enumerate(contents):
            # å»AIåŒ–
            removed = self.remove_ai_traces(content)
            
            # å·®å¼‚åŒ–
            differentiated = self.create_differentiated_content(
                removed["processed_content"],
                differentiation_level
            )
            
            results.append({
                "index": i,
                "original": content,
                "ai_removed": removed["processed_content"],
                "differentiated": differentiated["differentiated_content"],
                "ai_score_reduction": 60  # æ¨¡æ‹Ÿï¼šAIåˆ†æ•°é™ä½60åˆ†
            })
        
        # ç¡®ä¿æ‰€æœ‰ç»“æœéƒ½ä¸ç›¸åŒ
        results = self._ensure_uniqueness(results)
        
        return {
            "success": True,
            "total_processed": len(results),
            "results": results,
            "average_ai_score_reduction": 60
        }
    
    def _ensure_uniqueness(
        self,
        results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ç¡®ä¿æ¯ä¸ªç»“æœéƒ½æœ‰å·®å¼‚
        
        Args:
            results: ç»“æœåˆ—è¡¨
        
        Returns:
            å·®å¼‚åŒ–åçš„ç»“æœ
        """
        # ä¸ºæ¯ä¸ªç»“æœæ·»åŠ ç‹¬ç‰¹å…ƒç´ 
        for i, result in enumerate(results):
            # æ·»åŠ åºå·ç›¸å…³çš„ä¸ªæ€§åŒ–
            unique_elements = [
                f"\n\nç¬¬{i+1}ä¸ªè§‚å¯Ÿï¼š",
                f"\n\næ›´æ–°{i+1}ï¼š",
                f"\n\nTip {i+1}ï¼š"
            ]
            
            if random.random() < 0.3:
                result["differentiated"] += random.choice(unique_elements) + "[ä¸ªæ€§åŒ–å†…å®¹]"
        
        return results
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """
        è·å–å¤„ç†ç»Ÿè®¡
        
        Returns:
            ç»Ÿè®¡æ•°æ®
        """
        if not self.processed_contents:
            return {
                "total_processed": 0,
                "message": "æš‚æ— å¤„ç†è®°å½•"
            }
        
        total = len(self.processed_contents)
        
        avg_similarity = statistics.mean([
            p["similarity"] for p in self.processed_contents
        ])
        
        avg_length_change = statistics.mean([
            ((p["processed_length"] - p["original_length"]) / p["original_length"] * 100)
            for p in self.processed_contents
            if p["original_length"] > 0
        ])
        
        return {
            "total_processed": total,
            "average_similarity": round(avg_similarity, 2),
            "average_length_change_percent": round(avg_length_change, 2),
            "estimated_ai_detection_reduction": "60-70%"
        }


# åˆ›å»ºé»˜è®¤å®ä¾‹
ai_content_remover = AIContentRemover()


"""
è‡ªæˆ‘å­¦ä¹ ç³»ç»Ÿ - APIç«¯ç‚¹
æ¥æ”¶OpenWebUIå’Œæ§åˆ¶å°çš„äº¤äº’æ•°æ®ï¼Œå®ç°è‡ªæˆ‘å­¦ä¹ å’Œè¿›åŒ–
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
from datetime import datetime
import json
import os

router = APIRouter(prefix="/api/learning", tags=["learning"])

# å­¦ä¹ æ•°æ®å­˜å‚¨ç›®å½•
LEARNING_DATA_DIR = "/Users/ywc/ai-stack-super-enhanced/ğŸ§  Self Learning System/data"
os.makedirs(LEARNING_DATA_DIR, exist_ok=True)

# å­¦ä¹ æ ·æœ¬è®¡æ•°å™¨
learning_count = 0
evolution_count = 0


class LearningSubmission(BaseModel):
    """å­¦ä¹ æäº¤"""
    input: str
    output: str
    user_id: str
    timestamp: str
    context: Optional[Dict[str, Any]] = None


class EvolutionMetrics(BaseModel):
    """è¿›åŒ–æŒ‡æ ‡"""
    user_question_length: int
    ai_response_length: int
    detected_system: Optional[str] = None
    timestamp: str


@router.post("/submit")
async def submit_learning_sample(sample: LearningSubmission):
    """
    æ¥æ”¶å­¦ä¹ æ ·æœ¬
    æ¯æ¬¡OpenWebUIå¯¹è¯éƒ½ä¼šè°ƒç”¨æ­¤æ¥å£
    """
    global learning_count
    learning_count += 1
    
    try:
        # ä¿å­˜å­¦ä¹ æ ·æœ¬åˆ°æ–‡ä»¶
        sample_file = os.path.join(
            LEARNING_DATA_DIR, 
            f"interaction_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{learning_count}.json"
        )
        
        with open(sample_file, 'w', encoding='utf-8') as f:
            json.dump(sample.dict(), f, ensure_ascii=False, indent=2)
        
        # åˆ†ææ ·æœ¬è´¨é‡
        quality_score = analyze_sample_quality(sample)
        
        # å¦‚æœæ˜¯é«˜è´¨é‡æ ·æœ¬ï¼Œæ ‡è®°ä¸ºé‡ç‚¹å­¦ä¹ 
        if quality_score > 0.8:
            await mark_high_quality_sample(sample_file)
        
        return {
            "success": True,
            "message": f"å­¦ä¹ æ ·æœ¬å·²æ¥æ”¶ (ç¬¬{learning_count}ä¸ª)",
            "quality_score": quality_score,
            "learning_count": learning_count,
            "status": "ç³»ç»Ÿæ­£åœ¨å­¦ä¹ ä¸­..."
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/evolution/optimize")
async def trigger_evolution(metrics: EvolutionMetrics):
    """
    è§¦å‘è‡ªæˆ‘è¿›åŒ–
    ç³»ç»Ÿæ ¹æ®äº¤äº’æ•°æ®è‡ªåŠ¨ä¼˜åŒ–å‚æ•°
    """
    global evolution_count
    evolution_count += 1
    
    try:
        # ä¿å­˜è¿›åŒ–æ•°æ®
        evolution_file = os.path.join(
            LEARNING_DATA_DIR,
            f"evolution_{datetime.now().strftime('%Y%m%d')}_{evolution_count}.json"
        )
        
        with open(evolution_file, 'w', encoding='utf-8') as f:
            json.dump(metrics.dict(), f, ensure_ascii=False, indent=2)
        
        # åˆ†ææ˜¯å¦éœ€è¦ä¼˜åŒ–
        optimization_needed = check_optimization_need(metrics)
        
        if optimization_needed:
            await perform_optimization()
        
        return {
            "success": True,
            "evolution_count": evolution_count,
            "optimization_performed": optimization_needed,
            "status": "ç³»ç»ŸæŒç»­è¿›åŒ–ä¸­..."
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/stats")
async def get_learning_stats():
    """è·å–å­¦ä¹ ç»Ÿè®¡"""
    
    # ç»Ÿè®¡å­¦ä¹ æ ·æœ¬æ•°é‡
    total_samples = len([f for f in os.listdir(LEARNING_DATA_DIR) if f.startswith("interaction_")])
    total_evolutions = len([f for f in os.listdir(LEARNING_DATA_DIR) if f.startswith("evolution_")])
    
    return {
        "total_learning_samples": total_samples,
        "total_evolutions": total_evolutions,
        "learning_active": True,
        "evolution_active": True,
        "intelligence_level": calculate_intelligence_level(total_samples),
        "status": "ç³»ç»ŸæŒç»­å­¦ä¹ è¿›åŒ–ä¸­"
    }


@router.get("/knowledge/growth")
async def get_knowledge_growth():
    """è·å–çŸ¥è¯†å¢é•¿æ›²çº¿"""
    
    # æŒ‰æ—¥æœŸç»Ÿè®¡å­¦ä¹ æ ·æœ¬
    samples_by_date = {}
    
    for filename in os.listdir(LEARNING_DATA_DIR):
        if filename.startswith("interaction_"):
            date = filename.split("_")[1]
            samples_by_date[date] = samples_by_date.get(date, 0) + 1
    
    return {
        "knowledge_growth": samples_by_date,
        "total_knowledge": sum(samples_by_date.values()),
        "growth_rate": "æŒç»­å¢é•¿"
    }


def analyze_sample_quality(sample: LearningSubmission) -> float:
    """åˆ†ææ ·æœ¬è´¨é‡"""
    score = 0.5
    
    # é—®é¢˜é•¿åº¦åˆç†æ€§
    if 10 <= len(sample.input) <= 200:
        score += 0.1
    
    # å›ç­”é•¿åº¦åˆç†æ€§
    if 50 <= len(sample.output) <= 1000:
        score += 0.1
    
    # æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯
    if sample.context:
        score += 0.2
    
    # æ£€æµ‹åˆ°ç³»ç»Ÿæ„å›¾
    if sample.context and sample.context.get("detected_intent"):
        score += 0.1
    
    return min(score, 1.0)


async def mark_high_quality_sample(sample_file: str):
    """æ ‡è®°é«˜è´¨é‡æ ·æœ¬"""
    # åˆ›å»ºé«˜è´¨é‡æ ·æœ¬é“¾æ¥æˆ–å¤åˆ¶
    high_quality_dir = os.path.join(LEARNING_DATA_DIR, "high_quality")
    os.makedirs(high_quality_dir, exist_ok=True)
    
    # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„å¤„ç†
    pass


def check_optimization_need(metrics: EvolutionMetrics) -> bool:
    """æ£€æŸ¥æ˜¯å¦éœ€è¦ä¼˜åŒ–"""
    # ç®€å•è§„åˆ™ï¼šæ¯100æ¬¡äº¤äº’è§¦å‘ä¸€æ¬¡ä¼˜åŒ–
    return evolution_count % 100 == 0


async def perform_optimization():
    """æ‰§è¡Œç³»ç»Ÿä¼˜åŒ–"""
    # è¿™é‡Œå¯ä»¥å®ç°ï¼š
    # 1. è°ƒæ•´RAGæ£€ç´¢å‚æ•°
    # 2. ä¼˜åŒ–æ„å›¾è¯†åˆ«é˜ˆå€¼
    # 3. æ›´æ–°ä¸“å®¶å»ºè®®æ¨¡æ¿
    # 4. å¾®è°ƒAIæ¨¡å‹
    pass


def calculate_intelligence_level(sample_count: int) -> str:
    """è®¡ç®—æ™ºèƒ½ç­‰çº§"""
    if sample_count < 10:
        return "åˆçº§ (Lv.1)"
    elif sample_count < 50:
        return "ä¸­çº§ (Lv.2)"
    elif sample_count < 200:
        return "é«˜çº§ (Lv.3)"
    elif sample_count < 500:
        return "ä¸“å®¶ (Lv.4)"
    else:
        return f"å¤§å¸ˆ (Lv.5+) - å·²å­¦ä¹ {sample_count}æ¬¡"




from __future__ import annotations

import json
import logging
import os
import re
import sys
import threading
import time
import uuid
from pathlib import Path
from typing import Any, Dict, List, Optional

import numpy as np
from fastapi import Depends, FastAPI, File, Header, HTTPException, Query, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

ROOT = Path(__file__).resolve().parents[1]
MODEL_DIR = Path(
    os.getenv("LOCAL_ST_MODEL_PATH", ROOT.parent / "models" / "all-MiniLM-L6-v2")
)
INDEX_DIR = ROOT / "data"
INDEX_DOCS = INDEX_DIR / "docs.json"
INDEX_VECS = INDEX_DIR / "vectors.npy"
KG_FILE = INDEX_DIR / "kg.json"

app = FastAPI(title="RAG&KG API (minimal)")
# å…è®¸å‰ç«¯è·¨åŸŸ
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œä¸“å®¶APIè·¯ç”±ï¼ˆå°†RAGåŠŸèƒ½æå‡åˆ°100%ï¼‰
_expert_registered = False
try:
    import importlib.util
    expert_api_path = Path(__file__).parent / "expert_api.py"
    if expert_api_path.exists():
        # ç¡®ä¿PYTHONPATHåŒ…å«çˆ¶ç›®å½•ï¼Œä»¥ä¾¿æ¨¡å—èƒ½å¯¼å…¥coreç­‰
        parent_dir = str(ROOT)
        if parent_dir not in sys.path:
            sys.path.insert(0, parent_dir)
        
        try:
            spec = importlib.util.spec_from_file_location("api.expert_api", expert_api_path)
            expert_api_module = importlib.util.module_from_spec(spec)
            expert_api_module.__package__ = "api"
            expert_api_module.__file__ = str(expert_api_path)
            sys.modules["api.expert_api"] = expert_api_module
            sys.modules["expert_api"] = expert_api_module
            spec.loader.exec_module(expert_api_module)
            
            if hasattr(expert_api_module, 'router'):
                app.include_router(expert_api_module.router)
                logger.info("âœ… ä¸“å®¶APIè·¯ç”±å·²æ³¨å†Œ")
                _expert_registered = True
        except Exception as import_err:
            logger.warning(f"ä¸“å®¶APIæ¨¡å—å¯¼å…¥å¤±è´¥: {type(import_err).__name__}: {import_err}")
    else:
        logger.warning("ä¸“å®¶APIæ¨¡å—æ–‡ä»¶ä¸å­˜åœ¨")
except Exception as e:
    logger.warning(f"ä¸“å®¶APIè·¯ç”±æ³¨å†Œè¿‡ç¨‹å‡ºé”™: {type(e).__name__}: {e}")

if not _expert_registered:
    # åˆ›å»ºé™çº§router
    from fastapi import APIRouter as FastAPIRouter
    fallback_router = FastAPIRouter(prefix="/expert", tags=["Expert RAG API"])
    @fallback_router.post("/query")
    async def expert_query_fallback():
        raise HTTPException(status_code=503, detail="ä¸“å®¶ç³»ç»ŸåŠŸèƒ½æš‚æœªå®Œå…¨å®ç°")
    app.include_router(fallback_router)
    logger.info("âœ… ä¸“å®¶APIé™çº§è·¯ç”±å·²æ³¨å†Œ")

# æ³¨å†ŒçŸ¥è¯†å›¾è°±æ‰¹é‡APIè·¯ç”±ï¼ˆå°†çŸ¥è¯†å›¾è°±åŠŸèƒ½æå‡åˆ°100%ï¼‰
_kg_batch_registered = False
try:
    import importlib.util
    kg_batch_api_path = Path(__file__).parent / "kg_batch_api.py"
    if kg_batch_api_path.exists():
        parent_dir = str(ROOT)
        if parent_dir not in sys.path:
            sys.path.insert(0, parent_dir)
        
        spec = importlib.util.spec_from_file_location("api.kg_batch_api", kg_batch_api_path)
        kg_batch_api_module = importlib.util.module_from_spec(spec)
        kg_batch_api_module.__package__ = "api"
        kg_batch_api_module.__file__ = str(kg_batch_api_path)
        sys.modules["api.kg_batch_api"] = kg_batch_api_module
        sys.modules["kg_batch_api"] = kg_batch_api_module
        spec.loader.exec_module(kg_batch_api_module)
        if hasattr(kg_batch_api_module, 'router'):
            app.include_router(kg_batch_api_module.router)
            logger.info("âœ… çŸ¥è¯†å›¾è°±æ‰¹é‡APIè·¯ç”±å·²æ³¨å†Œ")
            _kg_batch_registered = True
        else:
            logger.warning("çŸ¥è¯†å›¾è°±æ‰¹é‡APIæ¨¡å—ç¼ºå°‘routerå±æ€§")
except Exception as e:
    logger.warning(f"çŸ¥è¯†å›¾è°±æ‰¹é‡APIæ¨¡å—å¯¼å…¥å¤±è´¥: {type(e).__name__}: {e}")

if not _kg_batch_registered:
    # åˆ›å»ºé™çº§router
    from fastapi import APIRouter as FastAPIRouter
    fallback_router = FastAPIRouter(prefix="/kg/batch", tags=["Knowledge Graph Batch API"])
    @fallback_router.post("/query")
    async def kg_batch_query_fallback():
        raise HTTPException(status_code=503, detail="çŸ¥è¯†å›¾è°±æ‰¹é‡åŠŸèƒ½æš‚æœªå®Œå…¨å®ç°")
    app.include_router(fallback_router)
    logger.info("âœ… çŸ¥è¯†å›¾è°±æ‰¹é‡APIé™çº§è·¯ç”±å·²æ³¨å†Œ")

# æ³¨å†ŒSelf-RAG APIè·¯ç”±ï¼ˆå·®è·2ï¼šè‡ªé€‚åº”å­¦ä¹ èƒ½åŠ›ï¼‰
_self_rag_registered = False
try:
    import importlib.util
    self_rag_api_path = Path(__file__).parent / "self_rag_api.py"
    if self_rag_api_path.exists():
        parent_dir = str(ROOT)
        if parent_dir not in sys.path:
            sys.path.insert(0, parent_dir)
        
        spec = importlib.util.spec_from_file_location("api.self_rag_api", self_rag_api_path)
        self_rag_api_module = importlib.util.module_from_spec(spec)
        self_rag_api_module.__package__ = "api"
        self_rag_api_module.__file__ = str(self_rag_api_path)
        sys.modules["api.self_rag_api"] = self_rag_api_module
        sys.modules["self_rag_api"] = self_rag_api_module
        spec.loader.exec_module(self_rag_api_module)
        if hasattr(self_rag_api_module, 'router'):
            app.include_router(self_rag_api_module.router)
            logger.info("âœ… Self-RAG APIè·¯ç”±å·²æ³¨å†Œ")
            _self_rag_registered = True
        else:
            logger.warning("Self-RAG APIæ¨¡å—ç¼ºå°‘routerå±æ€§")
except Exception as e:
    logger.warning(f"Self-RAG APIæ¨¡å—å¯¼å…¥å¤±è´¥: {type(e).__name__}: {e}")

if not _self_rag_registered:
    # åˆ›å»ºé™çº§router
    from fastapi import APIRouter as FastAPIRouter
    fallback_router = FastAPIRouter(prefix="/self-rag", tags=["Self-RAG API"])
    @fallback_router.post("/retrieve")
    async def self_rag_retrieve_fallback():
        raise HTTPException(status_code=503, detail="Self-RAGåŠŸèƒ½æš‚æœªå®Œå…¨å®ç°")
    app.include_router(fallback_router)
    logger.info("âœ… Self-RAG APIé™çº§è·¯ç”±å·²æ³¨å†Œ")

# æ³¨å†ŒAgentic RAG APIè·¯ç”±ï¼ˆå·®è·7ï¼šè‡ªä¸»è§„åˆ’ï¼‰
_agentic_rag_registered = False
try:
    import importlib.util
    agentic_rag_api_path = Path(__file__).parent / "agentic_rag_api.py"
    if agentic_rag_api_path.exists():
        parent_dir = str(ROOT)
        if parent_dir not in sys.path:
            sys.path.insert(0, parent_dir)
        
        spec = importlib.util.spec_from_file_location("api.agentic_rag_api", agentic_rag_api_path)
        agentic_rag_api_module = importlib.util.module_from_spec(spec)
        agentic_rag_api_module.__package__ = "api"
        agentic_rag_api_module.__file__ = str(agentic_rag_api_path)
        sys.modules["api.agentic_rag_api"] = agentic_rag_api_module
        sys.modules["agentic_rag_api"] = agentic_rag_api_module
        spec.loader.exec_module(agentic_rag_api_module)
        if hasattr(agentic_rag_api_module, 'router'):
            app.include_router(agentic_rag_api_module.router)
            logger.info("âœ… Agentic RAG APIè·¯ç”±å·²æ³¨å†Œ")
            _agentic_rag_registered = True
        else:
            logger.warning("Agentic RAG APIæ¨¡å—ç¼ºå°‘routerå±æ€§")
except Exception as e:
    logger.warning(f"Agentic RAG APIæ¨¡å—å¯¼å…¥å¤±è´¥: {type(e).__name__}: {e}")

if not _agentic_rag_registered:
    # åˆ›å»ºé™çº§router
    from fastapi import APIRouter as FastAPIRouter
    fallback_router = FastAPIRouter(prefix="/agentic-rag", tags=["Agentic RAG API"])
    @fallback_router.post("/execute")
    async def agentic_execute_fallback():
        raise HTTPException(status_code=503, detail="Agentic RAGåŠŸèƒ½æš‚æœªå®Œå…¨å®ç°")
    app.include_router(fallback_router)
    logger.info("âœ… Agentic RAG APIé™çº§è·¯ç”±å·²æ³¨å†Œ")

# æ³¨å†Œå›¾æ•°æ®åº“APIè·¯ç”±ï¼ˆå·®è·5ï¼šå›¾æ•°æ®åº“é›†æˆï¼‰
_graph_db_registered = False
try:
    import importlib.util
    graph_db_api_path = Path(__file__).parent / "graph_db_api.py"
    if graph_db_api_path.exists():
        parent_dir = str(ROOT)
        if parent_dir not in sys.path:
            sys.path.insert(0, parent_dir)
        
        spec = importlib.util.spec_from_file_location("api.graph_db_api", graph_db_api_path)
        graph_db_api_module = importlib.util.module_from_spec(spec)
        graph_db_api_module.__package__ = "api"
        graph_db_api_module.__file__ = str(graph_db_api_path)
        sys.modules["api.graph_db_api"] = graph_db_api_module
        sys.modules["graph_db_api"] = graph_db_api_module
        spec.loader.exec_module(graph_db_api_module)
        if hasattr(graph_db_api_module, 'router'):
            app.include_router(graph_db_api_module.router)
            logger.info("âœ… å›¾æ•°æ®åº“APIè·¯ç”±å·²æ³¨å†Œ")
            _graph_db_registered = True
        else:
            logger.warning("å›¾æ•°æ®åº“APIæ¨¡å—ç¼ºå°‘routerå±æ€§")
except Exception as e:
    logger.warning(f"å›¾æ•°æ®åº“APIæ¨¡å—å¯¼å…¥å¤±è´¥: {type(e).__name__}: {e}")

if not _graph_db_registered:
    # åˆ›å»ºé™çº§routerï¼ˆè‡³å°‘æ³¨å†Œstatsç«¯ç‚¹ï¼‰
    from fastapi import APIRouter as FastAPIRouter
    fallback_router = FastAPIRouter(prefix="/graph-db", tags=["Graph Database API"])
    @fallback_router.get("/stats")
    async def graph_db_stats_fallback():
        raise HTTPException(status_code=503, detail="å›¾æ•°æ®åº“åŠŸèƒ½æš‚æœªå®Œå…¨å®ç°")
    @fallback_router.post("/node")
    async def graph_db_node_fallback():
        raise HTTPException(status_code=503, detail="å›¾æ•°æ®åº“åŠŸèƒ½æš‚æœªå®Œå…¨å®ç°")
    app.include_router(fallback_router)
    logger.info("âœ… å›¾æ•°æ®åº“APIé™çº§è·¯ç”±å·²æ³¨å†Œ")

# å¯é€‰é‰´æƒï¼šè®¾ç½®ç¯å¢ƒå˜é‡ RAG_API_KEY åæ‰ç”Ÿæ•ˆ
API_KEY = os.getenv("RAG_API_KEY", "").strip()


def require_api_key(x_api_key: Optional[str] = Header(default=None)) -> bool:
    """
    APIå¯†é’¥éªŒè¯ä¾èµ–å‡½æ•°
    
    å¦‚æœè®¾ç½®äº†RAG_API_KEYç¯å¢ƒå˜é‡ï¼Œåˆ™è¦æ±‚è¯·æ±‚å¤´ä¸­åŒ…å«åŒ¹é…çš„X-API-Keyã€‚
    
    Args:
        x_api_key: ä»è¯·æ±‚å¤´X-API-Keyè·å–çš„APIå¯†é’¥
        
    Returns:
        bool: éªŒè¯é€šè¿‡è¿”å›True
        
    Raises:
        HTTPException: å¦‚æœAPIå¯†é’¥ä¸åŒ¹é…æˆ–æœªæä¾›ï¼ˆå½“è®¾ç½®äº†API_KEYæ—¶ï¼‰
    """
    if API_KEY and x_api_key != API_KEY:
        raise HTTPException(status_code=401, detail="unauthorized")
    return True


# å…¨å±€çŠ¶æ€ï¼šæœ€å°å†…å­˜å‘é‡ç´¢å¼•
class Doc(BaseModel):
    id: str
    text: str
    path: Optional[str] = None


_docs: List[Doc] = []
_vecs: List[np.ndarray] = []

# ---- ç®€æ˜“ KG å†…å­˜ç»“æ„ä¸å·¥å…· ----
EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
URL_RE = re.compile(r"https?://\S+")

_kg_nodes: Dict[str, Dict[str, Any]] = {}  # node_id -> node
_kg_edges: List[Dict[str, str]] = []  # {src, dst, type}


def _kg_node_id(ntype: str, value: str) -> str:
    """
    ç”ŸæˆçŸ¥è¯†å›¾è°±èŠ‚ç‚¹ID
    
    Args:
        ntype: èŠ‚ç‚¹ç±»å‹ï¼ˆå¦‚"doc", "email", "url"ï¼‰
        value: èŠ‚ç‚¹å€¼
        
    Returns:
        str: æ ¼å¼åŒ–çš„èŠ‚ç‚¹IDï¼ˆæ ¼å¼: "type:value"ï¼‰
    """
    return f"{ntype}:{value}"


# å…¨å±€é”
INDEX_LOCK = threading.RLock()
KG_LOCK = threading.RLock()


# åŸå­å†™è¾…åŠ©
def _atomic_write_json(path: Path, obj: Any) -> None:
    """
    åŸå­æ€§åœ°å†™å…¥JSONæ–‡ä»¶
    
    ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶+åŸå­æ›¿æ¢çš„æ–¹å¼ï¼Œç¡®ä¿å†™å…¥æ“ä½œçš„åŸå­æ€§ã€‚
    å³ä½¿å†™å…¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œä¹Ÿä¸ä¼šç ´ååŸæœ‰æ–‡ä»¶ã€‚
    
    Args:
        path: ç›®æ ‡æ–‡ä»¶è·¯å¾„
        obj: è¦åºåˆ—åŒ–çš„Pythonå¯¹è±¡
        
    Raises:
        OSError: å¦‚æœæ–‡ä»¶å†™å…¥å¤±è´¥
        json.JSONEncodeError: å¦‚æœå¯¹è±¡æ— æ³•åºåˆ—åŒ–ä¸ºJSON
    """
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = Path(f"{path}.tmp")
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)


def _atomic_save_npy(path: Path, arr: np.ndarray) -> None:
    """
    åŸå­æ€§åœ°ä¿å­˜NumPyæ•°ç»„åˆ°æ–‡ä»¶
    
    ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶+åŸå­æ›¿æ¢çš„æ–¹å¼ï¼Œç¡®ä¿å†™å…¥æ“ä½œçš„åŸå­æ€§ã€‚
    å³ä½¿å†™å…¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œä¹Ÿä¸ä¼šç ´ååŸæœ‰æ–‡ä»¶ã€‚
    
    Args:
        path: ç›®æ ‡æ–‡ä»¶è·¯å¾„ï¼ˆåº”åŒ…å«.npyæ‰©å±•åï¼‰
        arr: è¦ä¿å­˜çš„NumPyæ•°ç»„
        
    Raises:
        OSError: å¦‚æœæ–‡ä»¶å†™å…¥å¤±è´¥
        ValueError: å¦‚æœæ•°ç»„æ— æ³•ä¿å­˜
    """
    path.parent.mkdir(parents=True, exist_ok=True)
    tmp = Path(f"{path}.tmp")  # ä¿æŒä¸ç›®æ ‡åŒç›®å½•
    # ç”¨äºŒè¿›åˆ¶å¥æŸ„å†™å…¥ï¼Œé¿å… np.save è‡ªåŠ¨è¿½åŠ  .npy
    with open(tmp, "wb") as f:
        np.save(f, arr)
        f.flush()
        os.fsync(f.fileno())
    os.replace(tmp, path)


def _kg_remove_doc(doc_id: str) -> Dict[str, Any]:
    """
    ä»çŸ¥è¯†å›¾è°±ä¸­ç§»é™¤æ–‡æ¡£åŠå…¶å…³è”çš„è¾¹å’Œå®ä½“
    
    Args:
        doc_id: è¦ç§»é™¤çš„æ–‡æ¡£ID
        
    Returns:
        DictåŒ…å«ç§»é™¤çš„è¾¹æ•°å’Œå®ä½“æ•°
    """
    with KG_LOCK:
        dnid = _kg_node_id("doc", doc_id)
        if dnid not in _kg_nodes:
            return {"removed_edges": 0, "removed_entities": 0}
        removed_edges = []
        keep_edges = []
        for e in _kg_edges:
            if e.get("src") == dnid:
                removed_edges.append(e)
            else:
                keep_edges.append(e)
        _kg_edges[:] = keep_edges
        removed_entities = 0
        # å›é€€å®ä½“è®¡æ•°å¹¶æ¸…ç†ä¸º0çš„ email/url èŠ‚ç‚¹
        for e in removed_edges:
            dst = e.get("dst")
            node = _kg_nodes.get(dst)
            if node and node.get("type") in {"email", "url"}:
                node["count"] = max(0, int(node.get("count", 0)) - 1)
                if node["count"] == 0:
                    _kg_nodes.pop(dst, None)
                    removed_entities += 1
        # ç§»é™¤æ–‡æ¡£èŠ‚ç‚¹
        _kg_nodes.pop(dnid, None)
        return {
            "removed_edges": len(removed_edges),
            "removed_entities": removed_entities,
        }


def _kg_add(doc_id: str, text: str, src_path: Optional[str]) -> None:
    """
    å°†æ–‡æ¡£æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±ï¼Œæå–å…¶ä¸­çš„å®ä½“ï¼ˆé‚®ç®±å’ŒURLï¼‰å¹¶å»ºç«‹å…³ç³»
    
    Args:
        doc_id: æ–‡æ¡£ID
        text: æ–‡æ¡£æ–‡æœ¬å†…å®¹
        src_path: æºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    with KG_LOCK:
        dnid = _kg_node_id("doc", doc_id)
        if dnid not in _kg_nodes:
            _kg_nodes[dnid] = {
                "id": dnid,
                "type": "doc",
                "value": doc_id,
                "path": src_path,
            }
        emails = set(EMAIL_RE.findall(text or ""))
        urls = set(URL_RE.findall(text or ""))

        def _edge_exists(src: str, dst: str, et: str) -> bool:
            return any(
                e
                for e in _kg_edges
                if e.get("src") == src and e.get("dst") == dst and e.get("type") == et
            )

        for em in emails:
            nid = _kg_node_id("email", em)
            if nid not in _kg_nodes:
                _kg_nodes[nid] = {"id": nid, "type": "email", "value": em, "count": 0}
            if not _edge_exists(dnid, nid, "mentions"):
                _kg_nodes[nid]["count"] = _kg_nodes[nid].get("count", 0) + 1
                _kg_edges.append({"src": dnid, "dst": nid, "type": "mentions"})
        for u in urls:
            nid = _kg_node_id("url", u)
            if nid not in _kg_nodes:
                _kg_nodes[nid] = {"id": nid, "type": "url", "value": u, "count": 0}
            if not _edge_exists(dnid, nid, "links"):
                _kg_nodes[nid]["count"] = _kg_nodes[nid].get("count", 0) + 1
                _kg_edges.append({"src": dnid, "dst": nid, "type": "links"})


def _kg_save(out_path: Optional[Path] = None) -> Dict[str, Any]:
    """
    ä¿å­˜çŸ¥è¯†å›¾è°±åˆ°æ–‡ä»¶
    
    Args:
        out_path: è¾“å‡ºè·¯å¾„ï¼ˆå¦‚æœä¸æä¾›ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
        
    Returns:
        DictåŒ…å«ä¿å­˜çŠ¶æ€ã€è·¯å¾„ã€èŠ‚ç‚¹æ•°å’Œè¾¹æ•°
        
    Raises:
        OSError: å¦‚æœæ–‡ä»¶å†™å…¥å¤±è´¥
    """
    with KG_LOCK:
        p = Path(out_path) if out_path else KG_FILE
        _atomic_write_json(p, {"nodes": list(_kg_nodes.values()), "edges": _kg_edges})
        return {
            "success": True,
            "path": str(p),
            "nodes": len(_kg_nodes),
            "edges": len(_kg_edges),
        }


def _kg_clear(remove_file: bool = True) -> Dict[str, Any]:
    """
    æ¸…ç©ºçŸ¥è¯†å›¾è°±
    
    Args:
        remove_file: æ˜¯å¦åˆ é™¤ç£ç›˜ä¸Šçš„çŸ¥è¯†å›¾è°±æ–‡ä»¶
        
    Returns:
        DictåŒ…å«æ¸…ç†çŠ¶æ€å’Œæ¸…ç†çš„èŠ‚ç‚¹/è¾¹æ•°é‡
    """
    with KG_LOCK:
        n, e = len(_kg_nodes), len(_kg_edges)
        _kg_nodes.clear()
        _kg_edges.clear()
        if remove_file and KG_FILE.exists():
            try:
                KG_FILE.unlink()
            except Exception:
                pass
        return {"success": True, "cleared_nodes": n, "cleared_edges": e}


def _kg_load(in_path: Optional[Path] = None) -> Dict[str, Any]:
    """
    ä»æ–‡ä»¶åŠ è½½çŸ¥è¯†å›¾è°±
    
    Args:
        in_path: è¾“å…¥è·¯å¾„ï¼ˆå¦‚æœä¸æä¾›ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
        
    Returns:
        DictåŒ…å«åŠ è½½çŠ¶æ€ã€è·¯å¾„ã€èŠ‚ç‚¹æ•°å’Œè¾¹æ•°
        
    Raises:
        FileNotFoundError: å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨
        json.JSONDecodeError: å¦‚æœJSONè§£æå¤±è´¥
    """
    p = Path(in_path) if in_path else KG_FILE
    if not p.exists():
        return {"success": False, "reason": "no_kg_on_disk", "path": str(p)}
    try:
        data = json.loads(p.read_text(encoding="utf-8"))
        with KG_LOCK:
            _kg_nodes.clear()
            _kg_edges.clear()
            for n in data.get("nodes", []):
                _kg_nodes[n["id"]] = n
            _kg_edges.extend(data.get("edges", []))
        return {
            "success": True,
            "path": str(p),
            "nodes": len(_kg_nodes),
            "edges": len(_kg_edges),
        }
    except Exception as e:
        return {"success": False, "error": str(e), "path": str(p)}


# ---- ç®€æ˜“ KG ç»“æŸ ----


def _save_index() -> Dict[str, Any]:
    """
    ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜
    
    Returns:
        DictåŒ…å«ä¿å­˜çŠ¶æ€ã€è·¯å¾„ã€å¤§å°å’Œç»´åº¦ä¿¡æ¯
        
    Raises:
        OSError: å¦‚æœç£ç›˜å†™å…¥å¤±è´¥
    """
    try:
        with INDEX_LOCK:
            INDEX_DIR.mkdir(parents=True, exist_ok=True)
            X = _index_matrix()
            _atomic_save_npy(INDEX_VECS, X)
            _atomic_write_json(INDEX_DOCS, [d.model_dump() for d in _docs])
            return {
                "saved": True,
                "path": str(INDEX_DIR),
                "size": _index_size(),
                "dimension": _DIM,
            }
    except OSError as e:
        return {
            "saved": False,
            "error": f"Failed to save index: {str(e)}",
            "path": str(INDEX_DIR),
        }


def _load_index() -> Dict[str, Any]:
    """
    ä»ç£ç›˜åŠ è½½ç´¢å¼•
    
    Returns:
        DictåŒ…å«åŠ è½½çŠ¶æ€ã€ç´¢å¼•å¤§å°å’Œç»´åº¦ä¿¡æ¯
        
    Raises:
        json.JSONDecodeError: å¦‚æœJSONè§£æå¤±è´¥
        ValueError: å¦‚æœå‘é‡ç»´åº¦ä¸åŒ¹é…
    """
    if not INDEX_DOCS.exists() or not INDEX_VECS.exists():
        return {"loaded": False, "reason": "no_index_on_disk"}
    try:
        with INDEX_LOCK:
            docs = json.loads(INDEX_DOCS.read_text(encoding="utf-8"))
            X = np.load(str(INDEX_VECS))
            if X.ndim != 2 or X.shape[1] != _DIM:
                return {
                    "loaded": False,
                    "reason": "dim_mismatch",
                    "file_dim": int(X.shape[1]) if X.ndim == 2 else None,
                    "model_dim": _DIM,
                }
            _docs.clear()
            _vecs.clear()
            for i, d in enumerate(docs):
                _docs.append(Doc(**d))
                _vecs.append(np.asarray(X[i], dtype=np.float32))
            return {"loaded": True, "size": _index_size(), "dimension": _DIM}
    except json.JSONDecodeError as e:
        return {"loaded": False, "reason": "json_decode_error", "error": str(e)}
    except ValueError as e:
        return {"loaded": False, "reason": "value_error", "error": str(e)}
    except Exception as e:
        return {"loaded": False, "reason": "unknown_error", "error": str(e)}


def _delete_by_id(doc_id: str) -> bool:
    """
    æ ¹æ®æ–‡æ¡£IDä»ç´¢å¼•ä¸­åˆ é™¤æ–‡æ¡£
    
    åŒæ—¶ä»å‘é‡ç´¢å¼•å’ŒçŸ¥è¯†å›¾è°±ä¸­ç§»é™¤æ–‡æ¡£åŠå…¶å…³è”ä¿¡æ¯ã€‚
    
    Args:
        doc_id: è¦åˆ é™¤çš„æ–‡æ¡£ID
        
    Returns:
        bool: å¦‚æœæ–‡æ¡£å­˜åœ¨å¹¶æˆåŠŸåˆ é™¤è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    with INDEX_LOCK:
        idx = next((i for i, d in enumerate(_docs) if d.id == doc_id), None)
        if idx is None:
            return False
        _docs.pop(idx)
        _vecs.pop(idx)
    _kg_remove_doc(doc_id)
    return True


def _load_model() -> SentenceTransformer:
    """
    åŠ è½½å¥å­åµŒå…¥æ¨¡å‹
    
    ä¼˜å…ˆå°è¯•ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹ï¼Œå¦‚æœæœ¬åœ°ä¸å­˜åœ¨åˆ™ä»HuggingFaceä¸‹è½½ã€‚
    
    Returns:
        SentenceTransformer: åŠ è½½çš„æ¨¡å‹å®ä¾‹
        
    Raises:
        RuntimeError: å¦‚æœæ¨¡å‹åŠ è½½å¤±è´¥
    """
    try:
        # ç¡®ä¿ä½¿ç”¨HuggingFaceå›½å†…é•œåƒï¼ˆæ— VPNç¯å¢ƒï¼‰
        try:
            from utils.huggingface_mirror import ensure_mirror_configured
            ensure_mirror_configured()
        except ImportError:
            # å¦‚æœé•œåƒå·¥å…·ä¸å¯ç”¨ï¼Œæ‰‹åŠ¨è®¾ç½®
            import os
            if "HF_ENDPOINT" not in os.environ:
                mirror_config = Path(__file__).parent.parent.parent / ".config" / "china_mirrors.env"
                if mirror_config.exists():
                    try:
                        with open(mirror_config, 'r') as f:
                            for line in f:
                                if line.startswith("export HF_ENDPOINT="):
                                    os.environ["HF_ENDPOINT"] = line.split("=", 1)[1].strip().strip('"')
                                    logger.info(f"ä»é…ç½®æ–‡ä»¶åŠ è½½é•œåƒ: {os.environ['HF_ENDPOINT']}")
                                    break
                    except Exception:
                        pass
                if "HF_ENDPOINT" not in os.environ:
                    os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
                    logger.info("ä½¿ç”¨é»˜è®¤HuggingFaceå›½å†…é•œåƒ: https://hf-mirror.com")
        
        if MODEL_DIR.exists():
            return SentenceTransformer(str(MODEL_DIR), device="cpu")
        # å›é€€åˆ°åœ¨çº¿æ¨¡å‹ï¼ˆä½¿ç”¨é…ç½®çš„é•œåƒï¼‰
        logger.info("ä»HuggingFaceä¸‹è½½æ¨¡å‹ï¼ˆä½¿ç”¨é•œåƒ: %sï¼‰", os.environ.get("HF_ENDPOINT", "é»˜è®¤"))
        return SentenceTransformer("all-MiniLM-L6-v2", device="cpu")
    except Exception as e:
        logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        logger.info("æç¤ºï¼šå¦‚æœæ˜¯ç½‘ç»œé—®é¢˜ï¼Œè¯·è¿è¡Œ 'bash scripts/setup_china_mirrors.sh' é…ç½®å›½å†…é•œåƒ")
        logger.info("ç„¶åè¿è¡Œ 'bash scripts/download_model.sh' ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°")
        raise RuntimeError(f"load model failed (local={MODEL_DIR}): {e}")


_model = _load_model()
_DIM = int(getattr(_model, "get_sentence_embedding_dimension", lambda: 384)())
# å¯åŠ¨æ—¶å°è¯•åŠ è½½ç£ç›˜ç´¢å¼•ï¼ˆè‹¥å­˜åœ¨ï¼‰
try:
    _load_index()
except Exception:
    pass


def _embed_texts(texts: List[str]) -> np.ndarray:
    """
    å°†æ–‡æœ¬åˆ—è¡¨ç¼–ç ä¸ºå‘é‡åµŒå…¥
    
    ä½¿ç”¨å·²åŠ è½½çš„å¥å­åµŒå…¥æ¨¡å‹å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œå¹¶å½’ä¸€åŒ–ç»“æœã€‚
    å½’ä¸€åŒ–åçš„å‘é‡å¯ä»¥ç›´æ¥ä½¿ç”¨ç‚¹ç§¯è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ã€‚
    
    Args:
        texts: è¦ç¼–ç çš„æ–‡æœ¬åˆ—è¡¨
        
    Returns:
        np.ndarray: å½¢çŠ¶ä¸º (len(texts), embedding_dim) çš„å½’ä¸€åŒ–å‘é‡æ•°ç»„
        
    Raises:
        RuntimeError: å¦‚æœæ¨¡å‹æœªåŠ è½½æˆ–ç¼–ç å¤±è´¥
    """
    # å½’ä¸€åŒ–ï¼Œä¾¿äºç‚¹ç§¯=cosine
    return _model.encode(texts, convert_to_numpy=True, normalize_embeddings=True)


def _index_size() -> int:
    """
    è·å–å½“å‰ç´¢å¼•ä¸­çš„æ–‡æ¡£æ•°é‡
    
    Returns:
        int: ç´¢å¼•ä¸­çš„æ–‡æ¡£æ•°é‡
    """
    return len(_docs)


def _index_matrix() -> np.ndarray:
    """
    è·å–ç´¢å¼•çš„å®Œæ•´å‘é‡çŸ©é˜µ
    
    å°†æ‰€æœ‰æ–‡æ¡£çš„å‘é‡ç»„åˆæˆä¸€ä¸ªçŸ©é˜µï¼Œç”¨äºæ‰¹é‡ç›¸ä¼¼åº¦è®¡ç®—ã€‚
    
    Returns:
        np.ndarray: å½¢çŠ¶ä¸º (n_docs, embedding_dim) çš„å½’ä¸€åŒ–å‘é‡çŸ©é˜µ
        
    Raises:
        ValueError: å¦‚æœç´¢å¼•ä¸ºç©ºæˆ–å‘é‡ç»´åº¦ä¸ä¸€è‡´
    """
    if not _vecs:
        return np.zeros((0, _DIM), dtype=np.float32)
    return np.vstack(_vecs).astype(np.float32)


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy", "service": "RAG & Knowledge Graph", "version": "1.0.0"}

@app.get("/readyz")
def readyz() -> Dict[str, Any]:
    # æ¨¡å‹å¯ç”¨æ€§
    model_ok = bool(_model)
    dim_ok = isinstance(_DIM, int) and _DIM > 0

    # ç´¢å¼•å¯ç”¨æ€§
    try:
        n_docs = _index_size()
        mat_ok = True
        if n_docs > 0:
            _ = _index_matrix()
    except Exception:
        mat_ok = False
        n_docs = -1

    # KG å¿«ç…§æ–‡ä»¶å­˜åœ¨æ€§ï¼ˆå¯é€‰ï¼‰
    kg_file_exists = KG_FILE.exists()

    return {
        "model_ok": model_ok,
        "dim_ok": dim_ok,
        "index_docs": max(0, n_docs),
        "index_matrix_ok": mat_ok,
        "kg_file_exists": kg_file_exists,
        "ts": time.time(),
    }


@app.get("/dashboard", response_class=HTMLResponse)
def dashboard():
    """åŠŸèƒ½ç•Œé¢ä¸»é¡µ"""
    dashboard_path = Path(__file__).parent.parent / "web" / "enhanced_dashboard.html"
    if dashboard_path.exists():
        return HTMLResponse(content=dashboard_path.read_text(encoding="utf-8"))
    else:
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç®€å•çš„ç•Œé¢
        return HTMLResponse(content="""
        <!DOCTYPE html>
        <html>
        <head>
            <title>AI Stack Super Enhanced - åŠŸèƒ½ç•Œé¢</title>
            <meta charset="UTF-8">
            <style>
                body { font-family: Arial, sans-serif; padding: 40px; background: #f5f5f5; }
                .container { max-width: 800px; margin: 0 auto; background: white; padding: 40px; border-radius: 12px; }
                h1 { color: #667eea; }
                .link { display: block; padding: 15px; margin: 10px 0; background: #667eea; color: white; text-decoration: none; border-radius: 8px; }
                .link:hover { background: #764ba2; }
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸš€ AI Stack Super Enhanced</h1>
                <p>åŠŸèƒ½ç•Œé¢æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè¯·è®¿é—®ä»¥ä¸‹é“¾æ¥ï¼š</p>
                <a href="/docs" class="link">ğŸ“š APIæ–‡æ¡£ (Swagger)</a>
                <a href="/redoc" class="link">ğŸ“– APIæ–‡æ¡£ (ReDoc)</a>
                <a href="/readyz" class="link">ğŸ’š å¥åº·æ£€æŸ¥</a>
                <a href="/kg/snapshot" class="link">ğŸ•¸ï¸ çŸ¥è¯†å›¾è°±å¿«ç…§</a>
                <a href="/index/info" class="link">ğŸ“Š ç´¢å¼•ä¿¡æ¯</a>
            </div>
        </body>
        </html>
        """)


@app.get("/", response_class=HTMLResponse)
def root():
    """æ ¹è·¯å¾„é‡å®šå‘åˆ°åŠŸèƒ½ç•Œé¢"""
    return dashboard()


class IngestReq(BaseModel):
    path: Optional[str] = None
    text: Optional[str] = None
    doc_id: Optional[str] = None
    save_index: Optional[bool] = True
    chunk_size: Optional[int] = None
    chunk_overlap: int = 0
    upsert: bool = False


def _ingest_text(
    text: str,
    *,
    src_path: Optional[str],
    doc_id: Optional[str],
    verify_truth: bool = True,
    enable_semantic_dedup: bool = False,
    use_semantic_segmentation: bool = True,
) -> str:
    """
    å°†æ–‡æœ¬æ‘„å…¥åˆ°ç´¢å¼•å¹¶æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
    
    æ ¹æ®éœ€æ±‚1.3ï¼šæ‰€æœ‰è¿›å…¥RAGåº“çš„ä¿¡æ¯éƒ½ä¼šè¿›è¡Œå»ä¼ªçš„å¤„ç†ï¼Œä¿è¯ä¿¡æ¯çŸ¥è¯†æ•°æ®ç­‰çš„çœŸå®æ€§å’Œå‡†ç¡®æ€§
    æ ¹æ®å·®è·3ï¼šä½¿ç”¨è¯­ä¹‰åˆ†å‰²ä¼˜åŒ–ï¼ˆSAGEé£æ ¼ï¼‰æå‡æ£€ç´¢ç›¸å…³æ€§
    
    Args:
        text: è¦æ‘„å…¥çš„æ–‡æœ¬å†…å®¹
        src_path: æºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        doc_id: æ–‡æ¡£IDï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
        verify_truth: æ˜¯å¦è¿›è¡ŒçœŸå®æ€§éªŒè¯ï¼ˆé»˜è®¤Trueï¼‰
        enable_semantic_dedup: æ˜¯å¦å¯ç”¨è¯­ä¹‰å»é‡
        use_semantic_segmentation: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰åˆ†å‰²ä¼˜åŒ–ï¼ˆå·®è·3ï¼‰
        
    Returns:
        ç”Ÿæˆçš„æ–‡æ¡£ID
        
    Raises:
        ValueError: å¦‚æœæ–‡æœ¬ä¸ºç©ºæˆ–éªŒè¯å¤±è´¥
    """
    if not text or not text.strip():
        raise ValueError("æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
    
    # è¯­ä¹‰åˆ†å‰²ä¼˜åŒ–ï¼ˆå·®è·3ï¼šSAGEé£æ ¼è¯­ä¹‰åˆ†å‰²ï¼‰
    if use_semantic_segmentation and len(text) > 500:
        try:
            from ..core.semantic_segmentation import get_semantic_segmentation_optimizer
            
            optimizer = get_semantic_segmentation_optimizer(embedding_model=_model)
            
            # å¼‚æ­¥è°ƒç”¨éœ€è¦åœ¨åŒæ­¥å‡½æ•°ä¸­å¤„ç†
            import asyncio
            try:
                loop = asyncio.get_event_loop()
            except RuntimeError:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
            
            chunks = loop.run_until_complete(
                optimizer.segment_text(text, doc_id=doc_id)
            )
            
            # å¦‚æœæœ‰å¤šä¸ªè¯­ä¹‰åˆ†å—ï¼Œåˆ†åˆ«æ‘„å…¥
            if chunks and len(chunks) > 1:
                doc_ids = []
                for chunk in chunks:
                    chunk_id = f"{doc_id or str(uuid.uuid4())}-chunk-{chunk.id}"
                    # é€’å½’è°ƒç”¨ï¼ˆè·³è¿‡åˆ†å‰²ï¼Œé¿å…å¾ªç¯ï¼‰
                    _ingest_text_single(
                        chunk.content,
                        src_path=src_path,
                        doc_id=chunk_id,
                        verify_truth=verify_truth,
                        enable_semantic_dedup=enable_semantic_dedup,
                        use_semantic_segmentation=False,  # é¿å…é‡å¤åˆ†å‰²
                    )
                    doc_ids.append(chunk_id)
                
                # è¿”å›ä¸»æ–‡æ¡£ID
                return doc_ids[0] if doc_ids else doc_id or str(uuid.uuid4())
            elif chunks:
                # åªæœ‰ä¸€ä¸ªåˆ†å—ï¼Œä½¿ç”¨åˆ†å—å†…å®¹
                text = chunks[0].content
        except Exception as e:
            logger.warning(f"è¯­ä¹‰åˆ†å‰²ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬: {e}")
    
    # è°ƒç”¨å•æ¬¡æ‘„å…¥å‡½æ•°
    return _ingest_text_single(
        text,
        src_path=src_path,
        doc_id=doc_id,
        verify_truth=verify_truth,
        enable_semantic_dedup=enable_semantic_dedup,
        use_semantic_segmentation=False,
    )


def _ingest_text_single(
    text: str,
    *,
    src_path: Optional[str],
    doc_id: Optional[str],
    verify_truth: bool = True,
    enable_semantic_dedup: bool = False,
    use_semantic_segmentation: bool = False,
) -> str:
    """
    å•æ¬¡æ–‡æœ¬æ‘„å…¥ï¼ˆå†…éƒ¨å‡½æ•°ï¼Œé¿å…é€’å½’ï¼‰
    """
    if not text or not text.strip():
        raise ValueError("æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
    
    # çœŸå®æ€§éªŒè¯ï¼ˆéœ€æ±‚1.3ï¼‰
    if verify_truth:
        try:
            # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯ä¾èµ–
            from ..pipelines.truth_verification_integration import get_truth_verification_integration
            
            verifier = get_truth_verification_integration(
                min_credibility=0.65,  # å¯é…ç½®
                auto_filter=True,       # è‡ªåŠ¨è¿‡æ»¤ä½å¯ä¿¡åº¦å†…å®¹
            )
            
            # è·å–å·²æœ‰æ–‡æ¡£ç”¨äºä¸€è‡´æ€§æ£€æŸ¥ï¼ˆé‡‡æ ·ï¼‰
            existing_texts = [doc.text for doc in _docs[:10]]  # å–å‰10ä¸ªæ–‡æ¡£
            
            verification_result = verifier.verify_before_ingest(
                text=text,
                source=src_path,
                metadata={"doc_id": doc_id} if doc_id else None,
                existing_docs=existing_texts,
            )
            
            if not verification_result.get("verified", True):
                credibility = verification_result.get("credibility_score", 0.0)
                reason = verification_result.get("reason", "å¯ä¿¡åº¦ä¸è¶³")
                logger.warning(
                    f"æ–‡æ¡£çœŸå®æ€§éªŒè¯æœªé€šè¿‡: credibility={credibility:.3f}, reason={reason}"
                )
                # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ‹’ç»
                # å½“å‰é…ç½®ä¸ºè‡ªåŠ¨è¿‡æ»¤ï¼Œæ‰€ä»¥æ‹’ç»
                raise ValueError(
                    f"çœŸå®æ€§éªŒè¯æœªé€šè¿‡: {reason} (å¯ä¿¡åº¦: {credibility:.3f})"
                )
            
            logger.debug(
                f"æ–‡æ¡£çœŸå®æ€§éªŒè¯é€šè¿‡: credibility={verification_result.get('credibility_score', 0.0):.3f}"
            )
            
        except ImportError:
            # å¦‚æœæ¨¡å—æœªå®‰è£…ï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­å¤„ç†
            logger.warning("çœŸå®æ€§éªŒè¯æ¨¡å—æœªæ‰¾åˆ°ï¼Œè·³è¿‡éªŒè¯")
        except Exception as e:
            # éªŒè¯è¿‡ç¨‹å‡ºé”™ï¼Œæ ¹æ®é…ç½®å†³å®š
            logger.error(f"çœŸå®æ€§éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
            # é»˜è®¤ç»§ç»­å¤„ç†ï¼ˆå¯æ ¹æ®é…ç½®ä¿®æ”¹ï¼‰
            if verify_truth:
                # å¦‚æœè¦æ±‚éªŒè¯ï¼ŒéªŒè¯å¤±è´¥åˆ™æ‹’ç»
                raise ValueError(f"çœŸå®æ€§éªŒè¯å¤±è´¥: {str(e)}")
    
    vec = _embed_texts([text])[0]
    did = doc_id or str(uuid.uuid4())
    with INDEX_LOCK:
        _docs.append(Doc(id=did, text=text, path=src_path))
        _vecs.append(vec)
    _kg_add(did, text, src_path)
    return did


@app.post("/rag/ingest")
def rag_ingest(req: IngestReq, _: bool = Depends(require_api_key)) -> Dict[str, Any]:
    """
    æ‘„å…¥æ–‡æœ¬æˆ–æ–‡ä»¶åˆ°RAGç´¢å¼•
    
    Args:
        req: æ‘„å…¥è¯·æ±‚ï¼ŒåŒ…å«pathæˆ–text
        _: APIå¯†é’¥éªŒè¯ï¼ˆé€šè¿‡Dependsè‡ªåŠ¨å¤„ç†ï¼‰
        
    Returns:
        DictåŒ…å«æˆåŠŸçŠ¶æ€ã€æ’å…¥æ•°é‡ã€æ–‡æ¡£IDåˆ—è¡¨å’Œç´¢å¼•å¤§å°
        
    Raises:
        HTTPException: å¦‚æœè·¯å¾„æˆ–æ–‡æœ¬æ— æ•ˆã€æ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥
    """
    if not req.path and not req.text:
        raise HTTPException(
            status_code=400,
            detail="å¿…é¡»æä¾›'path'æˆ–'text'å‚æ•°"
        )
    text = req.text
    if req.path:
        p = Path(req.path).expanduser()
        if not p.exists():
            raise HTTPException(
                status_code=404,
                detail=f"æ–‡ä»¶ä¸å­˜åœ¨: {p}"
            )
        if not p.is_file():
            raise HTTPException(
                status_code=400,
                detail=f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {p}"
            )
        try:
            # çº¯æ–‡æœ¬è¯»å–
            text = p.read_text(encoding="utf-8", errors="ignore")
        except PermissionError as e:
            raise HTTPException(
                status_code=403,
                detail=f"æ— æƒé™è¯»å–æ–‡ä»¶: {str(e)}"
            )
        except OSError as e:
            raise HTTPException(
                status_code=400,
                detail=f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"
            )
    if not text or not text.strip():
        raise HTTPException(
            status_code=400,
            detail="æ–‡æœ¬å†…å®¹ä¸ºç©º"
        )

    inserted = 0
    doc_ids: List[str] = []

    def add_one(_txt: str, _id: Optional[str] = None):
        nonlocal inserted
        if req.upsert and _id:
            _delete_by_id(_id)  # è¦†ç›–åŒ id
        did = _ingest_text(_txt, src_path=req.path, doc_id=_id)
        inserted += 1
        doc_ids.append(did)

    # å¯é€‰å­—ç¬¦çº§åˆ†ç‰‡
    if req.chunk_size and req.chunk_size > 0:
        s = text
        k = req.chunk_size
        ov = max(0, req.chunk_overlap or 0)
        i = 0
        part = 0
        while i < len(s):
            chunk = s[i : i + k]
            if chunk.strip():
                cid = f"{req.doc_id or Path(req.path or 'doc').stem}-chunk-{part}"
                add_one(chunk, cid)
                part += 1
            i += max(1, k - ov)
    else:
        add_one(text, req.doc_id)

    if req.save_index:
        _save_index()
    return {
        "success": True,
        "inserted": inserted,
        "ids": doc_ids,
        "size": _index_size(),
    }


@app.post("/rag/ingest_file")
async def rag_ingest_file(
    file: UploadFile = File(...),
    save_index: bool = True,
    doc_id: Optional[str] = None,
    chunk_size: Optional[int] = None,
    chunk_overlap: int = 0,
    upsert: bool = False,
    _: bool = Depends(require_api_key),
) -> Dict[str, Any]:
    """
    ä¸Šä¼ æ–‡ä»¶å¹¶æ‘„å…¥åˆ°RAGç´¢å¼•
    
    Args:
        file: ä¸Šä¼ çš„æ–‡ä»¶
        save_index: æ˜¯å¦ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜
        doc_id: å¯é€‰çš„æ–‡æ¡£ID
        chunk_size: åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        chunk_overlap: åˆ†å—é‡å å¤§å°
        upsert: å¦‚æœæ–‡æ¡£IDå·²å­˜åœ¨ï¼Œæ˜¯å¦æ›´æ–°
        
    Returns:
        DictåŒ…å«æˆåŠŸçŠ¶æ€ã€æ’å…¥æ•°é‡ã€æ–‡æ¡£IDåˆ—è¡¨å’Œç´¢å¼•å¤§å°
        
    Raises:
        HTTPException: å¦‚æœæ–‡ä»¶è¯»å–å¤±è´¥æˆ–æ–‡æœ¬ä¸ºç©º
    """
    try:
        data = await file.read()
        text = data.decode("utf-8", errors="ignore")
    except UnicodeDecodeError as e:
        raise HTTPException(
            status_code=400,
            detail=f"æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œæ— æ³•è§£ç ä¸ºUTF-8: {str(e)}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=400,
            detail=f"è¯»å–ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}"
        )

    inserted = 0
    ids: List[str] = []

    def add_one(_txt: str, _id: Optional[str] = None):
        nonlocal inserted
        if upsert and _id:
            _delete_by_id(_id)
        did = _ingest_text(_txt, src_path=file.filename, doc_id=_id)
        ids.append(did)
        inserted += 1

    if chunk_size and chunk_size > 0:
        s = text
        k = chunk_size
        ov = max(0, chunk_overlap or 0)
        i = 0
        part = 0
        while i < len(s):
            chunk = s[i : i + k]
            if chunk.strip():
                cid = f"{(doc_id or Path(file.filename or 'doc').stem)}-chunk-{part}"
                add_one(chunk, cid)
                part += 1
            i += max(1, k - ov)
    else:
        add_one(text, doc_id)

    if save_index:
        _save_index()
    return {"success": True, "inserted": inserted, "ids": ids, "size": _index_size()}


@app.post("/rag/ingest_dir")
def rag_ingest_dir(
    dir_path: str = Query(..., min_length=1),
    glob: str = Query(default="**/*.txt"),
    save_index: bool = True,
    limit: Optional[int] = None,
    chunk_size: Optional[int] = None,
    chunk_overlap: int = 0,
    _: bool = Depends(require_api_key),
) -> Dict[str, Any]:
    """
    æ‰¹é‡æ‘„å…¥ç›®å½•ä¸­çš„æ–‡ä»¶åˆ°RAGç´¢å¼•
    
    Args:
        dir_path: ç›®å½•è·¯å¾„
        glob: æ–‡ä»¶åŒ¹é…æ¨¡å¼
        save_index: æ˜¯å¦ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜
        limit: æœ€å¤§å¤„ç†æ–‡ä»¶æ•°
        chunk_size: åˆ†å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        chunk_overlap: åˆ†å—é‡å å¤§å°
        
    Returns:
        DictåŒ…å«æˆåŠŸçŠ¶æ€ã€æ’å…¥æ•°é‡ã€ç´¢å¼•å¤§å°å’Œæ–‡æ¡£IDæ•°é‡
        
    Raises:
        HTTPException: å¦‚æœç›®å½•ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®
    """
    p = Path(dir_path).expanduser()
    if not p.exists():
        raise HTTPException(
            status_code=404,
            detail=f"ç›®å½•ä¸å­˜åœ¨: {p}"
        )
    if not p.is_dir():
        raise HTTPException(
            status_code=400,
            detail=f"è·¯å¾„ä¸æ˜¯ç›®å½•: {p}"
        )
    inserted = 0
    ids: List[str] = []
    for i, f in enumerate(p.glob(glob)):
        if limit and i >= limit:
            break
        try:
            text = f.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            continue
        if chunk_size and chunk_size > 0:
            s = text
            k = chunk_size
            ov = max(0, chunk_overlap or 0)
            j = 0
            part = 0
            while j < len(s):
                ch = s[j : j + k]
                if ch.strip():
                    cid = f"{f.stem}-chunk-{part}"
                    did = _ingest_text(ch, src_path=str(f), doc_id=cid)
                    ids.append(did)
                    inserted += 1
                    part += 1
                j += max(1, k - ov)
        else:
            did = _ingest_text(text, src_path=str(f), doc_id=f.stem)
            ids.append(did)
            inserted += 1
    if save_index:
        _save_index()
    return {
        "success": True,
        "inserted": inserted,
        "size": _index_size(),
        "count_ids": len(ids),
    }


@app.get("/index/info")
def index_info() -> Dict[str, Any]:
    """
    è·å–ç´¢å¼•ä¿¡æ¯
    
    Returns:
        DictåŒ…å«ç´¢å¼•å¤§å°ã€ç»´åº¦å’Œåç«¯ç±»å‹
    """
    return {"size": _index_size(), "dimension": _DIM, "backend": "InMemory"}


@app.get("/index/ids")
def index_ids() -> Dict[str, List[str]]:
    """
    è·å–æ‰€æœ‰æ–‡æ¡£IDåˆ—è¡¨
    
    Returns:
        DictåŒ…å«æ‰€æœ‰æ–‡æ¡£IDåˆ—è¡¨
    """
    return {"ids": [d.id for d in _docs]}


@app.delete("/index/clear")
def index_clear(
    remove_file: bool = Query(default=True, description="æ˜¯å¦åˆ é™¤ç£ç›˜ä¸Šçš„ç´¢å¼•æ–‡ä»¶"),
    clear_kg: bool = Query(default=True, description="æ˜¯å¦åŒæ—¶æ¸…ç©ºçŸ¥è¯†å›¾è°±"),
    _: bool = Depends(require_api_key),
) -> Dict[str, Any]:
    """
    æ¸…ç©ºç´¢å¼•å’Œå¯é€‰çš„çŸ¥è¯†å›¾è°±
    
    Args:
        remove_file: æ˜¯å¦åˆ é™¤ç£ç›˜ä¸Šçš„ç´¢å¼•æ–‡ä»¶
        clear_kg: æ˜¯å¦åŒæ—¶æ¸…ç©ºçŸ¥è¯†å›¾è°±
        _: APIå¯†é’¥éªŒè¯
        
    Returns:
        DictåŒ…å«æ¸…ç©ºå‰çš„å¤§å°ã€æ¸…ç©ºåçš„å¤§å°å’ŒKGæ¸…ç†ç»“æœ
    """
    before = _index_size()
    _docs.clear()
    _vecs.clear()
    if remove_file:
        try:
            if INDEX_DOCS.exists():
                INDEX_DOCS.unlink()
            if INDEX_VECS.exists():
                INDEX_VECS.unlink()
        except OSError as e:
            # è®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­æ¸…ç†è¿‡ç¨‹
            pass
    # åŒæ—¶å¤„ç† KG æ¸…ç†ï¼ˆå†…å­˜ä¸å¯é€‰æ–‡ä»¶ï¼‰
    kg = {}
    if clear_kg:
        kg = _kg_clear(remove_file=remove_file)
    return {"cleared": before, "before": before, "kg": kg if clear_kg else None}


@app.post("/index/save")
def index_save(_: bool = Depends(require_api_key)) -> Dict[str, Any]:
    """
    ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜
    
    Returns:
        DictåŒ…å«ä¿å­˜çŠ¶æ€ã€è·¯å¾„ã€å¤§å°å’Œç»´åº¦ä¿¡æ¯
    """
    return _save_index()


@app.post("/index/load")
def index_load(_: bool = Depends(require_api_key)) -> Dict[str, Any]:
    """
    ä»ç£ç›˜åŠ è½½ç´¢å¼•
    
    Returns:
        DictåŒ…å«åŠ è½½çŠ¶æ€ã€ç´¢å¼•å¤§å°å’Œç»´åº¦ä¿¡æ¯
    """
    return _load_index()


@app.delete("/index/delete")
def index_delete(
    doc_id: str = Query(..., min_length=1, description="è¦åˆ é™¤çš„æ–‡æ¡£ID"),
    _: bool = Depends(require_api_key),
) -> Dict[str, Any]:
    """
    æ ¹æ®IDåˆ é™¤æ–‡æ¡£
    
    Args:
        doc_id: è¦åˆ é™¤çš„æ–‡æ¡£ID
        _: APIå¯†é’¥éªŒè¯
        
    Returns:
        DictåŒ…å«åˆ é™¤æ•°é‡å’Œå½“å‰ç´¢å¼•å¤§å°
        
    Raises:
        HTTPException: å¦‚æœæ–‡æ¡£IDä¸å­˜åœ¨
    """
    ok = _delete_by_id(doc_id)
    if not ok:
        raise HTTPException(
            status_code=404,
            detail=f"æ–‡æ¡£IDä¸å­˜åœ¨: {doc_id}"
        )
    return {"deleted": 1, "size": _index_size()}


class SearchItem(BaseModel):
    id: str
    score: float
    snippet: str
    path: Optional[str] = None


class SearchResp(BaseModel):
    items: List[SearchItem]


@app.get("/rag/search", response_model=SearchResp)
def rag_search(
    query: str = Query(..., min_length=1, description="æœç´¢æŸ¥è¯¢æ–‡æœ¬"),
    top_k: int = Query(5, ge=1, le=50, description="è¿”å›ç»“æœæ•°é‡"),
    modality: Optional[str] = Query("text", description="æ£€ç´¢æ¨¡æ€: text, image, audio, multimodal"),
    fusion_strategy: Optional[str] = Query("weighted", description="èåˆç­–ç•¥: weighted, rank_fusion, simple"),
    use_kg_infused: Optional[bool] = Query(False, description="æ˜¯å¦ä½¿ç”¨KG-Infused RAGï¼ˆå·®è·4ï¼‰"),
    _: bool = Depends(require_api_key),
) -> SearchResp:
    """
    è¯­ä¹‰æœç´¢RAGç´¢å¼•ä¸­çš„æ–‡æ¡£ï¼ˆæ”¯æŒå¤šæ¨¡æ€æ£€ç´¢å’ŒKG-Infused RAGï¼‰
    
    æ ¹æ®éœ€æ±‚1.5ï¼šæ”¯æŒå¤šæ¨¡æ€æ£€ç´¢ï¼ˆæ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘ï¼‰
    æ ¹æ®å·®è·4ï¼šæ”¯æŒKG-Infused RAGï¼ˆçŸ¥è¯†å›¾è°±æ·±åº¦èåˆï¼‰
    
    Args:
        query: æœç´¢æŸ¥è¯¢æ–‡æœ¬
        top_k: è¿”å›ç»“æœæ•°é‡ï¼ˆ1-50ï¼‰
        modality: æ£€ç´¢æ¨¡æ€
            - text: ä»…æ–‡æœ¬æ£€ç´¢ï¼ˆé»˜è®¤ï¼‰
            - image: ä»…å›¾åƒæ£€ç´¢
            - audio: ä»…éŸ³é¢‘æ£€ç´¢
            - multimodal: æ··åˆæ¨¡æ€æ£€ç´¢ï¼ˆæ–‡æœ¬+å›¾åƒ+éŸ³é¢‘ï¼‰
        fusion_strategy: èåˆç­–ç•¥ï¼ˆä»…ç”¨äºmultimodalï¼‰
            - weighted: åŠ æƒèåˆï¼ˆé»˜è®¤ï¼‰
            - rank_fusion: æ’åºèåˆï¼ˆRRFï¼‰
            - simple: ç®€å•åˆå¹¶
        use_kg_infused: æ˜¯å¦ä½¿ç”¨KG-Infused RAGï¼ˆçŸ¥è¯†å›¾è°±æ·±åº¦èåˆï¼‰
            
    Returns:
        SearchRespåŒ…å«æœç´¢ç»“æœåˆ—è¡¨
        
    Raises:
        HTTPException: å¦‚æœæœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯
    """
    if _index_size() == 0:
        return SearchResp(items=[])
    
    try:
        # KG-Infused RAGï¼ˆå·®è·4ï¼šçŸ¥è¯†å›¾è°±æ·±åº¦èåˆï¼‰
        if use_kg_infused:
            try:
                from ..core.kg_infused_rag import get_kg_infused_rag
                
                # åˆ›å»ºç®€å•çš„RAGæ£€ç´¢å™¨é€‚é…å™¨
                class SimpleRAGRetriever:
                    async def search(self, query: str, top_k: int):
                        q = _embed_texts([query])[0].astype(np.float32)
                        X = _index_matrix()
                        scores = (X @ q).tolist()
                        order = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
                        items = []
                        for i in order:
                            d = _docs[i]
                            items.append({
                                "id": d.id,
                                "document_id": d.id,
                                "content": d.text,
                                "snippet": d.text[:200],
                                "score": float(scores[i]),
                                "metadata": {"path": d.path},
                            })
                        return {"items": items}
                
                # è·å–KGæŸ¥è¯¢å¼•æ“
                kg_query_engine = None
                try:
                    from ..knowledge_graph.enhanced_kg_query import get_kg_query_engine
                    if _kg_nodes and _kg_edges:
                        kg_query_engine = get_kg_query_engine(_kg_nodes, _kg_edges)
                except Exception:
                    pass
                
                # åˆ›å»ºKG-Infused RAGå®ä¾‹
                rag_retriever = SimpleRAGRetriever()
                kg_infused_rag = get_kg_infused_rag(
                    rag_retriever=rag_retriever,
                    kg_query_engine=kg_query_engine,
                )
                
                # æ‰§è¡ŒKGå¢å¼ºæ£€ç´¢ï¼ˆå¼‚æ­¥éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
                import asyncio
                try:
                    loop = asyncio.get_event_loop()
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                
                result = loop.run_until_complete(
                    kg_infused_rag.retrieve_with_kg(query, top_k=top_k)
                )
                
                # è½¬æ¢ä¸ºSearchItemæ ¼å¼
                items = []
                for doc in result.get("documents", []):
                    items.append(
                        SearchItem(
                            id=doc.get("id") or doc.get("document_id", ""),
                            score=doc.get("score", doc.get("kg_enhanced_score", 0.0)),
                            snippet=doc.get("snippet", doc.get("content", ""))[:200],
                            path=doc.get("metadata", {}).get("path", ""),
                        )
                    )
                
                logger.info(f"KG-Infused RAGæ£€ç´¢å®Œæˆï¼š{len(items)} ä¸ªç»“æœï¼ŒKGä¸Šä¸‹æ–‡ï¼š{len(result.get('kg_context', ''))} å­—ç¬¦")
                return SearchResp(items=items)
                
            except (ImportError, Exception) as e:
                logger.warning(f"KG-Infused RAGå¤±è´¥ï¼Œå›é€€åˆ°æ ‡å‡†æ£€ç´¢: {e}")
                # ç»§ç»­ä½¿ç”¨æ ‡å‡†æ£€ç´¢
                pass
        
        # å¦‚æœæ˜¯å¤šæ¨¡æ€æ£€ç´¢ï¼Œå°è¯•ä½¿ç”¨å¤šæ¨¡æ€æ£€ç´¢å™¨
        if modality and modality.lower() in ["multimodal", "image", "audio"]:
            try:
                from ..core.multimodal_retrieval import get_multimodal_retriever
                
                multimodal_retriever = get_multimodal_retriever()
                
                # ç¡®å®šè¦æ£€ç´¢çš„æ¨¡æ€
                if modality.lower() == "multimodal":
                    modalities = ["text", "image", "audio"]
                elif modality.lower() == "image":
                    modalities = ["image"]
                elif modality.lower() == "audio":
                    modalities = ["audio"]
                else:
                    modalities = ["text"]
                
                # æ‰§è¡Œå¤šæ¨¡æ€æ£€ç´¢ï¼ˆå¼‚æ­¥éœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰
                import asyncio
                try:
                    loop = asyncio.get_event_loop()
                except RuntimeError:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                
                results = loop.run_until_complete(
                    multimodal_retriever.hybrid_retrieve(
                        query=query,
                        modalities=modalities,
                        top_k=top_k,
                        fusion_strategy=fusion_strategy or "weighted",
                    )
                )
                
                # è½¬æ¢ä¸ºSearchItemæ ¼å¼
                items = []
                for result in results:
                    items.append(
                        SearchItem(
                            id=result.document_id,
                            score=result.similarity_score,
                            snippet=result.content[:200] if result.content else "",
                            path=result.source,
                        )
                    )
                
                return SearchResp(items=items)
                
            except (ImportError, Exception) as e:
                logger.warning(f"å¤šæ¨¡æ€æ£€ç´¢å¤±è´¥ï¼Œå›é€€åˆ°æ–‡æœ¬æ£€ç´¢: {e}")
                # å›é€€åˆ°æ–‡æœ¬æ£€ç´¢
                pass
        
        # é»˜è®¤æ–‡æœ¬æ£€ç´¢ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        q = _embed_texts([query])[0].astype(np.float32)
        X = _index_matrix()  # å·²å½’ä¸€åŒ–ï¼Œç‚¹ç§¯=cos
        scores = (X @ q).tolist()
        order = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
        items = []
        for i in order:
            d = _docs[i]
            items.append(
                SearchItem(
                    id=d.id,
                    score=float(scores[i]),
                    snippet=d.text[:200],
                    path=d.path,
                )
            )
        return SearchResp(items=items)
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"æœç´¢å¤±è´¥: {str(e)}"
        )


@app.get("/rag/groups")
def rag_groups(
    k: int = Query(3, ge=1, le=50, description="åˆ†ç»„æ•°é‡"),
    max_items: int = Query(100, ge=1, le=1000, description="æœ€å¤§å¤„ç†æ–‡æ¡£æ•°"),
) -> Dict[str, Any]:
    """
    å¯¹ç´¢å¼•ä¸­çš„æ–‡æ¡£è¿›è¡Œè¯­ä¹‰åˆ†ç»„
    
    Args:
        k: åˆ†ç»„æ•°é‡ï¼ˆ1-50ï¼‰
        max_items: æœ€å¤§å¤„ç†æ–‡æ¡£æ•°ï¼ˆ1-1000ï¼‰
        
    Returns:
        DictåŒ…å«åˆ†ç»„ç»“æœã€åˆ†ç»„æ•°é‡å’Œæ€»æ•°
    """
    n = _index_size()
    if n == 0:
        return {"success": True, "k": 0, "total": 0, "groups": []}
    idx = list(range(min(n, max_items)))
    k = min(k, len(idx))
    # é€‰æ‹©å‰kä¸ªä½œä¸ºä¸­å¿ƒï¼ŒæŒ‰æœ€å¤§ä½™å¼¦ç›¸ä¼¼åº¦åˆ†é…
    X = _index_matrix()[idx]
    ids = [_docs[i].id for i in idx]
    centers = X[:k]
    assigns: List[List[int]] = [[] for _ in range(k)]
    for i, vec in enumerate(X):
        if k == 0:
            break
        sims = (centers @ vec).tolist()
        c = int(np.argmax(sims))
        assigns[c].append(i)
    groups = []
    for ci, members in enumerate(assigns):
        gids = [ids[m] for m in members]
        groups.append(
            {
                "center": ids[ci] if ci < len(ids) else None,
                "size": len(gids),
                "ids": gids,
            }
        )
    return {"success": True, "k": k, "total": len(idx), "groups": groups}


@app.get("/kg/snapshot")
def kg_snapshot() -> Dict[str, Any]:
    """
    è·å–çŸ¥è¯†å›¾è°±å¿«ç…§
    
    Returns:
        DictåŒ…å«çŸ¥è¯†å›¾è°±çš„å®ä½“ã€èŠ‚ç‚¹å’Œè¾¹çš„ä¿¡æ¯ï¼Œä»¥åŠç¤ºä¾‹æ•°æ®
    """
    # æä¾›ç®€è¦ç»Ÿè®¡ã€å®ä½“åˆ—è¡¨ä¸å°‘é‡ç¤ºä¾‹
    entities = [
        {
            "id": n["id"],
            "type": n.get("type"),
            "value": n.get("value"),
            "count": n.get("count", 0),
        }
        for n in _kg_nodes.values()
        if n.get("type") in {"email", "url"}
    ]
    emails = [e["value"] for e in entities if e["type"] == "email"][:10]
    urls = [e["value"] for e in entities if e["type"] == "url"][:10]
    return {
        "success": True,
        "nodes": len(_kg_nodes),
        "edges": len(_kg_edges),
        "entities": entities,
        "sample": {"emails": emails, "urls": urls},
    }


@app.post("/kg/save")
def kg_save(
    path: Optional[str] = Query(default=None, description="ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰"),
    _: bool = Depends(require_api_key),
) -> Dict[str, Any]:
    """
    ä¿å­˜çŸ¥è¯†å›¾è°±åˆ°æ–‡ä»¶
    
    Args:
        path: ä¿å­˜è·¯å¾„ï¼ˆå¦‚æœä¸æä¾›ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
        _: APIå¯†é’¥éªŒè¯
        
    Returns:
        DictåŒ…å«ä¿å­˜çŠ¶æ€ã€è·¯å¾„ã€èŠ‚ç‚¹æ•°å’Œè¾¹æ•°
    """
    return _kg_save(Path(path) if path else None)


@app.delete("/kg/clear")
def kg_clear(
    remove_file: bool = Query(True, description="æ˜¯å¦åˆ é™¤ç£ç›˜æ–‡ä»¶"),
    _: bool = Depends(require_api_key),
) -> Dict[str, Any]:
    """
    æ¸…ç©ºçŸ¥è¯†å›¾è°±
    
    Args:
        remove_file: æ˜¯å¦åˆ é™¤ç£ç›˜ä¸Šçš„çŸ¥è¯†å›¾è°±æ–‡ä»¶
        _: APIå¯†é’¥éªŒè¯
        
    Returns:
        DictåŒ…å«æ¸…ç†çŠ¶æ€å’Œæ¸…ç†çš„èŠ‚ç‚¹/è¾¹æ•°é‡
    """
    return _kg_clear(remove_file=remove_file)


@app.post("/kg/load")
def kg_load(
    path: Optional[str] = Query(default=None, description="åŠ è½½è·¯å¾„ï¼ˆå¯é€‰ï¼‰"),
    _: bool = Depends(require_api_key),
) -> Dict[str, Any]:
    """
    ä»æ–‡ä»¶åŠ è½½çŸ¥è¯†å›¾è°±
    
    Args:
        path: åŠ è½½è·¯å¾„ï¼ˆå¦‚æœä¸æä¾›ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
        _: APIå¯†é’¥éªŒè¯
        
    Returns:
        DictåŒ…å«åŠ è½½çŠ¶æ€ã€è·¯å¾„ã€èŠ‚ç‚¹æ•°å’Œè¾¹æ•°
        
    Raises:
        HTTPException: å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–è¯»å–å¤±è´¥
    """
    return _kg_load(Path(path) if path else None)


# å·²æœ‰å ä½
@app.get("/kg/stats")
def kg_stats() -> Dict[str, Any]:
    """
    è·å–çŸ¥è¯†å›¾è°±ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        DictåŒ…å«èŠ‚ç‚¹æ•°ã€è¾¹æ•°å’ŒçŠ¶æ€ä¿¡æ¯
    """
    return {"nodes": len(_kg_nodes), "edges": len(_kg_edges), "ok": True}


@app.post("/index/rebuild")
def index_rebuild(
    reload_docs: bool = Query(True, description="æ˜¯å¦ä»ç£ç›˜é‡æ–°åŠ è½½æ–‡æ¡£"),
    batch: int = Query(256, ge=1, le=4096, description="æ‰¹å¤„ç†å¤§å°"),
    save_index: bool = Query(True, description="é‡å»ºåæ˜¯å¦ä¿å­˜ç´¢å¼•"),
    _: bool = Depends(require_api_key),
) -> Dict[str, Any]:
    """
    é‡å»ºç´¢å¼•ï¼ˆé‡æ–°è®¡ç®—æ‰€æœ‰å‘é‡ï¼‰
    
    ç”¨äºåœ¨æ¨¡å‹æ›´æ–°æˆ–ç´¢å¼•æŸåæ—¶é‡æ–°æ„å»ºæ•´ä¸ªç´¢å¼•ã€‚
    å¯ä»¥æ‰¹é‡å¤„ç†å¤§é‡æ–‡æ¡£ï¼Œé¿å…å†…å­˜æº¢å‡ºã€‚
    
    Args:
        reload_docs: æ˜¯å¦ä»ç£ç›˜é‡æ–°åŠ è½½æ–‡æ¡£åˆ—è¡¨
        batch: æ‰¹å¤„ç†å¤§å°ï¼ˆ1-4096ï¼‰ï¼Œæ§åˆ¶æ¯æ¬¡å¤„ç†çš„æ–‡æ¡£æ•°
        save_index: é‡å»ºåæ˜¯å¦ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜
        _: APIå¯†é’¥éªŒè¯
        
    Returns:
        DictåŒ…å«é‡å»ºçš„æ–‡æ¡£æ•°é‡ã€ç»´åº¦å’Œä¿å­˜çŠ¶æ€
        
    Raises:
        HTTPException: å¦‚æœé‡å»ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯
    """
    try:
        # å¯é€‰ä»ç£ç›˜é‡æ–°åŠ è½½ docs
        if reload_docs and INDEX_DOCS.exists():
            docs = json.loads(INDEX_DOCS.read_text(encoding="utf-8"))
            with INDEX_LOCK:
                _docs.clear()
                _docs.extend(Doc(**d) for d in docs)
        # é‡æ–°è®¡ç®—å…¨éƒ¨å‘é‡
        with INDEX_LOCK:
            _vecs.clear()
        texts = [d.text for d in _docs]
        new_vecs: List[np.ndarray] = []
        for i in range(0, len(texts), batch):
            new_vecs.append(_embed_texts(texts[i : i + batch]))
        with INDEX_LOCK:
            if new_vecs:
                _vecs.extend(v.astype(np.float32) for v in np.vstack(new_vecs))
        if save_index:
            _save_index()
        return {"rebuilt": _index_size(), "dimension": _DIM, "saved": bool(save_index)}
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ç´¢å¼•é‡å»ºå¤±è´¥: {str(e)}"
        )


@app.get("/kg/query")
def kg_query(
    query_type: str = Query("entities", description="æŸ¥è¯¢ç±»å‹ï¼šentities, relations, path, subgraph, statistics"),
    type: Optional[str] = Query(None, description="å®ä½“ç±»å‹ï¼ˆemailã€urlç­‰ï¼‰"),
    value: Optional[str] = Query(None, description="å®ä½“å€¼"),
    value_pattern: Optional[str] = Query(None, description="å®ä½“å€¼æ¨¡å¼ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼‰"),
    source: Optional[str] = Query(None, description="æºå®ä½“IDï¼ˆç”¨äºå…³ç³»æˆ–è·¯å¾„æŸ¥è¯¢ï¼‰"),
    target: Optional[str] = Query(None, description="ç›®æ ‡å®ä½“IDï¼ˆç”¨äºå…³ç³»æˆ–è·¯å¾„æŸ¥è¯¢ï¼‰"),
    relation_type: Optional[str] = Query(None, description="å…³ç³»ç±»å‹"),
    max_depth: int = Query(2, ge=1, le=5, description="æœ€å¤§æ·±åº¦ï¼ˆç”¨äºè·¯å¾„å’Œå­å›¾æŸ¥è¯¢ï¼‰"),
    limit: int = Query(100, ge=1, le=1000, description="è¿”å›æ•°é‡é™åˆ¶"),
) -> Dict[str, Any]:
    """
    å¢å¼ºçš„çŸ¥è¯†å›¾è°±æŸ¥è¯¢ï¼ˆéœ€æ±‚1.8ï¼‰
    
    æ”¯æŒå¤šç§æŸ¥è¯¢æ–¹å¼ï¼š
    - entities: æŸ¥è¯¢å®ä½“
    - relations: æŸ¥è¯¢å…³ç³»
    - path: æŸ¥è¯¢ä¸¤ä¸ªå®ä½“ä¹‹é—´çš„è·¯å¾„
    - subgraph: æŸ¥è¯¢å­å›¾ï¼ˆä»¥æŸä¸ªå®ä½“ä¸ºä¸­å¿ƒï¼‰
    - statistics: æŸ¥è¯¢ç»Ÿè®¡ä¿¡æ¯
    
    Args:
        query_type: æŸ¥è¯¢ç±»å‹
        type: å®ä½“ç±»å‹ï¼ˆå¯é€‰ï¼‰
        value: å®ä½“å€¼ï¼ˆå¯é€‰ï¼‰
        value_pattern: å®ä½“å€¼æ¨¡å¼ï¼ˆæ­£åˆ™è¡¨è¾¾å¼ï¼Œå¯é€‰ï¼‰
        source: æºå®ä½“IDï¼ˆå¯é€‰ï¼‰
        target: ç›®æ ‡å®ä½“IDï¼ˆå¯é€‰ï¼‰
        relation_type: å…³ç³»ç±»å‹ï¼ˆå¯é€‰ï¼‰
        max_depth: æœ€å¤§æ·±åº¦ï¼ˆ1-5ï¼‰
        limit: è¿”å›æ•°é‡é™åˆ¶ï¼ˆ1-1000ï¼‰
        
    Returns:
        æŸ¥è¯¢ç»“æœå­—å…¸ï¼Œæ ¼å¼æ ¹æ®æŸ¥è¯¢ç±»å‹è€Œå®š
    """
    try:
        # å¯¼å…¥å¢å¼ºæŸ¥è¯¢å¼•æ“
        from ..knowledge_graph.enhanced_kg_query import get_kg_query_engine
        
        # åˆ›å»ºæŸ¥è¯¢å¼•æ“
        query_engine = get_kg_query_engine(_kg_nodes, _kg_edges)
        
        if query_type == "entities":
            # å®ä½“æŸ¥è¯¢
            results = query_engine.query_entities(
                entity_type=type,
                value_pattern=value_pattern or (f".*{value}.*" if value else None),
                limit=limit,
            )
            return {
                "success": True,
                "query_type": "entities",
                "results": results,
                "count": len(results),
            }
        
        elif query_type == "relations":
            # å…³ç³»æŸ¥è¯¢
            source_entity = source or (_kg_node_id(type, value) if type and value else None)
            results = query_engine.query_relations(
                source_entity=source_entity,
                target_entity=target,
                relation_type=relation_type,
                limit=limit,
            )
            return {
                "success": True,
                "query_type": "relations",
                "results": results,
                "count": len(results),
            }
        
        elif query_type == "path":
            # è·¯å¾„æŸ¥è¯¢
            if not source or not target:
                # å…¼å®¹æ—§çš„æŸ¥è¯¢æ–¹å¼ï¼štype + value ä½œä¸º source
                if type and value:
                    source_entity = _kg_node_id(type, value)
                else:
                    return {"success": False, "error": "éœ€è¦æä¾›sourceå’Œtargetå‚æ•°"}
            else:
                source_entity = source
            
            path = query_engine.query_path(
                source_entity=source_entity,
                target_entity=target,
                max_depth=max_depth,
            )
            
            if path:
                return {
                    "success": True,
                    "query_type": "path",
                    "path": path,
                    "path_length": len(path) - 1,
                }
            else:
                return {
                    "success": True,
                    "query_type": "path",
                    "path": None,
                    "message": "æœªæ‰¾åˆ°è·¯å¾„",
                }
        
        elif query_type == "subgraph":
            # å­å›¾æŸ¥è¯¢
            center = source or (_kg_node_id(type, value) if type and value else None)
            if not center:
                return {"success": False, "error": "éœ€è¦æä¾›centerå®ä½“ï¼ˆé€šè¿‡sourceæˆ–type+valueï¼‰"}
            
            subgraph = query_engine.query_subgraph(
                center_entity=center,
                max_depth=max_depth,
                max_nodes=limit,
            )
            return {
                "success": True,
                "query_type": "subgraph",
                "subgraph": subgraph,
            }
        
        elif query_type == "statistics":
            # ç»Ÿè®¡æŸ¥è¯¢
            stats = query_engine.query_statistics()
            return {
                "success": True,
                "query_type": "statistics",
                "statistics": stats,
            }
        
        else:
            # å…¼å®¹æ—§çš„æŸ¥è¯¢æ–¹å¼ï¼ˆæŒ‰typeå’ŒvalueæŸ¥è¯¢æ–‡æ¡£ï¼‰
            if type and value:
                nid = _kg_node_id(type, value)
                if nid not in _kg_nodes:
                    return {"success": True, "docs": [], "count": 0}
                dnids = [e["src"] for e in _kg_edges if e.get("dst") == nid]
                ids = [n.split(":", 1)[1] for n in dnids if n.startswith("doc:")]
                # æ˜ å°„åˆ°ç°å­˜æ–‡æ¡£ï¼ˆå¯èƒ½å·²è¢«åˆ é™¤ï¼‰
                existing = {d.id for d in _docs}
                ids = [i for i in ids if i in existing]
                return {"success": True, "docs": ids, "count": len(ids)}
            else:
                return {"success": False, "error": f"æœªçŸ¥çš„æŸ¥è¯¢ç±»å‹: {query_type}"}
    
    except Exception as e:
        logger.error(f"çŸ¥è¯†å›¾è°±æŸ¥è¯¢å¤±è´¥: {e}")
        return {"success": False, "error": str(e)}

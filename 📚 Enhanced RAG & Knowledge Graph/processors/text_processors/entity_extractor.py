"""
å®ä½“æå–å™¨
æ”¯æŒNERå®ä½“è¯†åˆ«ã€å®ä½“é“¾æ¥ã€å®ä½“æ¶ˆæ­§ç­‰åŠŸèƒ½
"""
import re
from typing import List, Dict, Set, Optional


class EntityExtractor:
    """å®ä½“æå–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å®ä½“æå–å™¨"""
        # é¢„å®šä¹‰çš„å®ä½“æ¨¡å¼
        self.patterns = {
            "email": r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            "url": r'https?://[^\s<>"{}|\\^`\[\]]+',
            "phone": r'1[3-9]\d{9}',
            "date": r'\d{4}[-/å¹´]\d{1,2}[-/æœˆ]\d{1,2}[æ—¥]?',
            "money": r'[Â¥$â‚¬Â£]\s?\d+(?:,\d{3})*(?:\.\d{2})?',
            "percentage": r'\d+(?:\.\d+)?%'
        }
    
    def extract(
        self,
        text: str,
        entity_types: Optional[List[str]] = None
    ) -> Dict[str, List[Dict]]:
        """
        æå–å®ä½“
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            entity_types: è¦æå–çš„å®ä½“ç±»å‹åˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            
        Returns:
            æå–çš„å®ä½“å­—å…¸
        """
        entities = {}
        
        # åŸºäºè§„åˆ™çš„å®ä½“æå–
        for entity_type, pattern in self.patterns.items():
            if entity_types and entity_type not in entity_types:
                continue
            
            matches = re.finditer(pattern, text)
            entity_list = []
            
            for match in matches:
                entity_list.append({
                    "text": match.group(),
                    "type": entity_type,
                    "start": match.start(),
                    "end": match.end(),
                    "confidence": 0.95
                })
            
            if entity_list:
                entities[entity_type] = entity_list
        
        # å‘½åå®ä½“è¯†åˆ«ï¼ˆéœ€è¦NLPæ¨¡å‹ï¼‰
        ner_entities = self._extract_ner_entities(text)
        entities.update(ner_entities)
        
        return {
            "success": True,
            "text": text,
            "entities": entities,
            "entity_count": sum(len(v) for v in entities.values()),
            "entity_types": list(entities.keys())
        }
    
    def _extract_ner_entities(self, text: str) -> Dict[str, List[Dict]]:
        """
        å‘½åå®ä½“è¯†åˆ«
        
        ä½¿ç”¨NLPæ¨¡å‹è¯†åˆ«äººåã€åœ°åã€æœºæ„åç­‰
        
        å®é™…å®ç°éœ€è¦ä½¿ç”¨ï¼š
        - spaCy
        - transformers (BERT-NER)
        - æˆ–ç™¾åº¦ã€è®¯é£ç­‰NER API
        """
        # æ¨¡æ‹ŸNERç»“æœ
        # ç®€å•çš„è§„åˆ™è¯†åˆ«ï¼ˆå®é™…åº”ä½¿ç”¨æ¨¡å‹ï¼‰
        
        entities = {}
        
        # è¯†åˆ«ä¸­æ–‡äººåï¼ˆç®€å•è§„åˆ™ï¼‰
        person_pattern = r'[ç‹æå¼ åˆ˜é™ˆæ¨é»„èµµå‘¨å´å¾å­™é©¬æœ±èƒ¡éƒ­ä½•é«˜æ—ç½—éƒ‘æ¢å®‹è°¢å”éŸ©æ›¹è®¸é‚“è§å†¯æ›¾ç¨‹è”¡å½­æ½˜è¢äºè‘£ä½™è‹å¶å•é­è’‹ç”°æœä¸æ²ˆå§œèŒƒæ±Ÿå‚…é’Ÿå¢æ±ªæˆ´å´”ä»»é™†å»–å§šæ–¹é‡‘é‚±å¤è°­éŸ¦è´¾é‚¹çŸ³ç†Šå­Ÿç§¦é˜è–›ä¾¯é›·ç™½é¾™æ®µéƒå­”é‚µå²æ¯›å¸¸ä¸‡é¡¾èµ–æ­¦åº·è´ºä¸¥å°¹é’±æ–½ç‰›æ´ªé¾š]\w{1,3}'
        persons = re.findall(person_pattern, text)
        if persons:
            entities["person"] = [
                {"text": p, "type": "person", "confidence": 0.75}
                for p in set(persons)
            ]
        
        # è¯†åˆ«æœºæ„åï¼ˆç®€å•è§„åˆ™ï¼‰
        org_keywords = ['å…¬å¸', 'é›†å›¢', 'ä¼ä¸š', 'è‚¡ä»½', 'æœ‰é™', 'ç§‘æŠ€', 'é“¶è¡Œ', 'å¤§å­¦', 'å­¦é™¢', 'åŒ»é™¢', 'æ”¿åºœ', 'éƒ¨é—¨']
        org_pattern = r'[\u4e00-\u9fa5]{2,10}(?:' + '|'.join(org_keywords) + ')'
        orgs = re.findall(org_pattern, text)
        if orgs:
            entities["organization"] = [
                {"text": o, "type": "organization", "confidence": 0.80}
                for o in set(orgs)
            ]
        
        # è¯†åˆ«åœ°åï¼ˆç®€å•è§„åˆ™ï¼‰
        location_keywords = ['çœ', 'å¸‚', 'å¿', 'åŒº', 'é•‡', 'æ‘', 'è·¯', 'è¡—', 'é“']
        loc_pattern = r'[\u4e00-\u9fa5]{2,6}(?:' + '|'.join(location_keywords) + ')'
        locations = re.findall(loc_pattern, text)
        if locations:
            entities["location"] = [
                {"text": l, "type": "location", "confidence": 0.78}
                for l in set(locations)
            ]
        
        return entities
    
    def extract_with_context(
        self,
        text: str,
        entity_types: Optional[List[str]] = None,
        context_window: int = 50
    ) -> List[Dict]:
        """
        æå–å®ä½“å¹¶åŒ…å«ä¸Šä¸‹æ–‡
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            entity_types: å®ä½“ç±»å‹
            context_window: ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆå­—ç¬¦ï¼‰
            
        Returns:
            å®ä½“åŠå…¶ä¸Šä¸‹æ–‡
        """
        result = self.extract(text, entity_types)
        entities_with_context = []
        
        for entity_type, entity_list in result["entities"].items():
            for entity in entity_list:
                start = max(0, entity["start"] - context_window)
                end = min(len(text), entity["end"] + context_window)
                
                entities_with_context.append({
                    **entity,
                    "context": text[start:end],
                    "context_start": start,
                    "context_end": end
                })
        
        return entities_with_context
    
    def link_entities(self, entities: List[Dict]) -> List[Dict]:
        """
        å®ä½“é“¾æ¥
        
        å°†æå–çš„å®ä½“é“¾æ¥åˆ°çŸ¥è¯†åº“
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            
        Returns:
            é“¾æ¥åçš„å®ä½“
        """
        # å®é™…åº”æŸ¥è¯¢çŸ¥è¯†å›¾è°±æˆ–å¤–éƒ¨çŸ¥è¯†åº“
        linked = []
        
        for entity in entities:
            linked_entity = entity.copy()
            
            # æ¨¡æ‹Ÿé“¾æ¥åˆ°çŸ¥è¯†åº“ID
            linked_entity["kb_id"] = f"KB_{entity['type'].upper()}_{hash(entity['text']) % 10000}"
            linked_entity["linked"] = True
            
            linked.append(linked_entity)
        
        return linked
    
    def disambiguate(self, entity_text: str, context: str) -> Dict:
        """
        å®ä½“æ¶ˆæ­§
        
        åœ¨å¤šä¸ªå¯èƒ½çš„å®ä½“ä¸­é€‰æ‹©æ­£ç¡®çš„ä¸€ä¸ª
        
        Args:
            entity_text: å®ä½“æ–‡æœ¬
            context: ä¸Šä¸‹æ–‡
            
        Returns:
            æ¶ˆæ­§ç»“æœ
        """
        # å®é™…åº”ä½¿ç”¨ä¸Šä¸‹æ–‡å’ŒçŸ¥è¯†åº“è¿›è¡Œæ¶ˆæ­§
        return {
            "entity": entity_text,
            "context": context,
            "candidates": [
                {"text": entity_text, "type": "person", "score": 0.85},
                {"text": entity_text, "type": "location", "score": 0.15}
            ],
            "selected": {
                "text": entity_text,
                "type": "person",
                "reason": "åŸºäºä¸Šä¸‹æ–‡åˆ†æ"
            }
        }
    
    def get_entity_statistics(self, entities: Dict) -> Dict:
        """è·å–å®ä½“ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            "total_entities": sum(len(v) for v in entities.values()),
            "entity_types": len(entities),
            "by_type": {}
        }
        
        for entity_type, entity_list in entities.items():
            stats["by_type"][entity_type] = {
                "count": len(entity_list),
                "unique": len(set(e["text"] for e in entity_list))
            }
        
        return stats


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    extractor = EntityExtractor()
    
    test_text = """
åä¸ºæŠ€æœ¯æœ‰é™å…¬å¸äº2025å¹´11æœˆ9æ—¥åœ¨æ·±åœ³å¸‚å‘å¸ƒäº†æ–°äº§å“ã€‚
è”ç³»æ–¹å¼ï¼šcontact@huawei.comï¼Œç”µè¯ï¼š13800138000ã€‚
äº§å“å”®ä»·ä¸ºÂ¥5,999ï¼Œé¢„è®¡é”€å”®é¢è¾¾åˆ°50äº¿å…ƒã€‚
è¯¦æƒ…è¯·è®¿é—® https://www.huawei.com/cn/
    """
    
    print("âœ… å®ä½“æå–å™¨å·²åŠ è½½\n")
    
    # æå–å®ä½“
    result = extractor.extract(test_text)
    
    print(f"ğŸ“Š æå–ç»“æœ:")
    print(f"  æ€»å®ä½“æ•°: {result['entity_count']}")
    print(f"  å®ä½“ç±»å‹: {', '.join(result['entity_types'])}")
    print()
    
    for entity_type, entities in result["entities"].items():
        print(f"  {entity_type}:")
        for e in entities:
            print(f"    - {e['text']}")
    
    print("\nğŸ’¡ å®é™…éƒ¨ç½²å»ºè®®:")
    print("  â€¢ å®‰è£… spaCy + ä¸­æ–‡æ¨¡å‹: python -m spacy download zh_core_web_sm")
    print("  â€¢ æˆ–ä½¿ç”¨ transformers åŠ è½½BERT-NERæ¨¡å‹")
    print("  â€¢ æˆ–å¯¹æ¥ç™¾åº¦ã€è®¯é£ç­‰NER API")



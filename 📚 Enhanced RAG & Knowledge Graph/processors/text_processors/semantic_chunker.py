"""
è¯­ä¹‰åˆ†å—å™¨
åŸºäºè¯­ä¹‰è¾¹ç•Œæ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼Œä¿æŒä¸Šä¸‹æ–‡å®Œæ•´æ€§
"""
from typing import List, Dict, Optional
import re


class SemanticChunker:
    """è¯­ä¹‰åˆ†å—å™¨"""
    
    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        min_chunk_size: int = 100
    ):
        """
        åˆå§‹åŒ–è¯­ä¹‰åˆ†å—å™¨
        
        Args:
            chunk_size: ç›®æ ‡å—å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            chunk_overlap: å—ä¹‹é—´çš„é‡å ï¼ˆå­—ç¬¦æ•°ï¼‰
            min_chunk_size: æœ€å°å—å¤§å°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size
    
    def chunk(self, text: str, strategy: str = "semantic") -> List[Dict]:
        """
        åˆ†å—æ–‡æœ¬
        
        Args:
            text: è¦åˆ†å—çš„æ–‡æœ¬
            strategy: åˆ†å—ç­–ç•¥ï¼ˆsemantic, paragraph, sentence, fixedï¼‰
            
        Returns:
            åˆ†å—ç»“æœåˆ—è¡¨
        """
        if strategy == "semantic":
            return self._semantic_chunk(text)
        elif strategy == "paragraph":
            return self._paragraph_chunk(text)
        elif strategy == "sentence":
            return self._sentence_chunk(text)
        elif strategy == "fixed":
            return self._fixed_chunk(text)
        else:
            return self._semantic_chunk(text)
    
    def _semantic_chunk(self, text: str) -> List[Dict]:
        """
        è¯­ä¹‰åˆ†å—
        
        åŸºäºè¯­ä¹‰è¾¹ç•Œè¯†åˆ«ï¼ˆæ®µè½ã€æ ‡é¢˜ã€åˆ—è¡¨ç­‰ï¼‰
        """
        chunks = []
        
        # 1. é¦–å…ˆæŒ‰æ®µè½åˆ†å‰²
        paragraphs = self._split_paragraphs(text)
        
        current_chunk = ""
        chunk_id = 0
        
        for para in paragraphs:
            # å¦‚æœå½“å‰å—åŠ ä¸Šæ–°æ®µè½ä¸è¶…è¿‡é™åˆ¶ï¼Œåˆå¹¶
            if len(current_chunk) + len(para) <= self.chunk_size:
                current_chunk += para + "\n\n"
            else:
                # ä¿å­˜å½“å‰å—
                if len(current_chunk) >= self.min_chunk_size:
                    chunks.append({
                        "chunk_id": chunk_id,
                        "text": current_chunk.strip(),
                        "size": len(current_chunk),
                        "type": "semantic"
                    })
                    chunk_id += 1
                
                # å¼€å§‹æ–°å—ï¼ˆä¿ç•™é‡å ï¼‰
                overlap_text = current_chunk[-self.chunk_overlap:] if len(current_chunk) > self.chunk_overlap else ""
                current_chunk = overlap_text + para + "\n\n"
        
        # ä¿å­˜æœ€åä¸€å—
        if current_chunk.strip():
            chunks.append({
                "chunk_id": chunk_id,
                "text": current_chunk.strip(),
                "size": len(current_chunk),
                "type": "semantic"
            })
        
        return chunks
    
    def _split_paragraphs(self, text: str) -> List[str]:
        """åˆ†å‰²æ®µè½"""
        # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²
        paragraphs = re.split(r'\n\s*\n', text)
        
        # è¿‡æ»¤ç©ºæ®µè½
        paragraphs = [p.strip() for p in paragraphs if p.strip()]
        
        return paragraphs
    
    def _paragraph_chunk(self, text: str) -> List[Dict]:
        """æŒ‰æ®µè½åˆ†å—"""
        paragraphs = self._split_paragraphs(text)
        
        chunks = []
        for i, para in enumerate(paragraphs):
            chunks.append({
                "chunk_id": i,
                "text": para,
                "size": len(para),
                "type": "paragraph"
            })
        
        return chunks
    
    def _sentence_chunk(self, text: str) -> List[Dict]:
        """æŒ‰å¥å­åˆ†å—"""
        # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        chunks = []
        current_chunk = ""
        chunk_id = 0
        
        for sent in sentences:
            if len(current_chunk) + len(sent) <= self.chunk_size:
                current_chunk += sent + "ã€‚"
            else:
                if current_chunk:
                    chunks.append({
                        "chunk_id": chunk_id,
                        "text": current_chunk.strip(),
                        "size": len(current_chunk),
                        "type": "sentence"
                    })
                    chunk_id += 1
                current_chunk = sent + "ã€‚"
        
        if current_chunk:
            chunks.append({
                "chunk_id": chunk_id,
                "text": current_chunk.strip(),
                "size": len(current_chunk),
                "type": "sentence"
            })
        
        return chunks
    
    def _fixed_chunk(self, text: str) -> List[Dict]:
        """å›ºå®šå¤§å°åˆ†å—"""
        chunks = []
        chunk_id = 0
        
        for i in range(0, len(text), self.chunk_size - self.chunk_overlap):
            chunk_text = text[i:i + self.chunk_size]
            
            if chunk_text:
                chunks.append({
                    "chunk_id": chunk_id,
                    "text": chunk_text,
                    "size": len(chunk_text),
                    "type": "fixed"
                })
                chunk_id += 1
        
        return chunks
    
    def identify_semantic_boundaries(self, text: str) -> List[int]:
        """
        è¯†åˆ«è¯­ä¹‰è¾¹ç•Œä½ç½®
        
        è¯†åˆ«æ®µè½ã€æ ‡é¢˜ã€åˆ—è¡¨ç­‰ç»“æ„
        
        Returns:
            è¾¹ç•Œä½ç½®åˆ—è¡¨
        """
        boundaries = []
        
        # æ®µè½è¾¹ç•Œ
        for match in re.finditer(r'\n\s*\n', text):
            boundaries.append(match.start())
        
        # æ ‡é¢˜è¾¹ç•Œï¼ˆMarkdownæ ¼å¼ï¼‰
        for match in re.finditer(r'^#+\s+', text, re.MULTILINE):
            boundaries.append(match.start())
        
        # åˆ—è¡¨è¾¹ç•Œ
        for match in re.finditer(r'^\s*[-*â€¢]\s+', text, re.MULTILINE):
            boundaries.append(match.start())
        
        return sorted(set(boundaries))
    
    def get_chunk_statistics(self, chunks: List[Dict]) -> Dict:
        """è·å–åˆ†å—ç»Ÿè®¡ä¿¡æ¯"""
        if not chunks:
            return {
                "total_chunks": 0,
                "total_characters": 0
            }
        
        sizes = [c["size"] for c in chunks]
        
        return {
            "total_chunks": len(chunks),
            "total_characters": sum(sizes),
            "avg_chunk_size": sum(sizes) / len(sizes),
            "min_chunk_size": min(sizes),
            "max_chunk_size": max(sizes),
            "chunk_types": list(set(c["type"] for c in chunks))
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    chunker = SemanticChunker(chunk_size=500, chunk_overlap=50)
    
    test_text = """
# AIæŠ€æœ¯å‘å±•æŠ¥å‘Š

## å¼•è¨€
äººå·¥æ™ºèƒ½æŠ€æœ¯åœ¨è¿‡å»åå¹´å–å¾—äº†å·¨å¤§è¿›å±•ã€‚æ·±åº¦å­¦ä¹ ã€å¤§è¯­è¨€æ¨¡å‹ç­‰æŠ€æœ¯çš„çªç ´ï¼Œä½¿å¾—AIåœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰äº†å¹¿æ³›åº”ç”¨ã€‚

## æŠ€æœ¯è¶‹åŠ¿
1. å¤§è¯­è¨€æ¨¡å‹æŒç»­å‘å±•
2. å¤šæ¨¡æ€AIæˆä¸ºçƒ­ç‚¹
3. AIä¸ä¼ ç»Ÿè¡Œä¸šæ·±åº¦èåˆ

## åº”ç”¨æ¡ˆä¾‹
åœ¨ä¼ä¸šç®¡ç†ä¸­ï¼ŒAIå·²ç»å¯ä»¥å¸®åŠ©è¿›è¡Œæ•°æ®åˆ†æã€å†³ç­–æ”¯æŒã€æµç¨‹ä¼˜åŒ–ç­‰å·¥ä½œã€‚

## æ€»ç»“
AIæŠ€æœ¯å°†ç»§ç»­å¿«é€Ÿå‘å±•ï¼Œä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥æ·±åˆ»å˜é©ã€‚
    """
    
    print("âœ… è¯­ä¹‰åˆ†å—å™¨å·²åŠ è½½\n")
    
    # æµ‹è¯•ä¸åŒç­–ç•¥
    for strategy in ["semantic", "paragraph", "sentence"]:
        chunks = chunker.chunk(test_text, strategy=strategy)
        stats = chunker.get_chunk_statistics(chunks)
        
        print(f"ğŸ“Š {strategy}ç­–ç•¥:")
        print(f"  åˆ†å—æ•°: {stats['total_chunks']}")
        print(f"  å¹³å‡å¤§å°: {stats['avg_chunk_size']:.0f}å­—ç¬¦")
        print()



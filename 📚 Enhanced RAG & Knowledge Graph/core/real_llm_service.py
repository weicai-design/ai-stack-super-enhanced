"""
çœŸå®çš„LLMæœåŠ¡é›†æˆ
æ”¯æŒOpenAI GPT-4å’Œæœ¬åœ°Ollama
"""
import os
import asyncio
from typing import Dict, List, Optional, Any
import httpx


class LLMService:
    """LLMæœåŠ¡ç®¡ç†å™¨"""
    
    def __init__(self, provider: str = "auto"):
        """
        åˆå§‹åŒ–LLMæœåŠ¡
        
        Args:
            provider: LLMæä¾›å•†ï¼ˆopenai, ollama, autoï¼‰
        """
        self.provider = provider
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.ollama_url = os.getenv("OLLAMA_BASE_URL", os.getenv("OLLAMA_URL", "http://localhost:11434"))
        self.ollama_model = os.getenv("OLLAMA_MODEL", "qwen2.5:7b")  # ä½¿ç”¨æ›´å¥½çš„ä¸­æ–‡æ¨¡å‹
        self.ollama_available = False
        
        # æ£€æµ‹Ollamaæ˜¯å¦å¯ç”¨
        self._check_ollama()
        
        # è‡ªåŠ¨é€‰æ‹©å¯ç”¨çš„provider
        if provider == "auto":
            if self.openai_api_key:
                self.provider = "openai"
                print("âœ… ä½¿ç”¨OpenAI GPT-4")
            elif self.ollama_available:
                self.provider = "ollama"
                print(f"âœ… ä½¿ç”¨æœ¬åœ°Ollama - æ¨¡å‹: {self.ollama_model}")
            else:
                self.provider = "ollama"  # é»˜è®¤ollamaï¼Œè¿è¡Œæ—¶ä¼šæç¤ºé”™è¯¯
                print("âš ï¸  OpenAIå’ŒOllamaå‡æœªé…ç½®ï¼Œå°†å°è¯•ä½¿ç”¨Ollama")
    
    def _check_ollama(self):
        """åŒæ­¥æ£€æµ‹Ollamaæ˜¯å¦å¯ç”¨"""
        try:
            import httpx
            with httpx.Client(timeout=2.0) as client:
                resp = client.get(f"{self.ollama_url}/api/tags")
                if resp.status_code == 200:
                    models = resp.json().get("models", [])
                    model_names = [m.get("name", "") for m in models]
                    self.ollama_available = True
                    print(f"âœ… OllamaæœåŠ¡è¿è¡Œä¸­ - å·²å®‰è£…{len(models)}ä¸ªæ¨¡å‹")
                    
                    # æ£€æŸ¥æ¨èçš„æ¨¡å‹æ˜¯å¦å­˜åœ¨
                    if self.ollama_model not in model_names:
                        # å°è¯•ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„æ¨¡å‹
                        if model_names:
                            self.ollama_model = model_names[0]
                            print(f"âš ï¸  {os.getenv('OLLAMA_MODEL', 'qwen2.5:7b')}æœªå®‰è£…ï¼Œæ”¹ç”¨: {self.ollama_model}")
        except Exception as e:
            self.ollama_available = False
            print(f"âš ï¸  OllamaæœåŠ¡æœªè¿è¡Œ: {e}")
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            ç”Ÿæˆç»“æœ
        """
        if self.provider == "openai":
            return await self._generate_openai(prompt, system_prompt, temperature, max_tokens)
        elif self.provider == "ollama":
            return await self._generate_ollama(prompt, system_prompt, temperature, max_tokens)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„provider: {self.provider}")
    
    async def _generate_openai(
        self,
        prompt: str,
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int
    ) -> Dict[str, Any]:
        """ä½¿ç”¨OpenAI APIç”Ÿæˆ"""
        if not self.openai_api_key:
            return {
                "success": False,
                "error": "OPENAI_API_KEYæœªè®¾ç½®",
                "provider": "openai",
                "text": ""
            }
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                messages = []
                
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                
                messages.append({"role": "user", "content": prompt})
                
                response = await client.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.openai_api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "gpt-4",
                        "messages": messages,
                        "temperature": temperature,
                        "max_tokens": max_tokens
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return {
                        "success": True,
                        "text": data["choices"][0]["message"]["content"],
                        "provider": "openai",
                        "model": "gpt-4",
                        "usage": data.get("usage", {})
                    }
                else:
                    return {
                        "success": False,
                        "error": f"APIé”™è¯¯: {response.status_code}",
                        "provider": "openai",
                        "text": ""
                    }
        
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "provider": "openai",
                "text": ""
            }
    
    async def _generate_ollama(
        self,
        prompt: str,
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int
    ) -> Dict[str, Any]:
        """ä½¿ç”¨æœ¬åœ°Ollamaç”Ÿæˆ"""
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                # æ£€æŸ¥Ollamaæ˜¯å¦è¿è¡Œ
                try:
                    await client.get(f"{self.ollama_url}/api/tags")
                except:
                    return {
                        "success": False,
                        "error": "Ollamaæœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨OllamaæœåŠ¡",
                        "provider": "ollama",
                        "text": ""
                    }
                
                # æ„å»ºå®Œæ•´æç¤ºè¯
                full_prompt = prompt
                if system_prompt:
                    full_prompt = f"{system_prompt}\n\n{prompt}"
                
                # è°ƒç”¨Ollama API - ä½¿ç”¨é…ç½®çš„æ¨¡å‹
                response = await client.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": self.ollama_model,
                        "prompt": full_prompt,
                        "stream": False,
                        "options": {
                            "temperature": temperature,
                            "num_predict": max_tokens
                        }
                    }
                )
                
                print(f"ğŸ” Ollamaå“åº”çŠ¶æ€: {response.status_code}")
                if response.status_code == 200:
                    data = response.json()
                    print(f"âœ… Ollamaè¿”å›æ•°æ®: {list(data.keys())}")
                    if "response" in data:
                        return {
                            "success": True,
                            "text": data["response"],
                            "provider": "ollama",
                            "model": self.ollama_model
                        }
                    else:
                        print(f"âŒ Ollamaè¿”å›ç¼ºå°‘responseå­—æ®µ: {data}")
                        return {
                            "success": False,
                            "error": f"Ollamaè¿”å›æ ¼å¼é”™è¯¯: ç¼ºå°‘responseå­—æ®µ",
                            "provider": "ollama",
                            "text": ""
                        }
                else:
                    error_text = await response.aread()
                    print(f"âŒ Ollama APIé”™è¯¯: {response.status_code}, {error_text[:200]}")
                    return {
                        "success": False,
                        "error": f"Ollama APIé”™è¯¯: {response.status_code}",
                        "provider": "ollama",
                        "text": ""
                    }
        
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "provider": "ollama",
                "text": ""
            }
    
    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> Dict[str, Any]:
        """
        å¤šè½®å¯¹è¯
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ [{"role": "user/assistant", "content": "..."}]
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            
        Returns:
            ç”Ÿæˆç»“æœ
        """
        if self.provider == "openai":
            return await self._chat_openai(messages, temperature, max_tokens)
        elif self.provider == "ollama":
            # Ollamaçš„chatåŠŸèƒ½
            return await self._chat_ollama(messages, temperature, max_tokens)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„provider: {self.provider}")
    
    async def _chat_openai(
        self,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int
    ) -> Dict[str, Any]:
        """OpenAIå¤šè½®å¯¹è¯"""
        if not self.openai_api_key:
            return {
                "success": False,
                "error": "OPENAI_API_KEYæœªè®¾ç½®"
            }
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.openai_api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": "gpt-4",
                        "messages": messages,
                        "temperature": temperature,
                        "max_tokens": max_tokens
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return {
                        "success": True,
                        "text": data["choices"][0]["message"]["content"],
                        "provider": "openai",
                        "usage": data.get("usage", {})
                    }
                else:
                    return {
                        "success": False,
                        "error": f"APIé”™è¯¯: {response.status_code}"
                    }
        
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _chat_ollama(
        self,
        messages: List[Dict[str, str]],
        temperature: float,
        max_tokens: int
    ) -> Dict[str, Any]:
        """Ollamaå¤šè½®å¯¹è¯"""
        try:
            # å°†æ¶ˆæ¯è½¬æ¢ä¸ºå•ä¸ªprompt
            prompt = "\n\n".join([
                f"{msg['role']}: {msg['content']}" for msg in messages
            ])
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": self.ollama_model,
                        "prompt": prompt,
                        "stream": False
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    return {
                        "success": True,
                        "text": data["response"],
                        "provider": "ollama"
                    }
                else:
                    return {
                        "success": False,
                        "error": f"Ollamaé”™è¯¯: {response.status_code}"
                    }
        
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def get_provider(self) -> str:
        """è·å–å½“å‰provider"""
        return self.provider
    
    async def test_connection(self) -> Dict[str, Any]:
        """æµ‹è¯•LLMè¿æ¥"""
        if self.provider == "openai":
            if not self.openai_api_key:
                return {
                    "success": False,
                    "provider": "openai",
                    "message": "OPENAI_API_KEYæœªè®¾ç½®"
                }
            
            # æµ‹è¯•ç®€å•è°ƒç”¨
            result = await self.generate("æµ‹è¯•", temperature=0.7)
            return {
                "success": result.get("success", False),
                "provider": "openai",
                "message": "OpenAI APIè¿æ¥æ­£å¸¸" if result.get("success") else f"è¿æ¥å¤±è´¥: {result.get('error')}"
            }
        
        elif self.provider == "ollama":
            try:
                async with httpx.AsyncClient(timeout=5.0) as client:
                    response = await client.get(f"{self.ollama_url}/api/tags")
                    if response.status_code == 200:
                        return {
                            "success": True,
                            "provider": "ollama",
                            "message": "OllamaæœåŠ¡è¿è¡Œæ­£å¸¸"
                        }
                    else:
                        return {
                            "success": False,
                            "provider": "ollama",
                            "message": "OllamaæœåŠ¡å“åº”å¼‚å¸¸"
                        }
            except:
                return {
                    "success": False,
                    "provider": "ollama",
                    "message": "OllamaæœåŠ¡æœªè¿è¡Œ"
                }


# å…¨å±€LLMæœåŠ¡å®ä¾‹
_llm_service = None

def get_llm_service() -> LLMService:
    """è·å–LLMæœåŠ¡å®ä¾‹"""
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService(provider="auto")
    return _llm_service


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import asyncio
    
    async def test():
        llm = get_llm_service()
        
        print(f"âœ… LLMæœåŠ¡å·²åŠ è½½")
        print(f"ğŸ“‹ å½“å‰Provider: {llm.get_provider()}")
        
        # æµ‹è¯•è¿æ¥
        test_result = await llm.test_connection()
        print(f"ğŸ”Œ è¿æ¥æµ‹è¯•: {test_result['message']}")
        
        if test_result["success"]:
            # æµ‹è¯•ç”Ÿæˆ
            result = await llm.generate(
                prompt="ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹AI-STACKç³»ç»Ÿ",
                system_prompt="ä½ æ˜¯AI-STACKçš„æ™ºèƒ½åŠ©æ‰‹",
                temperature=0.7
            )
            
            if result["success"]:
                print(f"\nâœ… ç”ŸæˆæˆåŠŸ:")
                print(f"{result['text'][:200]}...")
            else:
                print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {result['error']}")
    
    asyncio.run(test())



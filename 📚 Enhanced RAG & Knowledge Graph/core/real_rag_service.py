"""
çœŸå®çš„RAGæœåŠ¡
æ›¿æ¢æ‰€æœ‰æ¨¡æ‹Ÿæ•°æ®ï¼Œå®ç°çœŸå®çš„æ£€ç´¢åŠŸèƒ½
"""
import os
import json
from pathlib import Path
from typing import List, Dict, Any, Optional
import numpy as np


class RealRAGService:
    """çœŸå®çš„RAGæ£€ç´¢æœåŠ¡"""
    
    def __init__(self, index_dir: str = "data"):
        """
        åˆå§‹åŒ–RAGæœåŠ¡
        
        Args:
            index_dir: ç´¢å¼•æ•°æ®ç›®å½•
        """
        self.index_dir = Path(index_dir)
        self.index_dir.mkdir(parents=True, exist_ok=True)
        
        self.docs_file = self.index_dir / "docs.json"
        self.vectors_file = self.index_dir / "vectors.npy"
        
        # åŠ è½½æ–‡æ¡£å’Œå‘é‡
        self.documents = self._load_documents()
        self.vectors = self._load_vectors()
        
        # åŠ è½½embeddingæ¨¡å‹
        self.embedder = self._load_embedder()
    
    def _load_embedder(self):
        """åŠ è½½embeddingæ¨¡å‹"""
        try:
            from sentence_transformers import SentenceTransformer
            model_path = os.getenv("ST_MODEL_PATH", "all-MiniLM-L6-v2")
            return SentenceTransformer(model_path)
        except ImportError:
            print("âš ï¸  sentence-transformersæœªå®‰è£…ï¼ŒRAGåŠŸèƒ½å°†å—é™")
            return None
        except Exception as e:
            print(f"âš ï¸  åŠ è½½embeddingæ¨¡å‹å¤±è´¥: {e}")
            return None
    
    def _load_documents(self) -> List[Dict]:
        """åŠ è½½æ–‡æ¡£"""
        if self.docs_file.exists():
            with open(self.docs_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    
    def _load_vectors(self) -> Optional[np.ndarray]:
        """åŠ è½½å‘é‡"""
        if self.vectors_file.exists():
            return np.load(self.vectors_file)
        return None
    
    def _save_documents(self):
        """ä¿å­˜æ–‡æ¡£"""
        with open(self.docs_file, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
    
    def _save_vectors(self):
        """ä¿å­˜å‘é‡"""
        if self.vectors is not None:
            np.save(self.vectors_file, self.vectors)
    
    async def search(
        self,
        query: str,
        top_k: int = 5,
        filters: Optional[Dict[str, Any]] = None,
        use_reranking: bool = False
    ) -> Dict[str, Any]:
        """
        çœŸå®çš„RAGæ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            filters: è¿‡æ»¤æ¡ä»¶
            use_reranking: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            
        Returns:
            æ£€ç´¢ç»“æœ
        """
        if not self.documents:
            return {
                "success": True,
                "query": query,
                "results": [],
                "message": "çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£"
            }
        
        if self.embedder is None:
            # é™çº§åˆ°å…³é”®è¯æœç´¢
            return await self._keyword_search(query, top_k, filters)
        
        try:
            # 1. ç”ŸæˆæŸ¥è¯¢å‘é‡
            query_vector = self.embedder.encode([query])[0]
            
            # 2. å‘é‡æ£€ç´¢
            if self.vectors is not None:
                scores = np.dot(self.vectors, query_vector)
                top_indices = np.argsort(scores)[::-1][:top_k * 2]  # å–2å€ç”¨äºé‡æ’
            else:
                # å¦‚æœæ²¡æœ‰å‘é‡ç´¢å¼•ï¼Œè¿”å›å‰kä¸ªæ–‡æ¡£
                top_indices = list(range(min(top_k, len(self.documents))))
                scores = np.ones(len(top_indices))
            
            # 3. åº”ç”¨è¿‡æ»¤å™¨
            results = []
            for idx in top_indices:
                if idx >= len(self.documents):
                    continue
                
                doc = self.documents[idx]
                
                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                if filters:
                    if not self._match_filters(doc, filters):
                        continue
                
                results.append({
                    "doc_id": doc.get("id", f"doc_{idx}"),
                    "content": doc.get("text", doc.get("content", "")),
                    "metadata": doc.get("metadata", {}),
                    "score": float(scores[idx]) if idx < len(scores) else 0.0,
                    "snippet": self._generate_snippet(doc.get("text", ""), query)
                })
                
                if len(results) >= top_k:
                    break
            
            # 4. é‡æ’åºï¼ˆå¯é€‰ï¼‰
            if use_reranking and len(results) > 0:
                results = await self._rerank_results(query, results)
            
            return {
                "success": True,
                "query": query,
                "results": results[:top_k],
                "total_docs": len(self.documents),
                "retrieval_method": "vector_search" if self.vectors is not None else "keyword_search"
            }
        
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "query": query,
                "results": []
            }
    
    async def _keyword_search(
        self,
        query: str,
        top_k: int,
        filters: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """å…³é”®è¯æœç´¢ï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        results = []
        query_words = set(query.lower().split())
        
        for idx, doc in enumerate(self.documents):
            content = doc.get("text", doc.get("content", "")).lower()
            
            # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
            matches = sum(1 for word in query_words if word in content)
            score = matches / len(query_words) if query_words else 0
            
            if score > 0:
                # åº”ç”¨è¿‡æ»¤å™¨
                if filters and not self._match_filters(doc, filters):
                    continue
                
                results.append({
                    "doc_id": doc.get("id", f"doc_{idx}"),
                    "content": doc.get("text", doc.get("content", "")),
                    "metadata": doc.get("metadata", {}),
                    "score": score,
                    "snippet": self._generate_snippet(doc.get("text", ""), query)
                })
        
        # æ’åº
        results.sort(key=lambda x: x["score"], reverse=True)
        
        return {
            "success": True,
            "query": query,
            "results": results[:top_k],
            "total_docs": len(self.documents),
            "retrieval_method": "keyword_search"
        }
    
    def _match_filters(self, doc: Dict, filters: Dict[str, Any]) -> bool:
        """æ£€æŸ¥æ–‡æ¡£æ˜¯å¦åŒ¹é…è¿‡æ»¤æ¡ä»¶"""
        metadata = doc.get("metadata", {})
        
        for key, value in filters.items():
            if key not in metadata:
                return False
            if metadata[key] != value:
                return False
        
        return True
    
    def _generate_snippet(self, text: str, query: str, max_length: int = 200) -> str:
        """ç”Ÿæˆæ‘˜è¦ç‰‡æ®µ"""
        if not text:
            return ""
        
        # æ‰¾åˆ°åŒ…å«æŸ¥è¯¢è¯çš„ä½ç½®
        query_words = query.lower().split()
        text_lower = text.lower()
        
        best_pos = 0
        max_matches = 0
        
        for i in range(len(text) - max_length):
            snippet = text_lower[i:i+max_length]
            matches = sum(1 for word in query_words if word in snippet)
            if matches > max_matches:
                max_matches = matches
                best_pos = i
        
        snippet = text[best_pos:best_pos+max_length]
        if best_pos > 0:
            snippet = "..." + snippet
        if best_pos + max_length < len(text):
            snippet = snippet + "..."
        
        return snippet
    
    async def _rerank_results(
        self,
        query: str,
        results: List[Dict]
    ) -> List[Dict]:
        """é‡æ’åºç»“æœï¼ˆä½¿ç”¨æ›´ç²¾ç¡®çš„æ¨¡å‹ï¼‰"""
        # ç®€åŒ–ç‰ˆï¼šåŸºäºå†…å®¹é•¿åº¦å’Œåˆ†æ•°çš„ç»¼åˆæ’åº
        for result in results:
            content_quality = min(1.0, len(result["content"]) / 500)
            result["rerank_score"] = result["score"] * 0.7 + content_quality * 0.3
        
        results.sort(key=lambda x: x.get("rerank_score", x["score"]), reverse=True)
        return results
    
    async def add_document(
        self,
        text: str,
        metadata: Optional[Dict[str, Any]] = None,
        doc_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        
        Args:
            text: æ–‡æ¡£æ–‡æœ¬
            metadata: å…ƒæ•°æ®
            doc_id: æ–‡æ¡£ID
            
        Returns:
            æ·»åŠ ç»“æœ
        """
        if not doc_id:
            doc_id = f"doc_{len(self.documents)}"
        
        # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
        document = {
            "id": doc_id,
            "text": text,
            "metadata": metadata or {},
            "created_at": str(Path(__file__).stat().st_mtime)
        }
        
        # ç”Ÿæˆå‘é‡
        if self.embedder:
            vector = self.embedder.encode([text])[0]
            
            # æ›´æ–°å‘é‡çŸ©é˜µ
            if self.vectors is None:
                self.vectors = np.array([vector])
            else:
                self.vectors = np.vstack([self.vectors, vector])
            
            self._save_vectors()
        
        # æ·»åŠ æ–‡æ¡£
        self.documents.append(document)
        self._save_documents()
        
        return {
            "success": True,
            "doc_id": doc_id,
            "message": "æ–‡æ¡£å·²æ·»åŠ åˆ°çŸ¥è¯†åº“"
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_documents": len(self.documents),
            "has_embedder": self.embedder is not None,
            "has_vectors": self.vectors is not None,
            "vector_dimensions": self.vectors.shape[1] if self.vectors is not None else 0,
            "index_dir": str(self.index_dir)
        }


# å…¨å±€RAGæœåŠ¡å®ä¾‹
_rag_service = None

def get_rag_service() -> RealRAGService:
    """è·å–RAGæœåŠ¡å®ä¾‹"""
    global _rag_service
    if _rag_service is None:
        _rag_service = RealRAGService()
    return _rag_service


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import asyncio
    
    async def test():
        rag = get_rag_service()
        
        print("âœ… RAGæœåŠ¡å·²åŠ è½½")
        print(f"ğŸ“Š ç»Ÿè®¡: {rag.get_stats()}")
        
        # æ·»åŠ æµ‹è¯•æ–‡æ¡£
        await rag.add_document(
            text="AI-STACKæ˜¯ä¸€ä¸ªä¼ä¸šçº§AIæ™ºèƒ½ç³»ç»Ÿï¼Œæä¾›1200+åŠŸèƒ½",
            metadata={"source": "test", "type": "intro"}
        )
        
        # æµ‹è¯•æ£€ç´¢
        result = await rag.search("ä»‹ç»AI-STACK", top_k=3)
        
        if result["success"]:
            print(f"\nâœ… æ£€ç´¢æˆåŠŸ:")
            for r in result["results"]:
                print(f"  â€¢ {r['snippet']} (score: {r['score']:.3f})")
        else:
            print(f"\nâŒ æ£€ç´¢å¤±è´¥: {result.get('error')}")
    
    asyncio.run(test())



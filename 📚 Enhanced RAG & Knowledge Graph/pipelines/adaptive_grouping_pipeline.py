"""
è‡ªé€‚åº”åˆ†ç»„Pipeline
åŠ¨æ€ä¼˜åŒ–æ–‡æ¡£åˆ†ç»„ç­–ç•¥
"""
from typing import List, Dict, Optional, Any
import numpy as np
from collections import defaultdict


class AdaptiveGroupingPipeline:
    """è‡ªé€‚åº”åˆ†ç»„ç®¡é“"""
    
    def __init__(
        self,
        min_group_size: int = 3,
        max_group_size: int = 20,
        similarity_threshold: float = 0.7
    ):
        """
        åˆå§‹åŒ–è‡ªé€‚åº”åˆ†ç»„ç®¡é“
        
        Args:
            min_group_size: æœ€å°ç»„å¤§å°
            max_group_size: æœ€å¤§ç»„å¤§å°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
        """
        self.min_group_size = min_group_size
        self.max_group_size = max_group_size
        self.similarity_threshold = similarity_threshold
        self.groups = []
        self.group_stats = {}
    
    def group_documents(
        self,
        documents: List[Dict],
        vectors: Optional[np.ndarray] = None,
        method: str = "clustering"
    ) -> Dict:
        """
        å¯¹æ–‡æ¡£è¿›è¡Œåˆ†ç»„
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            vectors: æ–‡æ¡£å‘é‡ï¼ˆå¯é€‰ï¼‰
            method: åˆ†ç»„æ–¹æ³•ï¼ˆclustering, similarity, topicï¼‰
            
        Returns:
            åˆ†ç»„ç»“æžœ
        """
        if method == "clustering":
            return self._clustering_group(documents, vectors)
        elif method == "similarity":
            return self._similarity_group(documents, vectors)
        elif method == "topic":
            return self._topic_group(documents)
        else:
            return self._clustering_group(documents, vectors)
    
    def _clustering_group(
        self,
        documents: List[Dict],
        vectors: Optional[np.ndarray]
    ) -> Dict:
        """
        åŸºäºŽèšç±»çš„åˆ†ç»„
        
        ä½¿ç”¨K-meansæˆ–DBSCANç®—æ³•
        """
        if vectors is None or len(vectors) == 0:
            # å¦‚æžœæ²¡æœ‰å‘é‡ï¼Œä½¿ç”¨è¯é¢˜åˆ†ç»„
            return self._topic_group(documents)
        
        # æ¨¡æ‹Ÿèšç±»ï¼ˆå®žé™…åº”ä½¿ç”¨sklearnï¼‰
        # from sklearn.cluster import KMeans, DBSCAN
        
        # ä¼°è®¡æœ€ä¼˜èšç±»æ•°ï¼ˆå¯å‘å¼ï¼‰
        optimal_k = max(3, min(10, len(documents) // 5))
        
        # æ¨¡æ‹Ÿèšç±»ç»“æžœ
        import random
        cluster_labels = [random.randint(0, optimal_k-1) for _ in documents]
        
        # ç»„ç»‡åˆ†ç»„
        groups = defaultdict(list)
        for doc, label in zip(documents, cluster_labels):
            groups[f"group_{label}"].append(doc)
        
        return {
            "success": True,
            "method": "clustering",
            "num_groups": len(groups),
            "groups": dict(groups),
            "group_sizes": {k: len(v) for k, v in groups.items()},
            "note": "å®žé™…å®žçŽ°éœ€è¦: pip install scikit-learn"
        }
    
    def _similarity_group(
        self,
        documents: List[Dict],
        vectors: Optional[np.ndarray]
    ) -> Dict:
        """
        åŸºäºŽç›¸ä¼¼åº¦çš„åˆ†ç»„
        
        è®¡ç®—æ–‡æ¡£é—´ç›¸ä¼¼åº¦ï¼Œç›¸ä¼¼æ–‡æ¡£å½’ä¸ºä¸€ç»„
        """
        if vectors is None or len(vectors) == 0:
            return self._topic_group(documents)
        
        groups = []
        assigned = set()
        
        for i, doc in enumerate(documents):
            if i in assigned:
                continue
            
            # åˆ›å»ºæ–°ç»„
            group = [doc]
            assigned.add(i)
            
            # æ‰¾åˆ°ç›¸ä¼¼æ–‡æ¡£
            if vectors is not None:
                for j in range(i+1, len(documents)):
                    if j in assigned:
                        continue
                    
                    # è®¡ç®—ç›¸ä¼¼åº¦ï¼ˆæ¨¡æ‹Ÿï¼‰
                    similarity = np.random.uniform(0.5, 0.95)
                    
                    if similarity >= self.similarity_threshold:
                        group.append(documents[j])
                        assigned.add(j)
                        
                        if len(group) >= self.max_group_size:
                            break
            
            groups.append(group)
        
        return {
            "success": True,
            "method": "similarity",
            "num_groups": len(groups),
            "groups": {f"group_{i}": group for i, group in enumerate(groups)},
            "avg_group_size": len(documents) / len(groups) if groups else 0
        }
    
    def _topic_group(self, documents: List[Dict]) -> Dict:
        """
        åŸºäºŽä¸»é¢˜çš„åˆ†ç»„
        
        ä½¿ç”¨ä¸»é¢˜æ¨¡åž‹ï¼ˆå¦‚LDAï¼‰æˆ–å…³é”®è¯
        """
        # ç®€å•çš„å…³é”®è¯åˆ†ç»„
        groups = defaultdict(list)
        
        for doc in documents:
            # æå–ä¸»é¢˜æ ‡ç­¾ï¼ˆå¦‚æžœæœ‰ï¼‰
            topic = doc.get("topic", "æœªåˆ†ç±»")
            groups[topic].append(doc)
        
        return {
            "success": True,
            "method": "topic",
            "num_groups": len(groups),
            "groups": dict(groups),
            "topics": list(groups.keys())
        }
    
    def optimize_grouping(self, documents: List[Dict], vectors: np.ndarray) -> Dict:
        """
        ä¼˜åŒ–åˆ†ç»„ç­–ç•¥
        
        è‡ªåŠ¨è°ƒæ•´å‚æ•°ä»¥èŽ·å¾—æœ€ä½³åˆ†ç»„
        
        Args:
            documents: æ–‡æ¡£åˆ—è¡¨
            vectors: æ–‡æ¡£å‘é‡
            
        Returns:
            ä¼˜åŒ–ç»“æžœ
        """
        # å°è¯•ä¸åŒå‚æ•°
        best_score = 0
        best_config = None
        best_groups = None
        
        for threshold in [0.6, 0.7, 0.8]:
            for min_size in [2, 3, 5]:
                # ä¸´æ—¶è®¾ç½®å‚æ•°
                old_threshold = self.similarity_threshold
                old_min_size = self.min_group_size
                
                self.similarity_threshold = threshold
                self.min_group_size = min_size
                
                # å°è¯•åˆ†ç»„
                result = self.group_documents(documents, vectors, method="similarity")
                
                # è¯„åˆ†ï¼ˆåŸºäºŽç»„æ•°å’Œç»„å¤§å°çš„å¹³è¡¡ï¼‰
                num_groups = result["num_groups"]
                avg_size = len(documents) / num_groups if num_groups > 0 else 0
                
                # è¯„åˆ†å‡½æ•°ï¼šåå¥½ä¸­ç­‰æ•°é‡çš„ç»„ï¼Œä¸­ç­‰å¤§å°
                score = -(abs(num_groups - (len(documents) // 5))**2) - abs(avg_size - 7)**2
                
                if score > best_score:
                    best_score = score
                    best_config = {
                        "similarity_threshold": threshold,
                        "min_group_size": min_size
                    }
                    best_groups = result
                
                # æ¢å¤å‚æ•°
                self.similarity_threshold = old_threshold
                self.min_group_size = old_min_size
        
        return {
            "success": True,
            "best_config": best_config,
            "best_groups": best_groups,
            "optimization_score": best_score,
            "message": "åˆ†ç»„ç­–ç•¥å·²ä¼˜åŒ–"
        }
    
    def evaluate_grouping(self, groups: Dict) -> Dict:
        """
        è¯„ä¼°åˆ†ç»„è´¨é‡
        
        è®¡ç®—å†…èšåº¦ã€åˆ†ç¦»åº¦ç­‰æŒ‡æ ‡
        """
        num_groups = len(groups)
        group_sizes = [len(g) for g in groups.values()]
        
        return {
            "num_groups": num_groups,
            "avg_group_size": np.mean(group_sizes),
            "std_group_size": np.std(group_sizes),
            "min_group_size": min(group_sizes) if group_sizes else 0,
            "max_group_size": max(group_sizes) if group_sizes else 0,
            "balance_score": 1.0 - (np.std(group_sizes) / np.mean(group_sizes)) if group_sizes and np.mean(group_sizes) > 0 else 0,
            "quality_rating": "ä¼˜ç§€" if np.std(group_sizes) < 3 else "è‰¯å¥½" if np.std(group_sizes) < 5 else "ä¸€èˆ¬"
        }


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    pipeline = AdaptiveGroupingPipeline()
    
    # æ¨¡æ‹Ÿæ–‡æ¡£
    documents = [
        {"id": f"doc_{i}", "text": f"æ–‡æ¡£{i}å†…å®¹", "topic": f"ä¸»é¢˜{i%3}"}
        for i in range(20)
    ]
    
    print("âœ… è‡ªé€‚åº”åˆ†ç»„Pipelineå·²åŠ è½½\n")
    
    # æµ‹è¯•åˆ†ç»„
    result = pipeline.group_documents(documents, method="topic")
    
    print(f"ðŸ“Š åˆ†ç»„ç»“æžœ:")
    print(f"  åˆ†ç»„æ•°: {result['num_groups']}")
    print(f"  ä¸»é¢˜: {', '.join(result.get('topics', []))}")
    
    # è¯„ä¼°
    evaluation = pipeline.evaluate_grouping(result["groups"])
    print(f"\nðŸ“ˆ åˆ†ç»„è´¨é‡:")
    print(f"  å¹³å‡ç»„å¤§å°: {evaluation['avg_group_size']:.1f}")
    print(f"  å¹³è¡¡å¾—åˆ†: {evaluation['balance_score']:.2f}")
    print(f"  è´¨é‡è¯„çº§: {evaluation['quality_rating']}")
    
    print("\nðŸ’¡ å®žé™…éƒ¨ç½²å»ºè®®:")
    print("  â€¢ å®‰è£… scikit-learn ç”¨äºŽèšç±»ç®—æ³•")
    print("  â€¢ å®‰è£… umap-learn ç”¨äºŽé™ç»´å¯è§†åŒ–")

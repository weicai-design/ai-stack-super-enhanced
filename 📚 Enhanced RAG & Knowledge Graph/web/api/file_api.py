#!/usr/bin/env python3
"""
Enhanced File Processing API
ÂØπÂ∫îÈúÄÊ±Ç: 1.1 ÊâÄÊúâÊ†ºÂºèÊñá‰ª∂Â§ÑÁêÜ, 1.2 ÂõõÈ°πÈ¢ÑÂ§ÑÁêÜ, 1.3 Âéª‰º™Â§ÑÁêÜ
Êñá‰ª∂‰ΩçÁΩÆ: ai-stack-super-enhanced/üìö Enhanced RAG & Knowledge Graph/web/api/115. file-api.py
"""

import json
import logging
import os
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional

import aiofiles
from fastapi import (
    APIRouter,
    BackgroundTasks,
    Depends,
    File,
    Form,
    HTTPException,
    UploadFile,
)
from fastapi.responses import StreamingResponse
from pipelines.multi_stage_preprocessor import MultiStagePreprocessor
from pipelines.truth_verification_pipeline import TruthVerificationPipeline

# ÂØºÂÖ•Êñá‰ª∂Â§ÑÁêÜÊ®°Âùó
from processors.file_processors.universal_file_parser import UniversalFileParser
from pydantic import BaseModel, Field

logger = logging.getLogger("file_api")

# ÂàõÂª∫Ë∑ØÁî±
router = APIRouter(
    prefix="/files",
    tags=["File Processing API"],
    responses={404: {"description": "Not found"}},
)


# PydanticÊ®°ÂûãÂÆö‰πâ
class FileUploadResponse(BaseModel):
    """Êñá‰ª∂‰∏ä‰º†ÂìçÂ∫îÊ®°Âûã"""

    success: bool
    file_id: str
    filename: str
    file_size: int
    file_type: str
    message: str


class FileProcessRequest(BaseModel):
    """Êñá‰ª∂Â§ÑÁêÜËØ∑Ê±ÇÊ®°Âûã"""

    file_ids: List[str] = Field(..., description="Êñá‰ª∂IDÂàóË°®")
    preprocess: bool = Field(True, description="ÊòØÂê¶È¢ÑÂ§ÑÁêÜ")
    verify_truth: bool = Field(True, description="ÊòØÂê¶È™åËØÅÁúüÂÆûÊÄß")
    extract_entities: bool = Field(True, description="ÊòØÂê¶ÊèêÂèñÂÆû‰Ωì")
    chunk_documents: bool = Field(True, description="ÊòØÂê¶ÂàÜÂùóÊñáÊ°£")


class FileProcessResponse(BaseModel):
    """Êñá‰ª∂Â§ÑÁêÜÂìçÂ∫îÊ®°Âûã"""

    success: bool
    processed_files: List[Dict[str, Any]]
    failed_files: List[Dict[str, Any]]
    process_id: str


class FileInfo(BaseModel):
    """Êñá‰ª∂‰ø°ÊÅØÊ®°Âûã"""

    file_id: str
    filename: str
    file_size: int
    file_type: str
    upload_time: str
    status: str
    metadata: Dict[str, Any]


class BatchProcessRequest(BaseModel):
    """ÊâπÂ§ÑÁêÜËØ∑Ê±ÇÊ®°Âûã"""

    operation: str = Field(..., description="Êìç‰ΩúÁ±ªÂûã")
    file_ids: List[str] = Field(..., description="Êñá‰ª∂IDÂàóË°®")
    parameters: Optional[Dict[str, Any]] = Field(None, description="Â§ÑÁêÜÂèÇÊï∞")


# ÂÖ®Â±ÄÁªÑ‰ª∂ÂÆû‰æã
_file_parser = None
_preprocessor = None
_truth_verifier = None


# ‰æùËµñÊ≥®ÂÖ•
async def get_file_parser() -> UniversalFileParser:
    """Ëé∑ÂèñÊñá‰ª∂Ëß£ÊûêÂô®ÂÆû‰æã"""
    global _file_parser
    if _file_parser is None:
        try:
            _file_parser = UniversalFileParser()
            await _file_parser.initialize()
        except Exception as e:
            logger.error(f"Failed to initialize file parser: {str(e)}")
            raise HTTPException(status_code=503, detail="File parser not initialized")
    return _file_parser


async def get_preprocessor() -> MultiStagePreprocessor:
    """Ëé∑ÂèñÈ¢ÑÂ§ÑÁêÜÂô®ÂÆû‰æã"""
    file_parser = await get_file_parser()
    return file_parser.preprocessor


async def get_truth_verifier() -> TruthVerificationPipeline:
    """Ëé∑ÂèñÁúüÂÆûÊÄßÈ™åËØÅÂô®ÂÆû‰æã"""
    global _truth_verifier
    if _truth_verifier is None:
        try:
            _truth_verifier = TruthVerificationPipeline()
            await _truth_verifier.initialize()
        except Exception as e:
            logger.error(f"Failed to initialize truth verifier: {str(e)}")
            raise HTTPException(
                status_code=503, detail="Truth verifier not initialized"
            )
    return _truth_verifier


# Êñá‰ª∂Â≠òÂÇ®ÁõÆÂΩï
UPLOAD_DIR = os.path.join(os.path.dirname(__file__), "..", "..", "data", "uploads")
os.makedirs(UPLOAD_DIR, exist_ok=True)


# APIË∑ØÁî±
@router.post("/upload", response_model=FileUploadResponse)
async def upload_file(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(..., description="‰∏ä‰º†ÁöÑÊñá‰ª∂"),
    preprocess: bool = Form(True, description="ÊòØÂê¶È¢ÑÂ§ÑÁêÜ"),
    file_parser: UniversalFileParser = Depends(get_file_parser),
):
    """
    ‰∏ä‰º†Êñá‰ª∂

    Args:
        background_tasks: ÂêéÂè∞‰ªªÂä°
        file: ‰∏ä‰º†ÁöÑÊñá‰ª∂
        preprocess: ÊòØÂê¶È¢ÑÂ§ÑÁêÜ
        file_parser: Êñá‰ª∂Ëß£ÊûêÂô®

    Returns:
        FileUploadResponse: ‰∏ä‰º†ÂìçÂ∫î
    """
    try:
        # ÁîüÊàêÊñá‰ª∂ID
        file_id = str(uuid.uuid4())
        file_extension = os.path.splitext(file.filename)[1]
        saved_filename = f"{file_id}{file_extension}"
        file_path = os.path.join(UPLOAD_DIR, saved_filename)

        # ‰øùÂ≠òÊñá‰ª∂
        file_size = 0
        async with aiofiles.open(file_path, "wb") as f:
            content = await file.read()
            await f.write(content)
            file_size = len(content)

        # Ëé∑ÂèñÊñá‰ª∂Á±ªÂûã
        file_type = file.content_type or "unknown"

        # Âú®ÂêéÂè∞Â§ÑÁêÜÊñá‰ª∂
        if preprocess:
            background_tasks.add_task(
                process_uploaded_file, file_path, file_id, file.filename, file_parser
            )

        logger.info(
            f"File uploaded: {file.filename} -> {file_id}, size: {file_size} bytes"
        )

        return FileUploadResponse(
            success=True,
            file_id=file_id,
            filename=file.filename,
            file_size=file_size,
            file_type=file_type,
            message="File uploaded successfully",
        )

    except Exception as e:
        logger.error(f"File upload failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"File upload failed: {str(e)}")


async def process_uploaded_file(
    file_path: str,
    file_id: str,
    original_filename: str,
    file_parser: UniversalFileParser,
):
    """Â§ÑÁêÜ‰∏ä‰º†ÁöÑÊñá‰ª∂ÔºàÂêéÂè∞‰ªªÂä°Ôºâ"""
    try:
        # Ëß£ÊûêÊñá‰ª∂
        parsed_data = await file_parser.parse_file(file_path)

        # Â≠òÂÇ®Ëß£ÊûêÁªìÊûú
        result_path = os.path.join(UPLOAD_DIR, f"{file_id}_parsed.json")
        async with aiofiles.open(result_path, "w", encoding="utf-8") as f:
            await f.write(
                json.dumps(
                    {
                        "file_id": file_id,
                        "original_filename": original_filename,
                        "parsed_data": parsed_data,
                        "processed_at": datetime.now().isoformat(),
                    },
                    ensure_ascii=False,
                    indent=2,
                )
            )

        logger.info(f"File processed successfully: {file_id}")

    except Exception as e:
        logger.error(f"File processing failed: {file_id}, error: {str(e)}")


@router.post("/process", response_model=FileProcessResponse)
async def process_files(
    request: FileProcessRequest,
    background_tasks: BackgroundTasks,
    file_parser: UniversalFileParser = Depends(get_file_parser),
    preprocessor: MultiStagePreprocessor = Depends(get_preprocessor),
    truth_verifier: TruthVerificationPipeline = Depends(get_truth_verifier),
):
    """
    Â§ÑÁêÜÊñá‰ª∂

    Args:
        request: Â§ÑÁêÜËØ∑Ê±Ç
        background_tasks: ÂêéÂè∞‰ªªÂä°
        file_parser: Êñá‰ª∂Ëß£ÊûêÂô®
        preprocessor: È¢ÑÂ§ÑÁêÜÂô®
        truth_verifier: ÁúüÂÆûÊÄßÈ™åËØÅÂô®

    Returns:
        FileProcessResponse: Â§ÑÁêÜÂìçÂ∫î
    """
    try:
        process_id = str(uuid.uuid4())

        # Âú®ÂêéÂè∞ÊâßË°åÊñá‰ª∂Â§ÑÁêÜ
        background_tasks.add_task(
            batch_process_files,
            request.file_ids,
            request.preprocess,
            request.verify_truth,
            request.extract_entities,
            request.chunk_documents,
            file_parser,
            preprocessor,
            truth_verifier,
            process_id,
        )

        logger.info(
            f"File processing started: process_id={process_id}, file_count={len(request.file_ids)}"
        )

        return FileProcessResponse(
            success=True, processed_files=[], failed_files=[], process_id=process_id
        )

    except Exception as e:
        logger.error(f"File processing failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"File processing failed: {str(e)}")


async def batch_process_files(
    file_ids: List[str],
    preprocess: bool,
    verify_truth: bool,
    extract_entities: bool,
    chunk_documents: bool,
    file_parser: UniversalFileParser,
    preprocessor: MultiStagePreprocessor,
    truth_verifier: TruthVerificationPipeline,
    process_id: str,
):
    """ÊâπÈáèÂ§ÑÁêÜÊñá‰ª∂ÔºàÂêéÂè∞‰ªªÂä°Ôºâ"""
    processed_files = []
    failed_files = []

    for file_id in file_ids:
        try:
            file_info = await _get_file_info(file_id)
            if not file_info:
                failed_files.append({"file_id": file_id, "error": "File not found"})
                continue

            file_path = os.path.join(
                UPLOAD_DIR, f"{file_id}{os.path.splitext(file_info['filename'])[1]}"
            )

            # Ëß£ÊûêÊñá‰ª∂
            parsed_data = await file_parser.parse_file(file_path)

            # È¢ÑÂ§ÑÁêÜ
            if preprocess:
                parsed_data = await preprocessor.process(parsed_data)

            # È™åËØÅÁúüÂÆûÊÄß
            if verify_truth:
                truth_result = await truth_verifier.verify_content(
                    parsed_data.get("content", "")
                )
                parsed_data["truth_verification"] = truth_result

            # ÊèêÂèñÂÆû‰Ωì
            if extract_entities:
                entities = await file_parser.extract_entities(
                    parsed_data.get("content", "")
                )
                parsed_data["entities"] = entities

            # ÂàÜÂùóÊñáÊ°£
            if chunk_documents:
                chunks = await file_parser.chunk_document(parsed_data)
                parsed_data["chunks"] = chunks

            # ‰øùÂ≠òÂ§ÑÁêÜÁªìÊûú
            result_path = os.path.join(UPLOAD_DIR, f"{file_id}_processed.json")
            async with aiofiles.open(result_path, "w", encoding="utf-8") as f:
                await f.write(json.dumps(parsed_data, ensure_ascii=False, indent=2))

            processed_files.append(
                {
                    "file_id": file_id,
                    "filename": file_info["filename"],
                    "status": "processed",
                    "processed_at": datetime.now().isoformat(),
                }
            )

        except Exception as e:
            failed_files.append({"file_id": file_id, "error": str(e)})
            logger.warning(f"Failed to process file {file_id}: {str(e)}")

    logger.info(
        f"Batch file processing completed: process_id={process_id}, "
        f"processed={len(processed_files)}, failed={len(failed_files)}"
    )


@router.get("/info/{file_id}")
async def get_file_info(
    file_id: str, file_parser: UniversalFileParser = Depends(get_file_parser)
):
    """
    Ëé∑ÂèñÊñá‰ª∂‰ø°ÊÅØ

    Args:
        file_id: Êñá‰ª∂ID
        file_parser: Êñá‰ª∂Ëß£ÊûêÂô®

    Returns:
        FileInfo: Êñá‰ª∂‰ø°ÊÅØ
    """
    try:
        file_info = await _get_file_info(file_id)
        if not file_info:
            raise HTTPException(status_code=404, detail=f"File {file_id} not found")

        return FileInfo(**file_info)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get file info: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"File info retrieval failed: {str(e)}"
        )


async def _get_file_info(file_id: str) -> Optional[Dict[str, Any]]:
    """Ëé∑ÂèñÊñá‰ª∂‰ø°ÊÅØÔºàÂÜÖÈÉ®ÂáΩÊï∞Ôºâ"""
    try:
        # Êü•ÊâæÊñá‰ª∂
        for filename in os.listdir(UPLOAD_DIR):
            if filename.startswith(file_id) and not filename.endswith(
                ("_parsed.json", "_processed.json")
            ):
                file_path = os.path.join(UPLOAD_DIR, filename)
                stat = os.stat(file_path)

                return {
                    "file_id": file_id,
                    "filename": filename,
                    "file_size": stat.st_size,
                    "file_type": "unknown",  # ÂèØ‰ª•Ê∑ªÂä†MIMEÁ±ªÂûãÊ£ÄÊµã
                    "upload_time": datetime.fromtimestamp(stat.st_ctime).isoformat(),
                    "status": "uploaded",
                    "metadata": {},
                }

        return None

    except Exception:
        return None


@router.get("/list")
async def list_files(
    page: int = Query(1, description="È°µÁ†Å"),
    page_size: int = Query(20, description="ÊØèÈ°µÂ§ßÂ∞è"),
    file_type: Optional[str] = Query(None, description="Êñá‰ª∂Á±ªÂûãËøáÊª§"),
):
    """
    ÂàóÂá∫Êñá‰ª∂

    Args:
        page: È°µÁ†Å
        page_size: ÊØèÈ°µÂ§ßÂ∞è
        file_type: Êñá‰ª∂Á±ªÂûãËøáÊª§

    Returns:
        Dict: Êñá‰ª∂ÂàóË°®
    """
    try:
        files = []
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size

        for filename in os.listdir(UPLOAD_DIR):
            if not filename.endswith(("_parsed.json", "_processed.json")):
                file_id = filename.split(".")[0]
                file_info = await _get_file_info(file_id)
                if file_info:
                    if file_type and file_info["file_type"] != file_type:
                        continue
                    files.append(file_info)

        # ÂàÜÈ°µ
        paginated_files = files[start_idx:end_idx]

        return {
            "success": True,
            "files": paginated_files,
            "total_count": len(files),
            "page": page,
            "page_size": page_size,
            "total_pages": (len(files) + page_size - 1) // page_size,
        }

    except Exception as e:
        logger.error(f"Failed to list files: {str(e)}")
        raise HTTPException(status_code=500, detail=f"File listing failed: {str(e)}")


@router.delete("/{file_id}")
async def delete_file(
    file_id: str, file_parser: UniversalFileParser = Depends(get_file_parser)
):
    """
    Âà†Èô§Êñá‰ª∂

    Args:
        file_id: Êñá‰ª∂ID
        file_parser: Êñá‰ª∂Ëß£ÊûêÂô®

    Returns:
        Dict: Âà†Èô§ÁªìÊûú
    """
    try:
        deleted_count = 0

        # Âà†Èô§ÊâÄÊúâÁõ∏ÂÖ≥Êñá‰ª∂
        for filename in os.listdir(UPLOAD_DIR):
            if filename.startswith(file_id):
                file_path = os.path.join(UPLOAD_DIR, filename)
                os.remove(file_path)
                deleted_count += 1

        if deleted_count > 0:
            logger.info(f"File deleted: {file_id}, deleted_files: {deleted_count}")
            return {
                "success": True,
                "message": f"File {file_id} deleted successfully",
                "deleted_files": deleted_count,
            }
        else:
            raise HTTPException(status_code=404, detail=f"File {file_id} not found")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete file: {str(e)}")
        raise HTTPException(status_code=500, detail=f"File deletion failed: {str(e)}")


@router.get("/download/{file_id}")
async def download_file(
    file_id: str, file_parser: UniversalFileParser = Depends(get_file_parser)
):
    """
    ‰∏ãËΩΩÊñá‰ª∂

    Args:
        file_id: Êñá‰ª∂ID
        file_parser: Êñá‰ª∂Ëß£ÊûêÂô®

    Returns:
        StreamingResponse: Êñá‰ª∂ÊµÅ
    """
    try:
        file_info = await _get_file_info(file_id)
        if not file_info:
            raise HTTPException(status_code=404, detail=f"File {file_id} not found")

        filename = file_info["filename"]
        file_path = os.path.join(UPLOAD_DIR, filename)

        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail=f"File {file_id} not found")

        return StreamingResponse(
            open(file_path, "rb"),
            media_type="application/octet-stream",
            headers={"Content-Disposition": f'attachment; filename="{filename}"'},
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to download file: {str(e)}")
        raise HTTPException(status_code=500, detail=f"File download failed: {str(e)}")


@router.get("/processed/{file_id}")
async def get_processed_result(
    file_id: str, file_parser: UniversalFileParser = Depends(get_file_parser)
):
    """
    Ëé∑ÂèñÊñá‰ª∂Â§ÑÁêÜÁªìÊûú

    Args:
        file_id: Êñá‰ª∂ID
        file_parser: Êñá‰ª∂Ëß£ÊûêÂô®

    Returns:
        Dict: Â§ÑÁêÜÁªìÊûú
    """
    try:
        result_path = os.path.join(UPLOAD_DIR, f"{file_id}_processed.json")
        if not os.path.exists(result_path):
            raise HTTPException(
                status_code=404, detail=f"Processed result for {file_id} not found"
            )

        async with aiofiles.open(result_path, "r", encoding="utf-8") as f:
            content = await f.read()
            result = json.loads(content)

        return {
            "success": True,
            "file_id": file_id,
            "processed_result": result,
            "retrieved_at": datetime.now().isoformat(),
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to get processed result: {str(e)}")
        raise HTTPException(
            status_code=500, detail=f"Processed result retrieval failed: {str(e)}"
        )


@router.get("/health")
async def health_check():
    """ÂÅ•Â∫∑Ê£ÄÊü•Á´ØÁÇπ"""
    try:
        file_parser = await get_file_parser()
        preprocessor = await get_preprocessor()

        health_status = {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "components": {
                "file_parser": file_parser is not None,
                "preprocessor": preprocessor is not None,
                "truth_verifier": _truth_verifier is not None,
            },
        }
        return health_status
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        raise HTTPException(status_code=503, detail="Service unhealthy")

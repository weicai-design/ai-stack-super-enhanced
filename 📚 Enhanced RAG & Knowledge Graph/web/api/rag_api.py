#!/usr/bin/env python3
"""
Enhanced RAG Core API
å¯¹åº”éœ€æ±‚: 1.1 æ‰€æœ‰æ ¼å¼æ–‡ä»¶å¤„ç†, 1.2 å››é¡¹é¢„å¤„ç†, 1.3 å»ä¼ªå¤„ç†, 1.6 è¯ä¹‰è‡ªä¸»åˆ†ç»„
æ–‡ä»¶ä½ç½®: ai-stack-super-enhanced/ğŸ“š Enhanced RAG & Knowledge Graph/web/api/rag_api.py
"""

import logging
import uuid
from datetime import datetime
from typing import Any, Dict, List, Optional

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from core.hybrid_rag_engine import HybridRAGEngine
from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException
from pipelines.smart_ingestion_pipeline import SmartIngestionPipeline
from pydantic import BaseModel, Field

from processors.file_processors.universal_file_parser import UniversalFileParser

logger = logging.getLogger("rag_api")

# åˆ›å»ºè·¯ç”±
router = APIRouter(
    prefix="/rag", tags=["RAG Core API"], responses={404: {"description": "Not found"}}
)


# Pydanticæ¨¡å‹å®šä¹‰
class SearchQuery(BaseModel):
    """æœç´¢æŸ¥è¯¢æ¨¡å‹"""

    query: str = Field(..., description="æœç´¢æŸ¥è¯¢æ–‡æœ¬")
    top_k: int = Field(10, description="è¿”å›ç»“æœæ•°é‡")
    threshold: float = Field(0.7, description="ç›¸ä¼¼åº¦é˜ˆå€¼")
    filters: Optional[Dict[str, Any]] = Field(None, description="è¿‡æ»¤æ¡ä»¶")
    include_metadata: bool = Field(True, description="æ˜¯å¦åŒ…å«å…ƒæ•°æ®")


class SearchResponse(BaseModel):
    """æœç´¢å“åº”æ¨¡å‹"""

    results: List[Dict[str, Any]]
    total_count: int
    query_time: float
    query_id: str


class DocumentChunk(BaseModel):
    """æ–‡æ¡£åˆ†å—æ¨¡å‹"""

    content: str
    metadata: Dict[str, Any]
    chunk_id: str
    document_id: str


class IngestionRequest(BaseModel):
    """æ–‡æ¡£æ³¨å…¥è¯·æ±‚æ¨¡å‹"""

    documents: List[Dict[str, Any]]
    preprocess: bool = Field(True, description="æ˜¯å¦é¢„å¤„ç†")
    verify_truth: bool = Field(True, description="æ˜¯å¦éªŒè¯çœŸå®æ€§")
    group_semantically: bool = Field(True, description="æ˜¯å¦è¯­ä¹‰åˆ†ç»„")


class IngestionResponse(BaseModel):
    """æ–‡æ¡£æ³¨å…¥å“åº”æ¨¡å‹"""

    success: bool
    processed_count: int
    failed_count: int
    failed_documents: List[Dict[str, Any]]
    ingestion_id: str


class GroupingRequest(BaseModel):
    """è¯­ä¹‰åˆ†ç»„è¯·æ±‚æ¨¡å‹"""

    documents: List[Dict[str, Any]]
    grouping_strategy: str = Field("semantic", description="åˆ†ç»„ç­–ç•¥")
    max_groups: Optional[int] = Field(None, description="æœ€å¤§åˆ†ç»„æ•°")


class GroupingResponse(BaseModel):
    """è¯­ä¹‰åˆ†ç»„å“åº”æ¨¡å‹"""

    groups: List[Dict[str, Any]]
    total_groups: int
    grouping_strategy: str


# å…¨å±€ç»„ä»¶å®ä¾‹
_rag_engine = None
_file_parser = None
_ingestion_pipeline = None


# ä¾èµ–æ³¨å…¥
async def get_rag_engine() -> HybridRAGEngine:
    """è·å–RAGå¼•æ“å®ä¾‹"""
    global _rag_engine
    if _rag_engine is None:
        try:
            # åˆ›å»ºæœ¬åœ° RAG å¼•æ“å®ä¾‹ï¼Œå¹¶æ³¨å…¥æœ¬åœ°å‘é‡å­˜å‚¨ä¸è¯­ä¹‰å¼•æ“ï¼ˆA1 æ–¹æ¡ˆï¼‰
            _rag_engine = HybridRAGEngine()

            # å»¶è¿Ÿå¯¼å…¥ heavy ä¾èµ–
            try:
                import faiss
                from sentence_transformers import SentenceTransformer
            except Exception:
                SentenceTransformer = None
                faiss = None

            # Lightweight FAISS wrapper
            class FAISSWrapper:
                def __init__(self, dim: int = 384):
                    self.dim = dim
                    self.index = None
                    self.id_map = []

                def initialize(self):
                    if faiss is None:
                        return False
                    # ä½¿ç”¨ HNSW (æ›´é€‚åˆ CPU) æˆ– Flat ç´¢å¼•
                    try:
                        self.index = faiss.IndexHNSWFlat(self.dim, 32)
                        # å¯é€‰ï¼šè®¾ç½® efSearch/efConstruction
                        return True
                    except Exception:
                        try:
                            self.index = faiss.IndexFlatL2(self.dim)
                            return True
                        except Exception:
                            return False

                def add_documents(self, vectors: List[List[float]], ids: List[str]):
                    if self.index is None:
                        raise RuntimeError("FAISS index not initialized")
                    import numpy as np

                    vecs = np.array(vectors).astype("float32")
                    self.index.add(vecs)
                    self.id_map.extend(ids)

                def save_index(self, path: str):
                    try:
                        if faiss is None or self.index is None:
                            return False
                        faiss.write_index(self.index, path)
                        # save id_map
                        import pickle

                        with open(path + ".ids.pkl", "wb") as f:
                            pickle.dump(self.id_map, f)
                        return True
                    except Exception:
                        return False

                def load_index(self, path: str):
                    try:
                        if faiss is None:
                            return False
                        self.index = faiss.read_index(path)
                        import pickle

                        with open(path + ".ids.pkl", "rb") as f:
                            self.id_map = pickle.load(f)
                        return True
                    except Exception:
                        return False

                def retrieve(
                    self,
                    query: str = None,
                    vector: List[float] = None,
                    filters=None,
                    top_k: int = 10,
                ):
                    # æ”¯æŒåŸºäº vector çš„æ£€ç´¢ï¼›å¦‚æœæä¾› query åˆ™ä¸Šå±‚åº”å…ˆ encode
                    if self.index is None:
                        return []
                    import numpy as np

                    if vector is None:
                        return []

                    vec = np.array([vector]).astype("float32")
                    D, idxs = self.index.search(vec, top_k)
                    results = []
                    for score, idx in zip(D[0], idxs[0]):
                        if idx < 0 or idx >= len(self.id_map):
                            continue
                        results.append(
                            {
                                "document_id": self.id_map[idx],
                                "content": "",
                                "score": float(score),
                                "metadata": {},
                                "source": "faiss",
                            }
                        )
                    return results

            # Lightweight local semantic engine using sentence-transformers
            # Supports loading from a local cache folder to avoid remote downloads
            class LocalSemanticEngine:
                def __init__(
                    self,
                    model_name: str = "all-MiniLM-L6-v2",
                    local_model_path: Optional[str] = "./models/all-MiniLM-L6-v2",
                ):
                    self.model_name = model_name
                    self.local_model_path = local_model_path
                    self.model = None
                    # will be set to FAISSWrapper instance after instantiation
                    self.vector_store = None

                def initialize(self):
                    """Initialize semantic model only from a local path.

                    In offline environments we must NOT attempt to download models
                    from the Internet. If the local_model_path exists, load it; otherwise
                    return False immediately so the system falls back to the
                    in-memory semantic/fallback behavior.
                    """
                    if SentenceTransformer is None:
                        return False
                    try:
                        import os

                        if self.local_model_path and os.path.exists(
                            self.local_model_path
                        ):
                            try:
                                # SentenceTransformer accepts a local folder path
                                self.model = SentenceTransformer(self.local_model_path)
                                return True
                            except Exception:
                                self.model = None
                                return False

                        # Do not attempt remote downloads in offline mode; return False
                        return False
                    except Exception:
                        self.model = None
                        return False

                def encode_query(self, query: str):
                    if not self.model:
                        return None
                    return self.model.encode(query).tolist()

                async def retrieve(self, query: str, filters=None, top_k: int = 10):
                    # For local engine, encode and then delegate to vector_store.retrieve
                    try:
                        vec = self.encode_query(query)
                        if vec is None or self.vector_store is None:
                            return []
                        # delegate to FAISS wrapper
                        results = self.vector_store.retrieve(
                            vector=vec, filters=filters, top_k=top_k
                        )
                        return results
                    except Exception:
                        return []

            # å®ä¾‹åŒ–æœ¬åœ°ç»„ä»¶
            vector_store = FAISSWrapper(dim=384)
            # try to load existing index from disk to persist across restarts
            import os

            idx_path = os.path.join("./data", "faiss.index")
            if os.path.exists(idx_path):
                _ = vector_store.load_index(idx_path)
            else:
                # attempt initialize; if fails, HybridRAGEngine will fallback to in-memory store
                try:
                    _ = vector_store.initialize()
                except Exception:
                    _ = False

            # instantiate semantic engine and attach vector_store immediately so the
            # engine can perform fallback behaviors even before a heavyweight model
            # finishes loading in the background.
            import os as _os

            semantic_engine = LocalSemanticEngine(
                local_model_path=_os.environ.get(
                    "LOCAL_ST_MODEL_PATH", "./models/all-MiniLM-L6-v2"
                )
            )
            # In offline/local mode we do NOT attempt background model downloads or
            # initialization. The LocalSemanticEngine will only be initialized if a
            # local model path is present and a manual initialization is requested.
            # This avoids startup blocking in environments without Internet access.
            # (No-op here)
            pass

            # attach vector_store so semantic_engine.retrieve can call into it (even if
            # semantic model not yet loaded)
            try:
                semantic_engine.vector_store = vector_store
            except Exception:
                pass

            # æ³¨å…¥æœ¬åœ° DynamicKnowledgeGraphï¼ˆå¸¦æŒä¹…åŒ–é…ç½®ï¼‰
            try:
                from core.dynamic_knowledge_graph import DynamicKnowledgeGraph

                kg = DynamicKnowledgeGraph(config={"storage_path": "./data"})
                await kg.initialize()
            except Exception:
                kg = None

            # Always provide vector_store and semantic_engine instances to the
            # RAG engine; HybridRAGEngine will perform its own validation and
            # potentially enable in-memory fallbacks.
            dependencies = {
                "file_parser": await get_file_parser(),
                "knowledge_graph": kg,
                "ingestion_pipeline": None,
                "vector_store": vector_store,
                "semantic_engine": semantic_engine,
                "config": {},
            }

            await _rag_engine.initialize(dependencies)

            # Ensure the ingestion pipeline (created elsewhere) is wired to the
            # engine's semantic engine and vector store so that background
            # ingestion writes embeddings into the same store the engine uses.
            try:
                try:
                    ingestion_pipeline_instance = await get_ingestion_pipeline()
                except Exception:
                    ingestion_pipeline_instance = None

                if ingestion_pipeline_instance is not None:
                    # set attributes directly to avoid reinitialization
                    try:
                        setattr(
                            ingestion_pipeline_instance,
                            "semantic_engine",
                            getattr(_rag_engine, "semantic_engine", None),
                        )
                        setattr(
                            ingestion_pipeline_instance,
                            "vector_store",
                            getattr(_rag_engine, "vector_store", None),
                        )
                        # also keep reference on the engine
                        _rag_engine.ingestion_pipeline = ingestion_pipeline_instance
                        logger.info(
                            "Wired ingestion pipeline with engine semantic_engine and vector_store"
                        )
                    except Exception as _e:
                        logger.warning(
                            f"Failed to attach core services to ingestion pipeline: {_e}"
                        )
            except Exception:
                # Non-fatal wiring failure; continue without blocking initialization
                logger.warning(
                    "Unable to wire ingestion pipeline to engine (non-fatal)"
                )
            # åº•å±‚è¯Šæ–­æ—¥å¿—ï¼šè®°å½•è¿è¡Œæ—¶ç±»æ¥æºå’Œæ–¹æ³•åˆ—è¡¨ï¼Œå¸®åŠ©å®šä½æ–¹æ³•ç¼ºå¤±é—®é¢˜
            try:
                logger.info(
                    f"RAG engine class: {_rag_engine.__class__}, module: {_rag_engine.__class__.__module__}"
                )
                logger.info(
                    f"RAG engine dir: {sorted([n for n in dir(_rag_engine) if not n.startswith('__')])}"
                )
            except Exception:
                pass
        except Exception as e:
            logger.error(f"Failed to initialize RAG engine: {str(e)}")
            raise HTTPException(status_code=503, detail="RAG engine not initialized")
    return _rag_engine


async def get_file_parser() -> UniversalFileParser:
    """è·å–æ–‡ä»¶è§£æå™¨å®ä¾‹"""
    global _file_parser
    if _file_parser is None:
        try:
            _file_parser = UniversalFileParser()
            # UniversalFileParser.initialize may be sync or async; handle both
            init = getattr(_file_parser, "initialize", None)
            if callable(init):
                import inspect

                if inspect.iscoroutinefunction(init):
                    await init()
                else:
                    init()
            else:
                logger.warning("UniversalFileParser has no initialize() method")
        except Exception as e:
            logger.error(f"Failed to initialize file parser: {str(e)}")
            raise HTTPException(status_code=503, detail="File parser not initialized")
    return _file_parser


async def get_ingestion_pipeline() -> SmartIngestionPipeline:
    """è·å–æ³¨å…¥ç®¡é“å®ä¾‹"""
    global _ingestion_pipeline
    if _ingestion_pipeline is None:
        try:
            # Prefer the registered pipeline implementation which provides process_document
            try:
                from pipelines.smart_ingestion_pipeline import (
                    RegisteredSmartIngestionPipeline,
                )

                _ingestion_pipeline = RegisteredSmartIngestionPipeline()
            except Exception:
                # fallback to base class if registered subclass is not importable
                _ingestion_pipeline = SmartIngestionPipeline()

            # initialize may be sync or async
            init = getattr(_ingestion_pipeline, "initialize", None)
            if callable(init):
                import inspect

                if inspect.iscoroutinefunction(init):
                    await init()
                else:
                    init()
            else:
                logger.warning("Ingestion pipeline has no initialize() method")
        except Exception as e:
            logger.error(f"Failed to initialize ingestion pipeline: {str(e)}")
            raise HTTPException(
                status_code=503, detail="Ingestion pipeline not initialized"
            )
    return _ingestion_pipeline


# APIè·¯ç”±
@router.post("/search", response_model=SearchResponse)
async def semantic_search(
    query: SearchQuery, rag_engine: HybridRAGEngine = Depends(get_rag_engine)
):
    """
    è¯­ä¹‰æœç´¢

    Args:
        query: æœç´¢æŸ¥è¯¢
        rag_engine: RAGå¼•æ“

    Returns:
        SearchResponse: æœç´¢ç»“æœ
    """
    try:
        start_time = datetime.now()

        # æ‰§è¡Œæœç´¢ - å…¼å®¹ä¸åŒå¼•æ“å®ç°
        if hasattr(rag_engine, "search") and callable(getattr(rag_engine, "search")):
            results = await rag_engine.search(
                query=query.query,
                top_k=query.top_k,
                threshold=query.threshold,
                filters=query.filters,
                include_metadata=query.include_metadata,
            )
        elif hasattr(rag_engine, "hybrid_retrieve") and callable(
            getattr(rag_engine, "hybrid_retrieve")
        ):
            # hybrid_retrieve è¿”å› RetrievalResult å¯¹è±¡åˆ—è¡¨ï¼›å°†å…¶è½¬æ¢ä¸º API è¿”å›æ ¼å¼
            raw = await rag_engine.hybrid_retrieve(query=query.query)
            results = []
            for r in raw[: query.top_k]:
                # æ”¯æŒ dataclass æˆ– dict é£æ ¼
                try:
                    doc_id = getattr(r, "document_id", None) or r.get("document_id")
                    content = getattr(r, "content", None) or r.get("content", "")
                    score = getattr(r, "similarity_score", None) or r.get("score", 0.0)
                    metadata = getattr(r, "metadata", None) or r.get("metadata", {})
                    source = getattr(r, "source", None) or r.get("source", "")
                except Exception:
                    # æœ€åå›é€€ï¼šæŠŠå¯¹è±¡è½¬ä¸ºå­—ç¬¦ä¸²è®°å½•
                    doc_id = str(getattr(r, "document_id", ""))
                    content = str(getattr(r, "content", ""))
                    score = float(getattr(r, "similarity_score", 0.0) or 0.0)
                    metadata = {}
                    source = ""

                results.append(
                    {
                        "document_id": doc_id,
                        "content": content,
                        "score": float(score),
                        "metadata": metadata if query.include_metadata else {},
                        "source": source,
                    }
                )
        else:
            raise Exception("RAG engine does not provide search or hybrid_retrieve")

        query_time = (datetime.now() - start_time).total_seconds()
        query_id = str(uuid.uuid4())

        logger.info(
            f"Semantic search completed: query='{query.query}', results={len(results)}, time={query_time:.3f}s"
        )

        return SearchResponse(
            results=results,
            total_count=len(results),
            query_time=query_time,
            query_id=query_id,
        )

    except Exception as e:
        logger.error(f"Semantic search failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Search failed: {str(e)}")


@router.post("/ingest/documents", response_model=IngestionResponse)
async def ingest_documents(
    request: IngestionRequest,
    background_tasks: BackgroundTasks,
    ingestion_pipeline: SmartIngestionPipeline = Depends(get_ingestion_pipeline),
):
    """
    æ‰¹é‡æ³¨å…¥æ–‡æ¡£

    Args:
        request: æ³¨å…¥è¯·æ±‚
        background_tasks: åå°ä»»åŠ¡
        ingestion_pipeline: æ³¨å…¥ç®¡é“

    Returns:
        IngestionResponse: æ³¨å…¥å“åº”
    """
    try:
        ingestion_id = str(uuid.uuid4())

        # åœ¨åå°æ‰§è¡Œæ–‡æ¡£æ³¨å…¥
        background_tasks.add_task(
            process_documents_ingestion,
            request.documents,
            request.preprocess,
            request.verify_truth,
            request.group_semantically,
            ingestion_pipeline,
            ingestion_id,
        )

        logger.info(
            f"Document ingestion started: ingestion_id={ingestion_id}, document_count={len(request.documents)}"
        )

        return IngestionResponse(
            success=True,
            processed_count=0,  # å¼‚æ­¥å¤„ç†ï¼Œåˆå§‹ä¸º0
            failed_count=0,
            failed_documents=[],
            ingestion_id=ingestion_id,
        )

    except Exception as e:
        logger.error(f"Document ingestion failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Ingestion failed: {str(e)}")


async def process_documents_ingestion(
    documents: List[Dict[str, Any]],
    preprocess: bool,
    verify_truth: bool,
    group_semantically: bool,
    ingestion_pipeline: SmartIngestionPipeline,
    ingestion_id: str,
):
    """å¤„ç†æ–‡æ¡£æ³¨å…¥çš„åå°ä»»åŠ¡"""
    try:
        processed_count = 0
        failed_count = 0
        failed_documents = []

        for doc in documents:
            try:
                # æ‰§è¡Œæ–‡æ¡£æ³¨å…¥
                await ingestion_pipeline.process_document(
                    document=doc,
                    preprocess=preprocess,
                    verify_truth=verify_truth,
                    group_semantically=group_semantically,
                )
                processed_count += 1

            except Exception as e:
                failed_count += 1
                failed_documents.append(
                    {"document": doc.get("id", "unknown"), "error": str(e)}
                )
                logger.warning(f"Failed to process document {doc.get('id')}: {str(e)}")

        logger.info(
            f"Document ingestion completed: ingestion_id={ingestion_id}, "
            f"processed={processed_count}, failed={failed_count}"
        )

    except Exception as e:
        logger.error(f"Background ingestion task failed: {str(e)}")


@router.post("/group/semantic", response_model=GroupingResponse)
async def semantic_grouping(
    request: GroupingRequest, rag_engine: HybridRAGEngine = Depends(get_rag_engine)
):
    """
    è¯­ä¹‰åˆ†ç»„

    Args:
        request: åˆ†ç»„è¯·æ±‚
        rag_engine: RAGå¼•æ“

    Returns:
        GroupingResponse: åˆ†ç»„å“åº”
    """
    try:
        # æ‰§è¡Œè¯­ä¹‰åˆ†ç»„
        groups = await rag_engine.semantic_grouping(
            documents=request.documents,
            strategy=request.grouping_strategy,
            max_groups=request.max_groups,
        )

        logger.info(
            f"Semantic grouping completed: documents={len(request.documents)}, groups={len(groups)}"
        )

        return GroupingResponse(
            groups=groups,
            total_groups=len(groups),
            grouping_strategy=request.grouping_strategy,
        )

    except Exception as e:
        logger.error(f"Semantic grouping failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Grouping failed: {str(e)}")


@router.get("/chunks/{document_id}")
async def get_document_chunks(
    document_id: str, rag_engine: HybridRAGEngine = Depends(get_rag_engine)
):
    """
    è·å–æ–‡æ¡£åˆ†å—

    Args:
        document_id: æ–‡æ¡£ID
        rag_engine: RAGå¼•æ“

    Returns:
        List[DocumentChunk]: æ–‡æ¡£åˆ†å—åˆ—è¡¨
    """
    try:
        chunks = await rag_engine.get_document_chunks(document_id)

        return [
            DocumentChunk(
                content=chunk["content"],
                metadata=chunk["metadata"],
                chunk_id=chunk["chunk_id"],
                document_id=chunk["document_id"],
            )
            for chunk in chunks
        ]

    except Exception as e:
        logger.error(f"Failed to get document chunks: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get chunks: {str(e)}")


@router.delete("/documents/{document_id}")
async def delete_document(
    document_id: str, rag_engine: HybridRAGEngine = Depends(get_rag_engine)
):
    """
    åˆ é™¤æ–‡æ¡£

    Args:
        document_id: æ–‡æ¡£ID
        rag_engine: RAGå¼•æ“

    Returns:
        Dict: åˆ é™¤ç»“æœ
    """
    try:
        success = await rag_engine.delete_document(document_id)

        if success:
            logger.info(f"Document deleted: {document_id}")
            return {"success": True, "message": f"Document {document_id} deleted"}
        else:
            raise HTTPException(
                status_code=404, detail=f"Document {document_id} not found"
            )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Failed to delete document: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Deletion failed: {str(e)}")


@router.get("/stats")
async def get_rag_stats(rag_engine: HybridRAGEngine = Depends(get_rag_engine)):
    """
    è·å–RAGç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯

    Args:
        rag_engine: RAGå¼•æ“

    Returns:
        Dict: ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        stats = await rag_engine.get_system_stats()
        return stats

    except Exception as e:
        logger.error(f"Failed to get RAG stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Stats retrieval failed: {str(e)}")


@router.post("/clear-cache")
async def clear_cache(rag_engine: HybridRAGEngine = Depends(get_rag_engine)):
    """
    æ¸…é™¤ç¼“å­˜

    Args:
        rag_engine: RAGå¼•æ“

    Returns:
        Dict: æ¸…é™¤ç»“æœ
    """
    try:
        await rag_engine.clear_cache()
        logger.info("RAG cache cleared")
        return {"success": True, "message": "Cache cleared successfully"}

    except Exception as e:
        logger.error(f"Failed to clear cache: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Cache clearance failed: {str(e)}")


@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    try:
        rag_engine = await get_rag_engine()
        file_parser = await get_file_parser()
        # Provide more detailed readiness info for observability
        semantic_engine_ready = False
        semantic_engine_loading = False
        vector_store_ready = False
        try:
            # rag_engine may expose semantic_engine and vector_store attributes
            if rag_engine is not None:
                se = getattr(rag_engine, "semantic_engine", None)
                vs = getattr(rag_engine, "vector_store", None)
                # If semantic engine has a loaded model attribute, consider it ready
                if se is not None:
                    semantic_engine_loading = getattr(se, "model", None) is None
                    semantic_engine_ready = getattr(se, "model", None) is not None
                vector_store_ready = vs is not None

        except Exception:
            pass

        health_status = {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "components": {
                "rag_engine": rag_engine is not None,
                "file_parser": file_parser is not None,
                "ingestion_pipeline": _ingestion_pipeline is not None,
            },
            "readiness": {
                "semantic_engine_ready": semantic_engine_ready,
                "semantic_engine_loading": semantic_engine_loading,
                "vector_store_ready": vector_store_ready,
                "ingestion_pipeline_ready": _ingestion_pipeline is not None,
                "rag_engine_status": (
                    getattr(rag_engine, "status", None)
                    if rag_engine is not None
                    else None
                ),
            },
        }
        return health_status
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        raise HTTPException(status_code=503, detail="Service unhealthy")

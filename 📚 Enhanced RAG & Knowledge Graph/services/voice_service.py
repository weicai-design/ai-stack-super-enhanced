"""
çœŸå®çš„è¯­éŸ³æœåŠ¡
æ”¯æŒè¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰å’Œè¯­éŸ³åˆæˆï¼ˆTTSï¼‰
"""
import os
from pathlib import Path
from typing import Dict, Any, Optional
import tempfile


class VoiceService:
    """è¯­éŸ³æœåŠ¡"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯­éŸ³æœåŠ¡"""
        self.whisper_available = self._check_whisper()
        self.tts_available = self._check_tts()
    
    def _check_whisper(self) -> bool:
        """æ£€æŸ¥Whisperæ˜¯å¦å¯ç”¨"""
        try:
            import whisper
            return True
        except ImportError:
            return False
    
    def _check_tts(self) -> bool:
        """æ£€æŸ¥TTSæ˜¯å¦å¯ç”¨"""
        try:
            from gtts import gTTS
            return True
        except ImportError:
            return False
    
    async def recognize_speech(self, audio_file_path: str) -> Dict[str, Any]:
        """
        è¯­éŸ³è¯†åˆ«ï¼ˆçœŸå®å®ç°ï¼‰
        
        Args:
            audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            è¯†åˆ«ç»“æœ
        """
        if not self.whisper_available:
            return {
                "success": False,
                "error": "Whisperæœªå®‰è£…",
                "solution": "è¿è¡Œ: pip install openai-whisper",
                "text": ""
            }
        
        try:
            import whisper
            
            # åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨baseæ¨¡å‹ï¼Œé€Ÿåº¦å’Œå‡†ç¡®åº¦çš„å¹³è¡¡ï¼‰
            model = whisper.load_model("base")
            
            # è½¬å½•éŸ³é¢‘
            result = model.transcribe(audio_file_path, language="zh")
            
            return {
                "success": True,
                "text": result["text"],
                "language": result["language"],
                "segments": [
                    {
                        "start": seg["start"],
                        "end": seg["end"],
                        "text": seg["text"]
                    }
                    for seg in result["segments"]
                ],
                "confidence": 0.95,
                "model": "whisper-base"
            }
        
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "text": ""
            }
    
    async def synthesize_speech(
        self,
        text: str,
        language: str = "zh-cn",
        slow: bool = False
    ) -> Dict[str, Any]:
        """
        è¯­éŸ³åˆæˆï¼ˆçœŸå®å®ç°ï¼‰
        
        Args:
            text: è¦è½¬æ¢çš„æ–‡æœ¬
            language: è¯­è¨€ä»£ç 
            slow: æ˜¯å¦æ…¢é€Ÿ
            
        Returns:
            åˆæˆç»“æœ
        """
        if not self.tts_available:
            return {
                "success": False,
                "error": "gTTSæœªå®‰è£…",
                "solution": "è¿è¡Œ: pip install gtts",
                "audio_path": ""
            }
        
        try:
            from gtts import gTTS
            
            # åˆ›å»ºTTSå¯¹è±¡
            tts = gTTS(text=text, lang=language, slow=slow)
            
            # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
            output_dir = Path("data/audio")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            audio_path = output_dir / f"tts_{int(datetime.now().timestamp())}.mp3"
            tts.save(str(audio_path))
            
            # ä¼°ç®—æ—¶é•¿ï¼ˆä¸­æ–‡çº¦æ¯åˆ†é’Ÿ300å­—ï¼‰
            estimated_duration = len(text) / 300 * 60 if not slow else len(text) / 150 * 60
            
            return {
                "success": True,
                "audio_path": str(audio_path),
                "audio_url": f"/api/v5/agent/voice/audio/{audio_path.name}",
                "duration": estimated_duration,
                "format": "mp3",
                "language": language,
                "text_length": len(text)
            }
        
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "audio_path": ""
            }
    
    def get_status(self) -> Dict[str, Any]:
        """è·å–è¯­éŸ³æœåŠ¡çŠ¶æ€"""
        return {
            "whisper_available": self.whisper_available,
            "tts_available": self.tts_available,
            "supported_features": {
                "speech_recognition": self.whisper_available,
                "speech_synthesis": self.tts_available
            },
            "installation_guide": {
                "whisper": "pip install openai-whisper",
                "tts": "pip install gtts"
            }
        }


# å…¨å±€è¯­éŸ³æœåŠ¡å®ä¾‹
_voice_service = None

def get_voice_service() -> VoiceService:
    """è·å–è¯­éŸ³æœåŠ¡å®ä¾‹"""
    global _voice_service
    if _voice_service is None:
        _voice_service = VoiceService()
    return _voice_service


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import asyncio
    from datetime import datetime
    
    async def test():
        voice = get_voice_service()
        
        print("âœ… è¯­éŸ³æœåŠ¡å·²åŠ è½½")
        print(f"ğŸ“Š çŠ¶æ€: {voice.get_status()}")
        
        # æµ‹è¯•TTS
        if voice.tts_available:
            result = await voice.synthesize_speech("ä½ å¥½ï¼Œè¿™æ˜¯AI-STACKè¯­éŸ³æµ‹è¯•")
            if result["success"]:
                print(f"\nâœ… TTSæˆåŠŸ: {result['audio_path']}")
            else:
                print(f"\nâŒ TTSå¤±è´¥: {result['error']}")
        else:
            print("\nâš ï¸  TTSä¸å¯ç”¨ï¼Œè¯·å®‰è£…: pip install gtts")
    
    asyncio.run(test())



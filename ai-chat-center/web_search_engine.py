"""
éœ€æ±‚5: å¤–éƒ¨ç½‘ç«™å†…å®¹ç²¾å‡†æœç´¢å¼•æ“
æ”¯æŒå¤šç§æœç´¢å¼•æ“å’Œç½‘ç«™çˆ¬å–
"""

import httpx
from bs4 import BeautifulSoup
import re
from typing import List, Dict, Any
from urllib.parse import quote, urljoin
import asyncio


class WebSearchEngine:
    """å¤–éƒ¨ç½‘ç«™ç²¾å‡†æœç´¢å¼•æ“"""
    
    def __init__(self):
        self.search_engines = {
            "google": "https://www.google.com/search?q={}",
            "bing": "https://www.bing.com/search?q={}",
            "baidu": "https://www.baidu.com/s?wd={}",
        }
        
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36"
        }
    
    async def search_web(self, query: str, engine: str = "bing", max_results: int = 5) -> List[Dict[str, Any]]:
        """
        ç²¾å‡†æœç´¢å¤–éƒ¨ç½‘ç«™
        """
        results = []
        
        try:
            if engine == "bing":
                results = await self.search_bing(query, max_results)
            elif engine == "google":
                results = await self.search_google(query, max_results)
            elif engine == "baidu":
                results = await self.search_baidu(query, max_results)
            else:
                # é»˜è®¤ä½¿ç”¨bing
                results = await self.search_bing(query, max_results)
        
        except Exception as e:
            print(f"âŒ ç½‘é¡µæœç´¢å¤±è´¥: {e}")
        
        return results
    
    async def search_bing(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """Bingæœç´¢"""
        results = []
        
        try:
            async with httpx.AsyncClient(follow_redirects=True) as client:
                url = f"https://www.bing.com/search?q={quote(query)}"
                response = await client.get(url, headers=self.headers, timeout=10.0)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # è§£æBingæœç´¢ç»“æœ
                    search_results = soup.find_all('li', class_='b_algo', limit=max_results)
                    
                    for item in search_results:
                        title_elem = item.find('h2')
                        link_elem = item.find('a')
                        snippet_elem = item.find('p') or item.find('div', class_='b_caption')
                        
                        if title_elem and link_elem:
                            results.append({
                                "title": title_elem.get_text(strip=True),
                                "url": link_elem.get('href', ''),
                                "snippet": snippet_elem.get_text(strip=True) if snippet_elem else "",
                                "source": "Bing"
                            })
        
        except Exception as e:
            print(f"âŒ Bingæœç´¢å¤±è´¥: {e}")
        
        return results
    
    async def search_google(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """Googleæœç´¢ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        results = []
        
        try:
            async with httpx.AsyncClient(follow_redirects=True) as client:
                url = f"https://www.google.com/search?q={quote(query)}"
                response = await client.get(url, headers=self.headers, timeout=10.0)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Googleæœç´¢ç»“æœè§£æ
                    search_results = soup.find_all('div', class_='g', limit=max_results)
                    
                    for item in search_results:
                        title_elem = item.find('h3')
                        link_elem = item.find('a')
                        snippet_elem = item.find('div', class_='VwiC3b')
                        
                        if title_elem and link_elem:
                            results.append({
                                "title": title_elem.get_text(strip=True),
                                "url": link_elem.get('href', ''),
                                "snippet": snippet_elem.get_text(strip=True) if snippet_elem else "",
                                "source": "Google"
                            })
        
        except Exception as e:
            print(f"âŒ Googleæœç´¢å¤±è´¥: {e}")
        
        return results
    
    async def search_baidu(self, query: str, max_results: int = 5) -> List[Dict[str, Any]]:
        """ç™¾åº¦æœç´¢"""
        results = []
        
        try:
            async with httpx.AsyncClient(follow_redirects=True) as client:
                url = f"https://www.baidu.com/s?wd={quote(query)}"
                response = await client.get(url, headers=self.headers, timeout=10.0)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ç™¾åº¦æœç´¢ç»“æœè§£æ
                    search_results = soup.find_all('div', class_='result', limit=max_results)
                    
                    for item in search_results:
                        title_elem = item.find('h3') or item.find('a')
                        link_elem = item.find('a')
                        snippet_elem = item.find('span', class_='content-right_8Zs40')
                        
                        if title_elem and link_elem:
                            results.append({
                                "title": title_elem.get_text(strip=True),
                                "url": link_elem.get('href', ''),
                                "snippet": snippet_elem.get_text(strip=True) if snippet_elem else "",
                                "source": "ç™¾åº¦"
                            })
        
        except Exception as e:
            print(f"âŒ ç™¾åº¦æœç´¢å¤±è´¥: {e}")
        
        return results
    
    async def scrape_website(self, url: str) -> Dict[str, Any]:
        """
        ç²¾å‡†æŠ“å–ç‰¹å®šç½‘ç«™å†…å®¹
        """
        try:
            async with httpx.AsyncClient(follow_redirects=True) as client:
                response = await client.get(url, headers=self.headers, timeout=15.0)
                
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # ç§»é™¤è„šæœ¬å’Œæ ·å¼
                    for script in soup(["script", "style"]):
                        script.decompose()
                    
                    # æå–æ–‡æœ¬
                    text = soup.get_text()
                    lines = (line.strip() for line in text.splitlines())
                    chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                    text = '\n'.join(chunk for chunk in chunks if chunk)
                    
                    return {
                        "url": url,
                        "title": soup.title.string if soup.title else "æ— æ ‡é¢˜",
                        "content": text[:5000],  # å‰5000å­—ç¬¦
                        "success": True
                    }
        
        except Exception as e:
            return {
                "url": url,
                "error": str(e),
                "success": False
            }
    
    async def search_and_scrape(self, query: str, engine: str = "bing", scrape_top: int = 3) -> str:
        """
        æœç´¢å¹¶æŠ“å–topç»“æœçš„è¯¦ç»†å†…å®¹
        """
        # 1. æœç´¢
        search_results = await self.search_web(query, engine, max_results=5)
        
        if not search_results:
            return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ"
        
        # 2. æŠ“å–topç»“æœçš„è¯¦ç»†å†…å®¹
        detailed_results = []
        
        for i, result in enumerate(search_results[:scrape_top], 1):
            url = result.get("url", "")
            if url and url.startswith("http"):
                scraped = await self.scrape_website(url)
                
                if scraped.get("success"):
                    detailed_results.append({
                        "index": i,
                        "title": result.get("title"),
                        "url": url,
                        "snippet": result.get("snippet"),
                        "content": scraped.get("content", "")[:1000]  # æ¯ä¸ªé¡µé¢1000å­—ç¬¦
                    })
        
        # 3. æ ¼å¼åŒ–è¿”å›
        formatted_result = f"ğŸ” å¤–éƒ¨ç½‘ç«™æœç´¢ç»“æœï¼ˆæ‰¾åˆ° {len(search_results)} æ¡ï¼‰\n\n"
        
        for result in search_results:
            formatted_result += f"ğŸ“„ {result.get('title')}\n"
            formatted_result += f"ğŸ”— {result.get('url')}\n"
            formatted_result += f"ğŸ“ {result.get('snippet')}\n\n"
        
        if detailed_results:
            formatted_result += f"\nğŸ“– è¯¦ç»†å†…å®¹ï¼ˆæŠ“å–å‰ {len(detailed_results)} ä¸ªï¼‰\n\n"
            
            for detail in detailed_results:
                formatted_result += f"ã€{detail['index']}. {detail['title']}ã€‘\n"
                formatted_result += f"{detail['content'][:500]}...\n\n"
        
        return formatted_result


